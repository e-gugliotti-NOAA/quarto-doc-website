[
  {
    "objectID": "qmds/about.html",
    "href": "qmds/about.html",
    "title": "About",
    "section": "",
    "text": "About this site, testing nested builds\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "qmds/html.html",
    "href": "qmds/html.html",
    "title": "RAW HTML CONTENT",
    "section": "",
    "text": "Manual home page    \n  nmfs-stock-synthesis/doc\n  Report an issue\n  Table of contents\nFish population (aka \"stock\") assessment models determine the impact of past fishing on the historical and current abundance of the population, evaluate sustainable rates of removals (catch), and project future levels of catch reflecting one or more risk-averse catch rules. These catch rules are codified in regional Fishery Management Plans according to requirements of the Sustainable Fisheries Act. In the U.S., approximately 500 federally managed fish and shellfish populations are managed under approximately 50 Fishery Management Plans. About 200 of these populations are assessed each year, based on a prioritized schedule for their current status. Despite this, many minor species have never been quantitatively assessed. Although the pace is slower than that for weather forecasting, fish stock assessments are operational models for fisheries management.\nAssessment models typically assimilate annual catches, data on fish abundance from diverse surveys and fishery sources, and biological information regarding fish body size and proportions at age. A suite of models is available depending on the degree of data availability and unique characteristics of the fish population or its fishery. Where feasible, environmental time series are used as indicators of changes in population or observation processes, especially to improve the accuracy of the projections of abundance and sustainable catch into the future. Such linkages are based principally on correlations given the challenge of conducting field observations on an appropriate scale. The frontier of model development is in the rapid estimation of parameters to include random temporal effects, in the simultaneous modeling of a suite of interacting species, and in the explicit treatment of the spatial distribution of the population.\nAssessment models are loosely coupled to other models. For example, an ocean-temperature or circulation model or benthic-habitat map may be directly included in the pre-processing of the fish abundance survey. A time series of a derived ocean factor, like the North Atlantic Oscillation, can be included as an indicator of a change in a population process. Output of a multi-decadal time series of derived fish abundance can be an input to ecosystem and economic models to better understand cumulative impacts and benefits.\nStock Synthesis is an age- and size-structured assessment model in the class of models termed integrated analysis models. Stock Synthesis has evolved since its initial inception in order to model a wide range of fish populations and dynamics. The most recent major revision to Stock Synthesis occurred in 2016, when version 3.30 was introduced. This new version of Stock Synthesis required major revisions to the input files relative to earlier versions (see the Converting Files section for more information). The acronym for Stock Synthesis has evolved over time with earlier versions being referred to as SS2 (Stock Synthesis v.2.xx) and older versions as SS3 (Stock Synthesis v.3.xx).\nSS3 has a population sub-model that simulates a stock’s growth, maturity, fecundity, recruitment, movement, and mortality processes, an observation sub-model estimates expected values for various types of data, a statistical sub-model characterizes the data’s goodness of fit and obtains best-fitting parameters with associated variance, and a forecast sub-model projects needed management quantities. SS3 outputs the quantities, with confidence intervals, needed to implement risk-averse fishery control rules. The model is coded in C++ with parameter estimation enabled by automatic differentiation (admb). Windows, Linux, and iOS versions are available. Output processing and associated tools are in R, and a graphical interface is in QT. SS3 executables and support material is available on GitHub. The rich feature set in SS3 allows it to be configured for a wide range of situations. SS3 has become the basis for a large fraction of U.S. assessments and many other assessments around the world.\nThis manual provides a guide for using SS3. The guide contains a description of the input and output files and usage instructions. An overview and technical description of the model itself is in Methot and Wetzel (2013). However, SS3 has continued to evolve and grow since the publication in 2013, with this manual reflecting the most up to date information regarding SS3. The model and a graphical user interface are available on GitHub with older archived versions also available online at NOAA VLAB. The VLAB site also provides a user forum for posting questions and for accessing various additional materials. An output processor package, r4ss, in R is available for download from CRAN or GitHub.\nAdditional guidance for new users can be found online in the Getting Started Tutorial and on the Stock Synthesis GitHub page.\nPlease cite Stock Synthesis as:\nMethot, R.D. and Wetzel, C.R. (2013). Stock Synthesis: A biological and statistical framework for fish stock assessment and fishery management. Fisheries Research, 142: 86-99. https://doi.org/10.1016/j.fishres.2012.10.012\nPlease cite the Stock Synthesis User Manual as:\nMethot, R. D., Jr., C. R. Wetzel, I. G. Taylor, and K. Doering. (2020). Stock Synthesis User Manual Version 3.30.15. U.S. Department of Commerce, NOAA Processed Report NMFS-NWFSC-PR-2020-05.https://doi.org/10.25923/5wpn-qt71\nSS3 is typically run through the command line interface, although it can also be called from another program, R, the Stock Synthesis Interface, or a script file (such as a DOS batch file). SS3 is compiled for Windows, Mac, and Linux operating systems. The memory requirements depend on the complexity of the model you run, but in general, SS3 will run much slower on computers with inadequate memory. See Running Stock Synthesis for additional notes on methods of running SS3.\nCommunication with the program is through text files. When the program first starts, it reads the file starter.ss, which typically must be located in the same directory from which SS3 is being run. The file starter.ss contains required input information plus references to other required input files, as described in the File Organization section. The names of the control and data files must match the names specified in the starter.ss file. File names, including starter.ss, are case-sensitive on Linux and Mac systems but not on Windows. The echoinput.sso file outputs how the executable reads each input file and can be used for troubleshooting when trying to setup a model correctly. Output from SS3 consists of text files containing specific keywords. Output processing programs, such as the SSI, Excel, or R can search for these keywords and parse the specific information located below that keyword in the text file.\nConverting files from version 3.24 to version 3.30 can be performed by using the program ss_trans.exe. This executable takes 3.24 files as input and will output 3.30 input and output files. SS_trans executables are available for v. 3.30.01 - 3.30.17. The transitional executable was phased out with v.3.30.18. If a model needs to be converted from v.3.24 to a recent version, one should use the v. 3.30.17 ss_trans.exe available from the 3.30.17 release page on GitHub to convert the files and then any additional adjustments needed between v.3.30.17 and newer versions should be done by hand.\nThe following file structure and steps are recommended for converting model files:\nThere are some options that have been substantially changed in SS3 v.3.30, which impedes the automatic converting of SS3 v.3.24 model files. Known examples of SS3 v.3.24 options that cannot be converted, but for which better alternatives are available in SS3 v.3.30 are:\nSS3 begins by reading the file starter.ss. The term COND appears in the \"Typical Value\" column of this documentation (it does not actually appear in the model files), it indicates that the following section is omitted except under certain conditions, or that the factors included in the following section depend upon certain conditions. In most cases, the description in the definition column is the same as the label output to the ss_new files."
  },
  {
    "objectID": "qmds/html.html#forecast-file-options-forecast.ss",
    "href": "qmds/html.html#forecast-file-options-forecast.ss",
    "title": "RAW HTML CONTENT",
    "section": "6.1 Forecast File Options (forecast.ss)",
    "text": "6.1 Forecast File Options (forecast.ss)\n\n\n\nValue\nOptions\nDescription\n\n\n\n\n\n\n\n\n\n1\nBenchmarks/Reference Points:\nSS3 checks for consistency of the Forecast specification and the benchmark specification. It will turn benchmarks on if necessary and report a warning.\n\n\n\n0 = omit;\n\n\n\n\n1 = calculate FSPR, FBtarget, and FMSY;\n\n\n\n\n2 = calculate FSPR, FMSY, F0.10; and\n\n\n\n\n3 = add F at BLIMIT\n\n\n\n1\nMSY Method:\nSpecifies basis for calculating a single population level FMSY value.\n\n\n\n1 = FSPR as proxy;\n\n\n\n\n2 = calculate FMSY;\n\n\n\n\n3 = FBtarget as proxy or F0.10;\n\n\n\n\n4 = Fend year as proxy; and\n\n\n\n\n5 = FMEY.\n\n\n\nCOND: MSY Method = 5\n\n\n\n1\nMaximum economic yield (MEY) units\n\n\n\n\n1 = dead biomass;\n\n\n\n\n2 = dead biomass without excluded bycatch fleet;\n\n\n\n\n3 = retained biomass; and\n\n\n\n\n4 = profits using price and costs.\n\n\n\n\nMEY options - Fleet, Cost/F, Price/F, and Include FMEY in Optimization\nTo calculate the FMEY enter fleet number, the cost per fishing mortality, price per mt, and whether optimization should adjust the fleet’s F or keep it at the mean from the benchmark years (0 = no, 1= yes). Care should taken when scaling the values used for cost/F and price/mt. Units in the example show cost=0 and price = 1, so will be identical to MSY in weight. Note, if a fleet’s catch is excluded from the FMEY search, its catch or profits are still included in the MSY value using historical F levels from benchmark years\n\n\n-9999 0 0 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.45\nSPRtarget\nSS3 searches for F multiplier that will produce this level of spawning biomass per recruit (reproductive output) relative to unfished value.\n\n\n\n\n\n\n\n\n\n\n\n\n0.40\nRelative Biomass Target\nSS3 searches for F multiplier that will produce this level of spawning biomass relative to unfished value. This is not “per recruit” and takes into account the spawner-recruitment relationship.\n\n\n\n\n\n\n\n\n\n\n\n\nCOND: Do Benchmark = 3\nBLIMIT as a fraction of the BMSY where a negative value will be applied as a fraction of B0\n\n\n\n-0.25\n\n\n\n\n\n\n\n\n0 0 0 0 0 0 0 0 0 0\nBenchmark Years:\nRequires 10 values, beginning and ending years for (1,2) biology (e.g., growth, natural mortality, maturity, fecundity), (3,4) selectivity, (5,6) relative Fs, (7,8) movement and recruitment distribution; (9,10) stock-recruitment parameters for averaging years in calculating benchmark quantities. If there is no time-varying biology it is recommend to select the first model year for the beginning year for biology.\n\n\n\n-999: start year;\n\n\n\n\n&gt;0: absolute year; and\n\n\n\n\n&lt;= 0: year relative to end year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nBenchmark Relative F Basis:\nThe specification does not affect year range for selectivity and biology.\n\n\n\n1 = use year range; and\n\n\n\n\n2 = set range for relF same as forecast below.\n\n\n\n2\nForecast:\nThis input is required but is ignored if benchmarks are turned off. This determines how forecast catches are calculated and removed from the population which is separate from the \"MSY Method\" above. If FMSY is selected, it uses whatever proxy (e.g., FSPR or FBTGT) is selected in the \"MSY Method\" row.\n\n\n\n-1 = none, no forecast years;\n\n\n\n\n0 = simple, single forecast year calculated;\n\n\n\n\n1 = use FSPR;\n\n\n\n\n2 = use FMSY;\n\n\n\n\n3 = use FBtarget or F0.10;\n\n\n\n\n4 = set to average F scalar for the forecast relative F years below; and\n\n\n\n\n5 = input annual F scalar.\n\n\n\n10\nN forecast years (must be &gt;= 1)\nAt least one forecast year now required if the Forecast option above is &gt;=0 (Note: SS3 v.3.24 allowed zero forecast years).\n\n\n\n\n\n\n\n1\nF scalar\nOnly used if Forecast option = 5 (input annual F scalar), but is a required line in the forecast file.\n\n\n\n\n\n\n\n0 0 0 0 0 0\nForecast Years:\nRequires 6 values: beginning and ending years for selectivity, relative Fs, and recruitment distribution that will be used to create averages to use in forecasts. In future, hope to allow random effects to propagate into forecast. Please note, relative F for bycatch only fleets is scaled just like other fleets. More options for this in future.\n\n\n\n-999 = start year;\n\n\n\n\n&gt;0 = absolute year; and\n\n\n\n\n&lt;= 0 = year relative to end year.\n\n\n\n\n\n\n\n\n0\nForecast Selectivity Option:\nDetermines the selectivity used in the forecast years. Selecting 1 will allow for application of time-varying selectivity parameters (e.g., random walk) to continue into the forecast period.\n\n\n\n0 = forecast selectivity is mean from year range; and\n\n\n\n\n1 = forecast selectivity from annual time-varying parameters.\n\n\n\n1\nControl Rule:\nUsed to apply reductions (\"buffer\") to either the catch or F based on the control rule during the forecast period. The buffer value is specified below via the Control Rule Buffer.\n\n\n\n0 = none (additional control rule inputs will be ignored);\n\n\n\n\n1 = catch as function of SSB, buffer on F;\n\n\n\n\n2 = F as function of SSB, buffer on F;\n\n\n\n\n3 = catch as function of SSB, buffer on catch (U.S. West Coast groundfish approach); and\n\n\n\n\n4 = F is a function of SSB, buffer on catch.\n\n\n\n0.40\nControl Rule Inflection\nRelative biomass level to unfished biomass above which F is constant at control rule F. If set to -1 the ratio of BMSY to the unfished spawning biomass will automatically be used.target.\n\n\n\n\n\n\n\n0.10\nControl Rule Cutoff\nRelative biomass level to unfished biomass below which F is set to 0 (management threshold).\n\n\n\n\n\n\n\n\nControl Rule Buffer (multiplier between 0-1.0 or -1)\nControl rule catch or Ftarget as a fraction of selected catch or FMSY proxy. The buffer will be applied to reduce catch from the estimated overfishing limit. The buffer value is a value between 0-1.0 where a value of 1.0 would set catch equal to the overfishing limit. As example if the buffer is applied to catch (Control Rule option 3 or 4 above) the catch will equal the buffer times the overfishing limit. Alternatively a value of -1 will allow the user to input a forecast year specific control rule fraction (added in v. 3.30.13).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOND -1: Conditional input for annual control rule buffer\nYear and control rule buffer value. Can enter a value for each year, or starting sequence of years. The final control rule buffer value will apply to all sequent forecast years.\n\n\n2019 0.8\n\n\n\n\n2020 0.6\n\n\n\n\n2021 0.5\n\n\n\n\n-9999 0\n\n\n\n\n3\nNumber of forecast loops (1,2,3)\nSS3 sequentially goes through the forecast up to three times. Maximum number of forecast loops: 1=OFL only, 2=ABC control rule and buffers, 3=set catches equal to control rule or input catch and redo forecast implementation error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\nFirst forecast loop with stochastic recruitment\nIf this is set to 1 or 2, then OFL and ABC will be calculated as if there was perfect knowledge about recruitment deviations in the future. If running a long forecast (e.g., 10-100 years) it is recommended to run without recruitment deviations since running long forecasts with recruitment deviations not turned on until loop 3 may have poor results (e.g., crashed stock), especially if below average forecast recruitment is assumed (via \"Forecast recruitment\" option, next input line).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecast recruitment:\nOption 0, ignore input and do forecast recruitment as before SS3 v.3.30.10, if 1, then use next value as a multiplier applied after env/block/regime is applied, if 2, then use value as multiplier times adjusted virgin recruitment (after time-varying adjustments to R0), and if 3, then use value as the number of years from end of main recruitment deviations to average (mean is the recruitments, not the deviations). Need to set phase to -1 in control to get constant recruitment in MCMC.\n\n\n\n0 = spawner recruit curve;\n\n\n\n\n1 = value*(spawner recruit curve);\n\n\n\n\n2 = value*(virgin recruitment); and\n\n\n\n\n3 = recent mean from year range above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nScalar or number of years of recent main recruitments to average.\nThis input depends upon option selected directly above. If option 1 or 2 selected this value should be a scalar value to be applied to recruitment. If option 3 is selected above this should be input as the number of years to average recruitment.\n\n\n\n\n\n\n\n\n\n\n\n\n0\nForecast loop control #5\nReserved for future model features.\n\n\n2015\nFirst year for caps and allocations\nShould be after years with fixed inputs.\n\n\n0\nImplementation Error\nThe standard deviation of the log of the ratio between the realized catch and the target catch in the forecast. (set value &gt;0.0 to cause implementation error deviations to be an estimated parameter that will add variance to forecast).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nRebuilder:\nCreates a rebuild.dat file to be used for U.S. West Coast groundfish rebuilder program.\n\n\n\n0 = omit U.S. West Coast rebuilder output; and\n\n\n\n\n1 = do abbreviated U.S. West Coast rebuilder output\n\n\n\n\nRebuilder catch (Year Declared):\nInput line is required even if Rebuilder = 0, specified in the line above.\n\n\n\n&gt;0 = year first catch should be set to zero; and\n\n\n\n\n-1 = set to 1999.\n\n\n\n2004\nRebuilder start year (Year Initial):\nInput line is required even if Rebuilder = 0, specified two line above.\n\n\n\n&gt;0 = year for current age structure; and\n\n\n\n\n-1 = set to end year +1.\n\n\n\n1\nFleet Relative F:\n\n\n\n\n1 = use first-last allocation year; and\n\n\n\n\n2 = read season(row) x fleet (column) set below.\n\n\n\n2\nBasis for maximum forecast catch:\nThe maximum basis for forecasted catch will be implemented for the for the \"First year for caps and allocations\" selected above. The maximum catch (biomass or numbers) by fleet is specified below on the \"Maximum total forecast catch by fleet\" line.\n\n\n\n2 = total catch biomass;\n\n\n\n\n3 = retained catch biomass;\n\n\n\n\n5 = total catch numbers; and\n\n\n\n\n6 = retained total numbers.\n\n\n\nCOND 2: Conditional input for fleet relative F (Enter: Season, Fleet, Relative F)\n\n\n1 1 0.6\nFleet allocation by relative F fraction.\nThe fraction of the forecast F value. For a multiple area model user must define a fraction for each fleet and each area. The total fractions must sum to one over all fleets and areas.\n\n\n1 2 0.4\n\n\n\n\n-9999 0 0\nTerminator line\n\n\n\n1 50\nMaximum total forecast catch by fleet (in units specified above total catch/numbers, retained catch/numbers)\nEnter fleet number and its maximum value. Last line of the entry must have fleet number = -9999.\n\n\n-9999 -1\n\n\n\n\n-9999 -1\nMaximum total catch by area\nEnter area number and its max. Last line of the entry must have area number = -9999.\n\n\n\n-1 = no maximum\n\n\n\n1 1\nFleet assignment to allocation group\nEnter list of fleet number and its allocation group number if it is in a group. Last line of the entry must have fleet number = -9999.\n\n\n-9999 -1\n\n\n\n\nCOND: if N allocation groups is &gt;0\nEnter a year and the allocation fraction to each group for that year. SS3 will fill those values to the end of the forecast, then read another year from this list. Terminate with -9999 in year field. Annual values are rescaled to sum to 1.0.\n\n\n2002 1\nAllocation to each group for each year of the forecast\n\n\n\n-9999 1\n\n\n\n\n-1\nBasis for forecast catch:\nThe dead or retained value in the forecast catch inputs will be interpreted in terms of numbers or biomass based on the units of the input catch for each fleet.\n\n\n\n-1 = Read basis with each observation, allows for a mixture of dead, retained, or F basis by different fleets for the fixed catches below;\n\n\n\n\n2 = Dead catch (retained + discarded);\n\n\n\n\n3 = Retained catch; and\n\n\n\n\n99 = Input apical F (the apical F value for the model years can be found in the EXPLOITATION section in the Report file).\n\n\n\nCOND: == -1\nForecasted catches - enter one line per number of fixed forecast year catch\n\n\n2012 1 1 1200 2\nYear & Season & Fleet & Catch or F value & Basis\n\n\n2013 1 1 1400 3\nYear & Season & Fleet & Catch or F value & Basis\n\n\n-9999 0 0 0 0\nIndicates end of inputted catches to read\n\n\n\n\n\n\n\nCOND: &gt; 0\nForecasted catches - enter one line per number of fixed forecast year catch\n\n\n2012 1 1 1200\nYear & Season & Fleet & Catch or F value\n\n\n2013 1 1 1200\nYear & Season & Fleet & Catch or F value\n\n\n-9999 0 0 0\nIndicates end of inputted catches to read\n\n\n999\nEnd of Input"
  },
  {
    "objectID": "qmds/html.html#including-a-new-fleet-in-the-forecast",
    "href": "qmds/html.html#including-a-new-fleet-in-the-forecast",
    "title": "RAW HTML CONTENT",
    "section": "6.2 Including a New Fleet in the Forecast",
    "text": "6.2 Including a New Fleet in the Forecast\nAs of v.3.30.16 users can have a forecast fleet without catches during the modeled period. Previously, fleets in the forecast period were required to have input catches at some amount during the modeled period. SS3 now has capability to have a fleet with no input catches during the modeled period that could be used as a fleet during the forecast."
  },
  {
    "objectID": "qmds/html.html#benchmark-calculations",
    "href": "qmds/html.html#benchmark-calculations",
    "title": "RAW HTML CONTENT",
    "section": "6.3 Benchmark Calculations",
    "text": "6.3 Benchmark Calculations\nThis feature of SS3 is designed to calculate an equilibrium fishing rate intended to serve as a proxy for the fishing rate that would provide maximum sustainable yield (FMSY). Then in the forecast module these fishing rates can be used in the projections.\nFour reference points can be calculated by SS3. The first is the estimate of FMSY within the model, while the other reference points use proxies or an alternative estimated point.\n\nFMSY: Search for the F that produces maximum equilibrium (e.g., dead catch).\nFSPR: Search for the F that produces spawning biomass per recruit this is a specific fraction, termed SPRtarget, of spawning biomass per recruit under unfished conditions. Note that this is in relative terms so it does not take into account the spawner-recruit relationship.\nFBtarget: Search for the F that produces an absolute spawning biomass that is a specified fraction, termed relative biomass target, of the unfished spawning biomass. Note that this is in absolute terms so takes into account the spawner-recruit relationship.\nF0.10: Search for the F that produces a slope in yield per recruit, dY/dF, that is 10% of the slope at the origin. Note that with SS3, this option is mutually exclusive with FBtarget. Only one will be calculated and the one that is calculated can serve as the proxy for FMSY and forecasting. The F0.1 search can fail if bycatch fleets are used and the bycatch setting includes the fleet’s catch in the catch to be optimized.\n\n\n6.3.0.1 Estimation\n\nEach of the potential reference points is calculated by searching across a range of F multiplier levels, calculating equilibrium biomass and catch at that F, using Newton-Raphson method to calculate a better F multiplier value, and iterating a fixed number of times to achieve convergence on the desired level.\n\n\n6.3.0.2 Calculations\n\nThe calculation of equilibrium biomass and catch uses the same code that is used to calculate the virgin conditions and the initial equilibrium conditions. This equilibrium calculation code takes into account all morph, timing, biology, selectivity, and movement conditions as they apply while doing the time series calculations. You can verify this by running SS3 to calculate FMSY then hardwire initial F to equal this value, use the F_method approach 2 so each annual F is equal to FMSY and then set forecast F to be the same FMSY. Then run SS3 without estimation and no recruitment deviations. You should see that the population has an initial equilibrium abundance equal to BMSY and stays at this level during the time series and forecast.\n\n\n6.3.0.3 Catch Units\n\nFor each fleet, SS3 always calculates catch in terms of biomass (mt) and numbers (1000s) for encountered (selected) catch, dead catch, and retained catch. These three categories differ only when some fleets have discarding or are designated as a bycatch fleet. SS3 uses total dead catch biomass as the quantity that is principally reported and the quantity that is optimized when searching for FMSY. The quantity \"dead catch\" may occasionally be referred to as \"yield\".\n\n\n6.3.0.4 Biomass Units\n\nThe principle measure of fish abundance, for the purpose of reference point calculation, is female reproductive output. This is referred to as SSB (spawning stock biomass) and sometimes just \"B\" because the typical user settings have one unit of reproductive output (fecundity) per kg of mature female biomass. So when the output label says BMSY, this is actually the female reproductive output at the proxy for FMSY.\n\n\n6.3.0.5 Fleet Allocation\n\nAn important concept for the reference point calculation is the allocation of fishing rate among fleets. Internally, this is benchmark years relative F (Bmark_relF(\\(f,s\\))) and it is the fraction of the F multiplier assigned to each fleet, \\(f\\) and season, \\(s\\). The value, F_multiplier * Bmark_relF(\\(f,s\\)), is the F level for a particular fleet in a particular season and for the age that has a selectivity of 1.0. Other ages will have different F values according to their selectivity.\n\nThe Bmark_relF values can be calculated by SS3 from a range of years specified in the input for Benchmark Years or it can be set to be the same as the Forecast_RelF, which in turn can be based on a range of years or can be input as a set of fixed values.\nThe biology years selected for the Bmark_relF calculations can have an effect on the standard deviation estimated, even in models with no time-varying biology. It is recommended to use the first model year except when biology is time-varying.Note that when biology is time-varying, the fecundity vector, which is used to calculate SSB, will be updated in every iteration for every year that has time-varying biology.\nNote that for Bycatch Fleets, the F’s calculated by application of Bmark_relF for a bycatch fleet can be overridden by a F value calculated from a range of years or a fixed F value that is input by the user. If such an override is selected for a bycatch fleet, that F value is not adjusted by changes to the F multiplier. This allows the user to treat a bycatch fleet as a constant background F while the optimal F for other fleets is sought. Also for bycatch fleets, there is user control for whether or not the dead catch from the bycatch fleet is included in the total dead catch that is optimized when searching for FMSY.\n\n\n\n6.3.0.6 Virgin vs. Unfished\n\nThe concept of unfished spawning biomass, SSB_unf, is important to the reference points calculations. Unfished spawning biomass can be potentially different than virgin spawning biomass, SSB_virgin.\n\nVirgin spawning biomass is calculated from the parameter values associated with the start year of the model configuration and it serves as the basis from which the population model starts and the basis for calculation of stock depletion.\nUnfished spawning biomass can be calculated for any year or range of years, so can change over time as R0, steepness, or biological parameters change.\nIn the reference points calculation, the Benchmark Years input specifies the range of time over which various quantities are averaged to calculate the reference points. For biology, selectivity, F’s, and movement the values being averaged are the year-specific derived quantities. But for the stock-recruitment parameters (R0 and steepness), the parameter values themselves are averaged over time.\nDuring the time series or forecast, the current year’s unfished spawning output (SSB_unf) is used as the basis for the spawner-recruitment curve against which deviations from the spawner-recruitment curve are applied. So, if R0 is made time-varying, then the spawner-recruit curve itself is changed. However, if the regime shift parameter is time-varying, then this is an offset from the spawner-recruitment curve and not a change in the curve itself. Changes in R0 will change year-specific reference points and change the expected value for annual recruitments, but changes in regime shift parameter only change the expected value for annual recruitments.\nIn reporting the time series of depletion level, the denominator can be based on virgin spawning output (SSB_virgin) or BMSY. Note that BMSY is based on unfished spawning output (SSB_unf) for the specified range of Benchmark years, not on SSB_virgin."
  },
  {
    "objectID": "qmds/html.html#forecast-recruitment-adjustment",
    "href": "qmds/html.html#forecast-recruitment-adjustment",
    "title": "RAW HTML CONTENT",
    "section": "6.4 Forecast Recruitment Adjustment",
    "text": "6.4 Forecast Recruitment Adjustment\nRecruitment during the forecast years sometimes needs to be set at a level other than that determined by the spawner-recruitment curve. One way to do this is by an environmental or block effect on the regime shift parameter. A more straightforward approach is now provided by the special forecast recruitment feature described here. There are 4 options provided for this feature. These are:\n\n0 = Do nothing: this is the default and will invoke no special treatment for the forecast recruitments.\n1 = Multiplier on spawner-recruitment: the expected recruitment from the stock recruitment relationship is multiplied by this factor.\n\nThis is a multiplier, so null effect comes from a value of 1.0.\nThe order of operations is to apply the SRR, then the regime effect, then this special forecast effect, then bias adjustment, then the deviations.\nIn the spawner recruit output of the report.sso there are 4 recruitment values stored.\n\n2 = Multiplier on virgin recruitment: The virgin recruitment is multiplied by this factor.\n\nThis is a multiplier, so null effect comes from a value of 1.0.\nThe order of operations is to apply any environmental or block effects to R0, then apply the special forecast effect, then bias adjustment, then the deviations.\nNote that environmental or block effects on R0 are rare and are different than environment or block effects on the regime parameter.\n\n3 = Mean recent recruitment: calculate the mean recruitment and use it during the forecast period.\n\nNote that bias adjustment is not applied to this mean because the values going into the mean have already been bias adjusted.\n\n\nThis feature affects the expected recruitment in all years after the last year of the main recruitment deviations. This means that if the last year of main recruitment deviations is before end year, then the last few recruitments, termed \"late\", are also affected by this forecast option. For example, option 3 would allow you to set the last 2 years of the time series and all forecast years to have recruitment equal to the mean recruitment for the last 10 years of the main recruitment era."
  },
  {
    "objectID": "qmds/html.html#overview-of-data-file",
    "href": "qmds/html.html#overview-of-data-file",
    "title": "RAW HTML CONTENT",
    "section": "7.1 Overview of Data File",
    "text": "7.1 Overview of Data File\n\nDimensions (years, ages, number of fleets, number of surveys, etc.)\nFleet and survey names, timing, etc.\nCatch data (biomass or numbers)\nDiscard totals or rate data\nMean body weight or mean body length data\nLength composition set-up\nLength composition data\nAge composition set-up\nAge imprecision definitions\nAge composition data\nMean length-at-age or mean bodyweight-at-age data\nGeneralized size composition (e.g., weight frequency) data\nEnvironmental data\nTag-recapture data\nStock composition (e.g., morphs identified by otolith microchemistry) data\nSelectivity observations (new placeholder, not yet implemented)"
  },
  {
    "objectID": "qmds/html.html#units-of-measure",
    "href": "qmds/html.html#units-of-measure",
    "title": "RAW HTML CONTENT",
    "section": "7.2 Units of Measure",
    "text": "7.2 Units of Measure\nThe normal units of measure are as follows:\n\nCatch biomass - metric tons\nBody weight - kilograms\nBody length - usually in centimeters, weight-at-length parameters must correspond to the units of body length and body weight\nSurvey abundance - any units if catchability (Q) is freely scaled; metric tons or thousands of fish if Q has a quantitative interpretation\nOutput biomass - metric tons\nNumbers - thousands of fish, because catch is in metric tons and body weight is in kilograms\nSpawning biomass - metric tons of mature females if eggs/kg = 1 for all weights; otherwise has units that are based on the user-specified fecundity"
  },
  {
    "objectID": "qmds/html.html#time-units",
    "href": "qmds/html.html#time-units",
    "title": "RAW HTML CONTENT",
    "section": "7.3 Time Units",
    "text": "7.3 Time Units\n\nSpawning:\n\nHappens once per year at a specified date (in real months, 1.0 - 12.99). To create multiple spawning events per year, change the definition of a year (e.g., call a 6 month period a \"year\" when there are two spawning events about 6 months apart).\n\nRecruitment:\n\nOccurs at specified recruitment events that occur at user-specified dates (in real months, 1.0 - 12.99).\nThere can be one to many recruitment events across a year; each producing a platoon as a portion of the total recruitment.\nA settlement platoon enters the model at age 0 if settlement is between the time of spawning and the end of the year; it enters at age 1 if settlement is after the first of the year; these ages at settlement can be overridden in the settlement setup.\n\nTiming\n\nAll fish advance to the next older integer age on January 1, no matter when they were born during the year. Consult with your ageing lab to assure consistent interpretation.\n\nParameters:\n\nTime-varying parameters are allowed to change annually, not seasonally.\nRates like growth and mortality are per year.\n\n\n\n7.3.1 Seasons\nSeasonal quantities in the model are calculated and treated in the following methods:\n\nSeasons are the time step during which constant rates apply.\nCatch and discard amounts are per season and F is calculated per season.\nThe year can have just one annual season, or be subdivided into seasons of unequal length.\nSeason duration is input in real months (1.0 - 12.99) and is converted into fractions of an annum. Annual rate values are multiplied by the per annum season duration.\nIf the sum of the input season durations do not equal 12.0, then the input durations are divided by the total duration input to rescale season duration to equal 1.0.\nAllows for a special situation in which the year could be only 0.25 in duration (e.g., seasons as years) so that spawning and time-varying parameters can occur more frequently. Other durations are also possible (e.g., where a model year is really only 6 months). Note that real month inputs should always have the range 1.0 - 12.99, even in cases where a model year is not a true year. See the continuous seasonal recruitment section for more information on how to set up models where the model year duration is different than a true year.\n\n\n\n7.3.2 Subseasons and Timing of Events\n\n\n\nThe treatment of subseasons in SS3 provide more precision in the timing of events compared to earlier model versions. In early versions, v.3.24 and before, there was effectively only two subseasons per season because the age-length-key (ALK) for each observation used the mid-season mean length-at-age and spawning occurred at the beginning of a specified season.\nTime steps can be broken into subseason and the ALK can be calculated multiple times over the course of a year:\n\n\n\n\nALK\nALK*\nALK*\nALK\nALK*\nALK\n\n\n\n\nSubseason 1\nSubseason 2\nSubseason 3\nSubseason 4\nSubseason 5\nSubseason 6\n\n\nALK* only re-calculated when there is a survey that subseason\n\n\n\n\n\nEven number (min = 2) of subseasons per season (regardless of season duration):\n\nTwo subseasons will mimic v.3.24\nSpecifying more subseasons will give finer temporal resolution, but will slow the model down, the effect of which is mitigated by only calculating growth as needed.\n\nSurvey timing is now cruise-specific and specified in units of months (e.g., April 15 = 4.5; possible inputs are 1.0 to 12.99).\n\nss_trans.exe will convert year, season in v.3.24 format to year, real month in v.3.30 format.\n\nSurvey integer season and spawn integer season assigned at run time based on real month and season duration(s).\nGrowth and the age-length-key (ALK) is calculated at the beginning and mid-season or when there is a defined subseason with data observation.\nFishery body weight uses mid-subseason growth.\nSurvey body weight and size composition is calculated using the nearest subseason.\nReproductive output now has specified spawn timing (in months fraction) and interpolates growth to that timing.\nSurvey numbers calculated at cruise survey timing using \\(e^{-z}\\).\nContinuous Z for entire season. Same as applied in version v.3.24."
  },
  {
    "objectID": "qmds/html.html#terminology",
    "href": "qmds/html.html#terminology",
    "title": "RAW HTML CONTENT",
    "section": "7.4 Terminology",
    "text": "7.4 Terminology\nThe term COND appears in the \"Typical Value\" column of this documentation (it does not actually appear in the model files), it indicates that the following section is omitted except under certain conditions, or that the factors included in the following section depend upon certain conditions. In most cases, the description in the definition column is the same as the label output to the ss_new files."
  },
  {
    "objectID": "qmds/html.html#model-dimensions",
    "href": "qmds/html.html#model-dimensions",
    "title": "RAW HTML CONTENT",
    "section": "7.5 Model Dimensions",
    "text": "7.5 Model Dimensions\n\n\n\n\n\n\n\n\nValue\nDescription\n\n\n\n\n#V3.30.XX.XX\nModel version number. This is written by SS3 in the new files and a good idea to keep updated in the input files.\n\n\n\n\n\n\n#C data using new survey\nData file comment. Must start with #C to be retained then written to top of various output files. These comments can occur anywhere in the data file, but must have #C in columns 1-2.\n\n\n\n\n\n\n1971\nStart year\n\n\n2001\nEnd year\n\n\n1\nNumber of seasons per year\n\n\n12\nVector with the number of months in each season. These do not need to be integers. Note: If the sum of this vector is close to 12.0, then it is rescaled to sum to 1.0 so that season duration is a fraction of a year. If the sum is not equal to 12.0, then the entered values are summed and rescaled to 1. So, with one season per year and 3 months per season, the calculated season duration will be 0.25, which allows a quarterly model to be run as if quarters are years. All rates in SS3 are calculated by season (growth, mortality, etc.) using annual rates and season duration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nThe number of subseasons. Entry must be even and the minimum value is 2. This is for the purpose of finer temporal granularity in calculating growth and the associated age-length key.\n\n\n\n\n\n\n\n\n\n\n\n1.5\n\nSpawning month; spawning biomass is calculated at this time of year (1.5 means January 15) and used as basis for the total recruitment of all settlement events resulting from this spawning.\n\n\n\n\n\n\n\n\n\n\n2\nNumber of sexes:\n\n\n\n1 = current one sex, ignore fraction female input in the control file;\n\n\n\n2 = current two sex, use fraction female in the control file; and\n\n\n\n-1 = one sex and multiply the spawning biomass by the fraction female in the control file.\n\n\n20\nNumber of ages. The value here will be the plus-group age. SS3 starts at age 0.\n\n\n1\nNumber of areas\n\n\n2\nTotal number of fishing and survey fleets (which now can be in any order)."
  },
  {
    "objectID": "qmds/html.html#fleet-definitions",
    "href": "qmds/html.html#fleet-definitions",
    "title": "RAW HTML CONTENT",
    "section": "7.6 Fleet Definitions",
    "text": "7.6 Fleet Definitions\n\nThe\n\ncatch data input has been modified to improve the user flexibility to add/subtract fishing and survey fleets to a model set-up. The fleet setup input is transposed so each fleet is now a row. Previous versions (SS3 v.3.24 and earlier) required that fishing fleets be listed first followed by survey only fleets. In SS3 all fleets have the same status within the model structure and each has a specified fleet type (except for models that use tag recapture data, this will be corrected in future versions). Available types are; catch fleet, bycatch only fleet, or survey.\n\n\n\n\nInputs that define the fishing and survey fleets:\n\n\n\n\n2\nNumber of fleets which includes survey in any order\n\n\nFleet Type\nTiming\nArea\nCatch Units\nCatch Mult.\nFleet Name\n\n\n1\n-1\n1\n1\n0\nFISHERY1\n\n\n3\n1\n1\n2\n0\nSURVEY1\n\n\n\n\n\n7.6.0.1 Fleet Type\n\nDefine the fleet type (e.g., fishery fleet, survey fleet):\n\n1 = fleet with input catches;\n2 = bycatch fleet (all catch discarded) and invoke extra input for treatment in equilibrium and forecast;\n3 = survey: assumes no catch removals even if associated catches are specified below. If you would like to remove survey catch set fleet type to option = 1 with specific month timing for removals (defined below in the \"Timing\" section); and\n4 = predator (M2) fleet that adds additional mortality without a fleet F (added in version 3.30.18). Ideal for modeling large mortality events such as fish kills or red tide. Requires additional long parameter lines for a second mortality component (M2) in the control file after the natural mortality/growth parameter lines (entered immediately after the fraction female parameter line).\n\n\n\n\n\n\n7.6.0.2 Timing\n\nTiming for data observations:\n\nFishery options:\n\n-1: catch is treated as if it occurred over the whole season. Stock Synthesis may change the CPUE data to occur in the middle of the season if it is specified otherwise (i.e., the CPUE observations may have a different month in the data_echo.ss_new file). A user can override this assumption for specific data observations (e.g., length or age) by specifying a month. This option works well for fisheries where fishing is spread throughout the year.\n1: The fleet timing is not used and only the month value associated with each observation is relevant. This option works well for pulse fisheries that occurs over a small subset of months.\n\nSurvey option, 1: The fleet timing is not used and only the month value associated with each observation is relevant (e.g., month specification in the indices of abundance or the month for composition data). This input should always be used for surveys.\n\n\n\n7.6.0.3 Area\n\nAn integer value indicating the area in which a fleet operates.\n\n\n7.6.0.4 Catch Units\n\nIgnored for survey fleets, their units are read later:\n\n1 = biomass (in metric tons); and\n2 = numbers (thousands of fish).\n\nSee Units of Measure for more information.\n\n\n\n\n\n7.6.0.5 Catch Multiplier\n\nInvokes use of a catch multiplier, which is then entered as a parameter in the mortality-growth parameter section. The estimated value or fixed value of the catch multiplier is used to adjust the observed catch:\n\n0 = No catch multiplier used; and\n1 = Apply a catch multiplier which is defined as an estimable parameter in the control file after the cohort growth deviation in the biology parameter section. The model’s estimated retained catch will be multiplied by this factor before being compared to the observed retained catch.\n\nA catch multiplier can be useful when trying to explore historical unrecorded catches or ongoing illegal and unregulated catches. The catch multiplier is a full parameter line in the control file and has the ability to be time-varying."
  },
  {
    "objectID": "qmds/html.html#bycatch-fleets",
    "href": "qmds/html.html#bycatch-fleets",
    "title": "RAW HTML CONTENT",
    "section": "7.7 Bycatch Fleets",
    "text": "7.7 Bycatch Fleets\nThe option to include bycatch fleets was introduced in SS3 v.3.30.10. This is an optional input and if no bycatch is to be included in to the catches this section can be ignored.\nA fishing fleet is designated as a bycatch fleet by indicating that its fleet type is 2. A bycatch fleet creates a fishing mortality, same as a fleet of type 1, but a bycatch fleet has all catch discarded so the input value for retained catch is ignored. However, an input value for retained catch is still needed to indicate that the bycatch fleet was active in that year and season. A catch multiplier cannot be used with bycatch fleets because catch multiplier works on retained catch. SS3 will expect that the retention function for this fleet will be set in the selectivity section to type 3, indicating that all selected catch is discarded dead. It is necessary to specify a selectivity pattern for the bycatch fleet and, due to generally lack of data, to externally derive values for the parameters of this selectivity.\nAll catch from a bycatch fleet is discarded, so one option to use a discard fleet is to enter annual values for the amount (not proportion) that is discarded in each time step. However, it is uncommon to have such data for all years. An alternative approach that has been used principally in the U.S. Gulf of Mexico is to input a time series of effort data for this fleet in the survey section (e.g., effort is a \"survey\" of F, for example, the shrimp trawl fleet in the Gulf of Mexico catches and discards small finfish and an effort time series is available for this fleet) and to input in the discard data section an observation for the average discard over time using the super year approach. Another use of bycatch fleet is to use it to estimate effect of an external source of mortality, such as a red tide event. In this usage there may be no data on the magnitude of the discards and SS3 will then rely solely on the contrast in other data to attempt to estimate the magnitude of the red tide kill that occurred. The benefit of doing this as a bycatch fleet, and not a block on natural mortality, is that the selectivity of the effect can be specified.\nBycatch fleets are not expected to be under the same type of fishery management controls as the retained catch fleets included in the model. This means that when SS3 enters into the reference point equilibrium calculations, it would be incorrect to have SS3 re-scale the magnitude of the F for the bycatch fleet as it searches for the F that produces, for example, F35%. Related issues apply to the forecast. Consequently, a separate set of controls is provided for bycatch fleets (defined below). Input is required for each fleet designated as fleet type = 2.\nIf a fleet above was set as a bycatch fleet (fleet type = 2), the following line is required:\n\n\n\n\nBycatch fleet input controls:\n\n\n\n\na:\nb:\nc:\nd:\ne:\nf:\n\n\nFleet Index\nInclude in MSY\nFmult\nF or First Year\nLast Year\nNot used\n\n\n2\n2\n3\n1982\n2010\n0\n\n\n\n\nThe above example set-up defines one fleet (fleet number 2) as a bycatch fleet with the dead catch from this fleet to not be included in the search for MSY (b: Include in MSY = 2). The level of F from the bycatch fleet in reference point and forecast is set to the mean (c: Fmult = 3) of the estimated F for the range of years from 1982-2010.\n\n7.7.0.1 Fleet Index\n\nFleet number for which to include bycatch catch. Fleet number is assigned within the model based on the order of listed fleets in the Fleet Definition section. If there are multiple bycatch fleets, then a line for each fleet is required in the bycatch section.\n\n\n7.7.0.2 Include in MSY\n\nThe options are:\n\n1 = deadfish in MSY, ABC, and other benchmark and forecast output; and\n2 = omit from MSY and ABC (but still include the mortality).\n\n\n\n7.7.0.3 Fmult\n\nThe options are:\n\n1 = F multiplier scales with other fleets;\n2 = bycatch F constant at input value in column d; and\n3 = bycatch F from range of years input in columns d and e.\n\n\n\n7.7.0.4 F or First Year\n\nThe specified F or first year for the bycatch fleet.\n\n\n7.7.0.5 F or Last Year\n\nThe specified F or last year for the bycatch fleet.\n\n\n7.7.0.6 Not Used\n\nThis column is not yet used and is reserved for future features.\n\n\n7.7.0.7 Bycatch Fleet Usage Instructions and Warnings\n\nWhen implementing a bycatch fleet, changes to both the data and control file are needed.\nThe needed changes to the data file are:\n\nFleet type - set to value of 2.\nSet bycatch fleet controls per information above.\nCatch input - you must enter a positive value for catch in each year/season that you want a bycatch calculated. The entered value of catch will be ignored by SS3, it is just a placeholder to invoke creating an F.\n\nInitial equilibrium - you may want to enter the bycatch amount as retained catch for the initial equilibrium year because there is no option to enter initial equilibrium discard in the discard section.\n\nDiscard input - It is useful, but not absolutely necessary, to enter the amount of discard to assist SS3 in estimating the F for the bycatch fleet.\nSurvey input - It is useful, but not absolutely necessary, to enter the effort time series by the bycatch fleet to assist SS3 in estimating the annual changes in F for the bycatch fleet.\n\nThe needed changes to the control file are:\n\nThe F method must be set to 2 in order for SS3 to estimate F with having information on retained catch.\nSelectivity -\n\nA selectivity pattern must be specified and fixed (or estimated if composition data is provided).\nThe discard column of selectivity input must be set to a value of 3 to cause all catch to be discarded.\n\n\nIn version 3.30.14 it was identified that there can be an interaction between the use of bycatch fleets and the search for the \\(F_{0.1}\\) reference point which may results in the search failing. Changes to the search feature were implemented to make the search more robust, however, issue may still be encountered. In these instances it is recommended to not select the \\(F_{0.1}\\) reference point calculation in the forecast file."
  },
  {
    "objectID": "qmds/html.html#predator-fleets",
    "href": "qmds/html.html#predator-fleets",
    "title": "RAW HTML CONTENT",
    "section": "7.8 Predator Fleets",
    "text": "7.8 Predator Fleets\nIntroduced in v.3.30.18, a predator fleet provides the capability to define an entity as a predator that adds additional mortality (M2, i.e., the predation mortality) to the base natural mortality. This new capability means that previous use of bycatch fleets to mimic predators (or fish kills, e.g., due to red tide) will no longer be necessary. The problem with using a bycatch fleet as a predator was that it still created an \"F\" that was included in the reporting of total F even if the bycatch was not included in the MSY search.\nFor each fleet that is designated as a predator, a new parameter line is created in the mortality-growth (MGparm) section in the control file. This parameter will have the label M2_pred1, where the \"1\" is the index for the predator (not the index of the fleet being used as a predator). More than one predator can be included. If the model has &gt; 1 season, it is normal to expect M2 to vary seasonally. Therefore, only if the number of seasons is greater than 1, follow each M2 parameter with number of season parameters to provide the seasonal multipliers. These are simple multipliers times M2, so at least one of these needs to have a non-estimated value. The set of multipliers can be used to set M2 to only operate in one season if desired. If there is more than one predator fleet, each will have its own seasonal multipliers. If there is only 1 season in the model, then no multiplier lines are included.\nThree types of data relevant to M2 can be input:\n\nTotal kill (as discard in the data file): M2 is a component of Z, so M2/Z can be used to calculate the amount of the total kill that is attributable to M2. This is completely analogous to calculating catch for the fishing fleets. The total kill (e.g., consumption) is output to the discard array. If data on the total kill by the M2 predator is available, it can be input as observed “discard” for this fleet and thus included in the total logL to estimate the magnitude of the M2 parameter.\nPredator effort (as a survey index in the data file): M2 is a rate analogous to F, so the survey of F approach (survey units = 2) can be used to input predator abundance as an indicator of the “effort” that produced the M2. Like all surveys, this survey of M2 will also need a Q specification. Note that in the future we can explore improved options for this Q.\nPredated age-length composition (as length or age composition data in the data file): M2 \"eats\" the modeled fish, so gut contents or other sources may have size and/or age composition data which may be input to estimate selectivity of the M2 source.\n\nWith the input of data on the time series of total kill or predator effort, it should be possible to estimate annual deviations around the base M2 for years with data. If the M2 time series is instead driven by environmental data, then also including data on kill or effort can provide a means to view consistency between the environmental time series and the additional data sets. Output of M2 is found in a report.sso section labeled predator (M2). In the example below, the M2 seasonal multiplier was defined to have random deviations by year. This allowed multipliers plus M2 itself to closely match the input consumption amounts (288 mt of consumption per season, the fit can be examined by looking at the discard output report)."
  },
  {
    "objectID": "qmds/html.html#catch",
    "href": "qmds/html.html#catch",
    "title": "RAW HTML CONTENT",
    "section": "7.9 Catch",
    "text": "7.9 Catch\n\nAfter\n\nreading the fleet-specific indicators, a list of catch values by fleet and season are read in by the model. The format for the catches is year and season that the catch is attributed to, fleet, a catch value, and a year-specific catch standard error. Only positive catches need to be entered, so there is no need for records corresponding to all years and fleets. To include an equilibrium catch value for a fleet and season, the year should be noted as -999. For each non-zero equilibirum catch value included, a short paramter line is required in the initial F section of the control file.\n\nThere\n\nis no longer a need to specify the number of records to be read; instead the list is terminated by entering a record with the value of -9999 in the year field. The updated list based approach extends throughout the data file (e.g., catch, length- and age-composition data), the control file (e.g., lambdas), and the forecast file (e.g., total catch by fleet, total catch by area, allocation groups, forecasted catch).\nIn addition, it is possible to collapse the number of seasons. So, if a season value is greater than the number of seasons for a particular model, that catch is added to the catch for the final season. This is one way to easily collapse a seasonal model into an annual model. The alternative option is to the use of season = 0. This will cause SS3 to distribute the input value of catch equally among the number of seasons. SS3 assumes that catch occurs continuously over seasons and hence is not specified as month in the catch data section. However, all other data types will need to be specified by month.\nThe format for a 2 season model with 2 fisheries looks like the table below. Example is sorted by fleet, but the sort order does not matter. In data.ss_new, the sort order is fleet, year, season.\n\n\n\n\nCatches by year, season for every fleet:\n\n\n\n\nYear\nSeason\nFleet\nCatch\nCatch SE\n\n\n-999\n1\n1\n56\n0.05\n\n\n-999\n2\n1\n62\n0.05\n\n\n1975\n1\n1\n876\n0.05\n\n\n1975\n2\n1\n343\n0.05\n\n\n...\n...\n...\n...\n...\n\n\n-999\n1\n2\n55\n0.05\n\n\n-999\n2\n2\n22\n0.05\n\n\n1975\n1\n2\n555\n0.05\n\n\n1975\n2\n2\n873\n0.05\n\n\n...\n...\n...\n...\n...\n\n\n-9999\n0\n0\n0\n0\n\n\n\n\n\nCatch can be in terms of biomass or numbers for each fleet, but cannot be mixed within a fleet.\nCatch is retained catch (aka landings). If there is discard also, then it is handled in the discard section below. This is the recommended setup which results in a model estimated retention curve based upon the discard data (specifically discard composition data). However, there may be instances where the data do not support estimation of retention curves. In these instances catches can be specified as all dead (retained + discard estimates).\nIf there are challenges to estimating discards within the model, catches can be input as total dead without the use of discard data and retention curves.\nIf there is reason to believe that the retained catch values underestimate the true catch, then it is possible in the retention parameter set up to create the ability for the model to estimate the degree of unrecorded catch. However, this is better handled with the new catch multiplier option."
  },
  {
    "objectID": "qmds/html.html#indices",
    "href": "qmds/html.html#indices",
    "title": "RAW HTML CONTENT",
    "section": "7.10 Indices",
    "text": "7.10 Indices\nIndices are data that are compared to aggregate quantities in the model. Typically the index is a measure of selected fish abundance, but this data section also allows for the index to be related to a fishing fleet’s F, or to another quantity estimated by the model. The first section of the \"Indices\" setup contains the fleet number, units, error distribution, and whether additional output (SD Report) will be written to the Report file for each fleet that has index data.\n\n\n\n\nCatch-per-unit-effort (CPUE) and Survey Abundance Observations:\n\n\n\n\nFleet/\n\nError\n\n\n\nSurvey\nUnits\nDistribution\nSD Report\n\n\n1\n1\n0\n0\n\n\n2\n1\n0\n0\n\n\n...\n...\n...\n...\n\n\n\n\n\n\n\n\n7.10.0.1 Units\n\nThe options for units for input data are:\n\n0 = numbers;\n1 = biomass;\n2 = F; and\n\nNote the F option can only be used for a fishing fleet and not for a survey, even if the survey selectivity is mirrored to a fishing fleet. The values of these effort data are interpreted as proportional to the level of the fishery F values. No adjustment is made for differentiating between continuous F values versus exploitation rate values coming from Pope’s approximation. A normal error structure is recommended so that the input effort data are compared directly to the model’s calculated F, rather than to log(F). The resultant proportionality constant has units of 1/Q where Q is the catchability coefficient.\n\n\n&gt;=30\n\nspecial survey types. These options bypass the calculation of survey selectivity so the no selectivity parameter are required and age/length selectivity pattern should be set as 0. A catchability parameter line in the control file will be required for each special survey. The expected values for these types are:\n\n30 = spawning biomass/output (e.g., for an egg and larvae survey);\n31 = exp(recruitment deviation), useful for environmental index affecting recruitment;\n32 = spawning biomass * exp(recruitment deviation), for a pre-recruit survey occurring before density-dependence;\n33 = recruitment, age-0 recruits;\n34 = depletion (spawning biomass/virgin spawning biomass);\n\nSpecial survey option 34 automatically adjusts phases of parameters. To use the depletion survey approach, the user will need to make the following revisions to the SS3 data file: 1) add a new survey fleet, 2) define the survey type as option 34, 3) add two depletion survey data points, and initial unfished set equal to 1 for an unfished modeled year and one for a later year with the depletion estimates, 4) set the input CV value for each survey data point to a low value (e.g., 0.0001) to force the model to fit these data, and in the control file 5) add the survey to the control file in the Q set-up and selectivity sections with float set to 0 with parameter value set to 0.\nThere are options for additional control over this in the control file catchability setup section under the \"link information\" column where:\n\n0 = add 1 to phases of all parameters. Only R0 active in new phase 1. Mimics the default option of previous model versions;\n1 = only R0 active in phase 1. Then finish with no other parameters becoming active; useful for data-limited draws of other fixed parameters. Essentially, this option allows SS3 to mimic DB-SRA; and\n2 = no phase adjustments, can be used when profiling on fixed R0.\n\nWarning: the depletion survey approach has not been tested on multiple area models. This approach may present challenges depending upon the dynamics within each area.\n\n35 = survey of a deviation vector (\\(e(survey(y)) = f(parm\\_dev(k,y))\\)), can be used for an environmental time-series with soft linkage to the index. The selected deviation vector is specified in Q section of the control file. The index of the deviation vector to which the index is related is specified in the 2nd column of the Q setup table (see Catchability).\n\n\n\n\n7.10.0.2 Error Distribution\n\nThe options for error distribution form are:\n\n-1 = normal error;\n0 = lognormal error; and\n&gt;0 = Student’s t-distribution in log space with degrees of freedom equal to this value. For DF&gt;30, results will be nearly identical to that for lognormal distribution. A DF value of about 4 gives a fat-tail to the distribution. The standard error values entered in the data file must be the standard error in loge space.\n\nAbundance indices typically have a lognormal error structure with units of standard error of loge(index). If the variance of the observations is available only as a coefficient of variation (CV), then the value of standard error can be approximated as \\(\\sqrt{(log_e(1+(CV)^2))}\\) where the CV is the standard error of the observation divided by the mean value of the observation.\nFor the normal error structure, the entered values for standard error are interpreted directly as a standard error in arithmetic space and not as a CV. Thus switching from a lognormal to a normal error structure forces the user to provide different values for the standard error input in the data file.\nIf the data exist as a set of normalized Z-scores, you can assert a lognormal error structure after entering the data as exp(Z-score) because it will be logged by SS3. Preferably, the Z-scores would be entered directly and the normal error structure would be used.\n\n\n7.10.0.3 Enable SD Report\n\nIndices with SD Report enabled will have the expected values for their historical values appear in the ss.std and ss.cor files. The default value is for this option is 0.\n\n0 = SD Report not enabled for this index; and\n1 = SD Report enabled for this index.\n\n\n\n7.10.0.4 Data Format\n\n\n\n\n\n\nYear\nMonth\nFleet/Survey\nObservation\nSE\n\n\n\n\n1991\n7\n3\n80000\n0.056\n\n\n1995\n7.2\n3\n65000\n0.056\n\n\n...\n...\n...\n...\n...\n\n\n2000\n7.1\n3\n42000\n0.056\n\n\n-9999\n0\n0\n0\n0\n\n\n\n\n\nFor fishing fleets, CPUE is defined in terms of retained catch (biomass or numbers).\nFor fishery independent surveys, retention/discard is not defined so CPUE is implicitly in terms of total CPUE.\nIf a survey has its selectivity mirrored to that of a fishery, only the selectivity is mirrored so the expected CPUE for this mirrored survey does not use the retention curve (if any) for the fishing fleet.\nIf the fishery or survey has time-varying selectivity, then this changing selectivity will be taken into account when calculating expected values for the CPUE or survey index.\nYear values that are before start year or after end year are excluded from model, so the easiest way to include provisional data in a data file is to put a negative sign on its year value.\nDuplicate survey observations for the same year are not allowed.\nObservations that are to be included in the model but not included in the negative log likelihood need to have a negative sign on their fleet ID. Previously the code for not using observations was to enter the observation itself as a negative value. However, that old approach prevented use of a Z-score environmental index as a “survey.” This approach is best for single or select years from an index rather than an approach to remove a whole index. Removing a whole index from the model should be done through the use of lambdas at the bottom of the control file which will eliminate the index from model fitting.\nObservations can be entered in any order, except if the super-year feature is used.\nSuper-periods are turned on and then turned back off again by putting a negative sign on the season. Previously, super-periods were started and stopped by entering -9999 and the -9998 in the SE field. See the Data Super-Period section of this manual for more information.\nIf the statistical analysis used to create the CPUE index of a fishery has been conducted in such a way that its inherent size/age selectivity differs from the size/age selectivity estimated from the fishery’s size and age composition, then you may want to enter the CPUE as if it was a separate survey and with a selectivity that differs from the fishery’s estimated selectivity. The need for this split arises because the fishery size and age composition should be derived through a catch-weighted approach (to appropriately represent the removals by the fishery) and the CPUE should be derived through an area-weighted approach to better serve as a survey of stock abundance."
  },
  {
    "objectID": "qmds/html.html#discard",
    "href": "qmds/html.html#discard",
    "title": "RAW HTML CONTENT",
    "section": "7.11 Discard",
    "text": "7.11 Discard\nIf discard is not a feature of the model specification, then just a single input is needed:\n\n\n\n\n0\nNumber of fleets with discard observations\n\n\n\n\n\n\nIf discard is being used, the input syntax is:\n\n\n\n\n1\nNumber of fleets with discard observations\n\n\n\n\nFleet\nUnits\nError Distribution\n\n\n1\n2\n-1\n\n\nYear\nMonth\nFleet\nObservation\nStandard Error\n\n\n1980\n7\n1\n0.05\n0.25\n\n\n1991\n7\n1\n0.10\n0.25\n\n\n-9999\n0\n0\n0\n0\n\n\n\n\nNote that although the user must specify a month for the observed discard data, the unit for discard data is in terms of a season rather than a specific month. So, if using a seasonal model, the input month values must corresponding to some time during the correct season. The actual value will not matter because the discard amount is calculated for the entirety of the season. However, discard length or age observations will be treated by entered observation month.\n\n7.11.0.1 Discard Units\n\nThe options are:\n\n1 = values are amount of discard in either biomass or numbers according to the selection made for retained catch;\n2 = values are fraction (in biomass or numbers) of total catch discarded, biomass/number selection matches that of retained catch; and\n3 = values are in numbers (thousands) of fish discarded, even if retained catch has units of biomass.\n\n\n\n7.11.0.2 Discard Error Distribution\n\nThe four options for discard error are:\n\n&gt;0 = degrees of freedom for Student’s t-distribution used to scale mean body weight deviations. Value of error in data file is interpreted as CV of the observation;\n0 = normal distribution, value of error in data file is interpreted as CV of the observation;\n-1 = normal distribution, value of error in data file is interpreted as standard error of the observation;\n-2 = lognormal distribution, value of error in data file is interpreted as standard error of the observation in log space; and\n-3 = truncated normal distribution (new with SS3 v.3.30, needs further testing), value of error in data file is interpreted as standard error of the observation. This is a good option for low observed discard rates.\n\n\n\n7.11.0.3 Discard Notes\n\n\n\nYear values that are before start year or after end year are excluded from model, so the easiest way to include provisional data in a data file is to put a negative sign on its year value.\nNegative value for fleet causes it to be included in the calculation of expected values, but excluded from the log likelihood.\nZero (0.0) is a legitimate discard observation, unless lognormal error structure is used.\nDuplicate discard observations from a fleet for the same year are not allowed.\nObservations can be entered in any order, except if the super-period feature is used.\nNote that in the control file you will enter information for retention such that 1-retention is the amount discarded. All discard is assumed dead, unless you enter information for discard mortality. Retention and discard mortality can be either size-based or age-based (new with SS3 v.3.30).\n\n\n\n7.11.0.4 Cautionary Note\n\nThe use of CV as the measure of variance can cause a small discard value to appear to be overly precise, even with the minimum standard error of the discard observation set to 0.001. In the control file, there is an option to add an extra amount of variance. This amount is added to the standard error, not to the CV, to help correct this problem of underestimated variance."
  },
  {
    "objectID": "qmds/html.html#mean-body-weight-or-length",
    "href": "qmds/html.html#mean-body-weight-or-length",
    "title": "RAW HTML CONTENT",
    "section": "7.12 Mean Body Weight or Length",
    "text": "7.12 Mean Body Weight or Length\nThis is the overall mean body weight or length across all selected sizes and ages. This may be useful in situations where individual fish are not measured but mean weight is obtained by counting the number of fish in a specified sample, e.g., a 25 kg basket.\n\n\n\n\nMean Body Weight Data Section:\n\n\n\n\n1\nUse mean body size data (0/1)\n\n\nCOND &gt; 0:\n\n\n30\nDegrees of freedom for Student’s t-distribution used to evaluate mean body weight\n\n\n\ndeviation.\n\n\nYear\nMonth\nFleet\nPartition\nType\nObservation\nCV\n\n\n1990\n7\n1\n0\n1\n4.0\n0.95\n\n\n1990\n7\n1\n0\n1\n1.0\n0.95\n\n\n-9999\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n7.12.0.1 Partition\n\nMean weight data and composition data require specification of what group the sample originated from (e.g., discard, retained, discard + retained).\n\n0 = whole catch in units of weight (discard + retained);\n1 = discarded catch in units of weight; and\n2 = retained catch in units of weight.\n\n\n\n7.12.0.2 Type\n\nSpecify the type of data:\n\n1 = mean length; and\n2 = mean body weight.\n\n\n\n7.12.0.3 Observation - Units\n\nUnits must correspond to the units of body weight, normally in kilograms, (or mean length in cm). The expected value of mean body weight (or mean length) is calculated in a way that incorporates effect of selectivity and retention.\n\n\n7.12.0.4 Error\n\nError is entered as the CV of the observed mean body weight (or mean length)"
  },
  {
    "objectID": "qmds/html.html#population-length-bins",
    "href": "qmds/html.html#population-length-bins",
    "title": "RAW HTML CONTENT",
    "section": "7.13 Population Length Bins",
    "text": "7.13 Population Length Bins\nThe first part of the length composition section sets up the bin structure for the population. These bins define the granularity of the age-length key and the coarseness of the length selectivity. Fine bins create smoother distributions, but a larger and slower running model. First read a single value to select one of three population length bin methods, then any conditional input for options 2 and 3:\n\n\n\n\n1\nUse data bins to be read later. No additional input here.\n\n\n\n\n2\ngenerate from bin width min max, read next:\n\n\n\n2\nBin width\n\n\n10\nLower size of first bin\n\n\n82\nLower size of largest bin\n\n\n\n\n\n\n3\nRead 1 value for number of bins, and then read vector of bin boundaries\n\n\n\n37\nNumber of population length bins to be read\n\n\n10 12 14 ... 82\nVector containing lower edge of each population size bin\n\n\n\n\n\n7.13.0.1 Notes\n\nThere are some items for users to consider when setting up population length bins:\n\nFor option 2, bin width should be a factor of min size and max size. For options 2 and 3, the data length bins must not be wider than the population length bins and the boundaries of the bins do not have to align. The transition matrix between population and data length bins is output to echoinput.sso.\nThe mean size at settlement (virtual recruitment age) is set equal to the min size of the first population length bin.\nWhen using more, finer population length bins, the model will create smoother length selectivity curves and smoother length distributions in the age-length key, but run more slowly (more calculations to do).\nThe mean weight-at-length, maturity-at-length and size-selectivity are based on the mid-length of the population bins. So these quantities will be rougher approximations if broad bins are defined.\nProvide a wide enough range of population size bins so that the mean body weight-at-age will be calculated correctly for the youngest and oldest fish. If the growth curve extends beyond the largest size bin, then these fish will be assigned a length equal to the mid-bin size for the purpose of calculating their body weight.\nWhile exploring the performance of models with finer bin structure, a potentially pathological situation has been identified. When the bin structure is coarse (note that some applications have used 10 cm bin widths for the largest fish), it is possible for a selectivity slope parameter or a retention parameter to become so steep that all of the action occurs within the range of a single size bin. In this case, the model will see zero gradient of the log likelihood with respect to that parameter and convergence will be hampered.\nA value read near the end of the starter.ss file defines the degree of tail compression used for the age-length key, called ALK tolerance. If this is set to 0.0, then no compression is used and all cells of the age-length key are processed, even though they may contain trivial (e.g., 1 e-13) fraction of the fish at a given age. With tail compression of, say 0.0001, the model, at the beginning of each phase, will calculate the min and max length bin to process for each age of each morphs ALK and compress accordingly. Depending on how many extra bins are outside this range, you may see speed increases near 10-20%. Large values of ALK tolerance, say 0.1, will create a sharp end to each distribution and likely will impede convergence. It is recommended to start with a value of 0 and if model speed is an issue, explore values greater than 0 and evaluate the trade-off between model estimates and run time. The user is encouraged to explore this feature."
  },
  {
    "objectID": "qmds/html.html#length-composition-data-structure",
    "href": "qmds/html.html#length-composition-data-structure",
    "title": "RAW HTML CONTENT",
    "section": "7.14 Length Composition Data Structure",
    "text": "7.14 Length Composition Data Structure\n\n\n\nEnter a code to indicate whether or not length composition data will be used:\n\n\n\n\n1\nUse length composition data (0/1/2)\n\n\n\nIf the value 0 is entered, then skip all length related inputs below and skip to the age data setup section. If value 1 is entered, all data weighting options for composition data apply equally to all partitions within a fleet. If value 2 is entered, then the data weighting options are applied by the partition specified. Note that the partitions must be entered in numerical order within each fleet.\nIf the value for fleet is negative, then the vector of inputs is copied to all partitions (0 = combined, 1 = discard, and 2 = retained) for that fleet and all higher numbered fleets. This as a good practice so that the user controls the values used for all fleets.\n\n\n\nExample table of length composition settings when \"Use length composition data\" = 1 (where here\n\n\n\nthe first fleet has multinomial error structure with no associated parameter, and the second fleet\n\n\n\nuses Dirichlet-multinomial structure):\n\n\n\nMin.\nConstant\nCombine\n\nComp.\n\nMin.\n\n\n\nTail\nadded\nmales &\nCompress.\nError\nParam.\nSample\n\n\n\nCompress.\nto prop.\nfemales\nBins\nDist.\nSelect\nSize\n\n\n\n0\n0.0001\n0\n0\n0\n0\n0.1\n\n\n\n0\n0.0001\n0\n0\n1\n1\n0.1\n\n\n\n\n\n\n\nExample table of length composition settings when \"Use length composition data\" = 2 (where here\n\n\n\nthe -1 in the fleet column applies the first parameter to all partitions for fleet 1 while fleet 2 has\n\n\n\nseparate parameters for discards and retained fish):\n\n\n\n\n\nMin.\nConstant\nCombine\n\nComp.\n\nMin.\n\n\n\n\n\nTail\nadded\nmales &\nCompress.\nError\nParam.\nSample\n\n\n\nFleet\nPartition\nCompress.\nto prop.\nfemales\nBins\nDist.\nSelect\nSize\n\n\n\n-1\n0\n0\n0.0001\n0\n0\n1\n1\n0.1\n\n\n\n2\n1\n0\n0.0001\n0\n0\n1\n2\n0.1\n\n\n\n2\n2\n0\n0.0001\n0\n0\n1\n3\n0.1\n\n\n\n...\n\n\n\n\n\n\n\n\n\n\n\n-9999\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n7.14.0.1 Minimum Tail Compression\n\nCompress tails of composition until observed proportion is greater than this value; negative value causes no compression; Advise using no compression if data are very sparse, and especially if the set-up is using age composition within length bins because of the sparseness of these data.\n\n\n7.14.0.2 Added Constant to Proportions\n\nConstant added to observed and expected proportions at length and age to make logL calculations more robust. Tail compression occurs before adding this constant. Proportions are renormalized to sum to 1.0 after constant is added.\n\n\n7.14.0.3 Combine Males & Females\n\nCombine males into females at or below this bin number. This is useful if the sex determination of very small fish is doubtful so allows the small fish to be treated as combined sex. If Combine Males & Females &gt; 0, then add males into females for bins 1 through this number, zero out the males, set male data to start at the first bin above this bin. Note that Combine Males & Females &gt; 0 is entered as a bin index, not as the size associated with that bin. Comparable option is available for age composition data.\n\n\n7.14.0.4 Compress Bins\n\nThis option allows for the compression of length or age bins beyond a specific length or age by each data source. As an example, a value of 5 in the compress bins column would condense the final five length bins for the specified data source.\n\n\n7.14.0.5 Composition Error Distribution\n\nThe options are:\n\n0 = Multinomial Error;\n1 = Dirichlet Multinomial Error (linear); and\n\nThe Dirichlet Multinomial Error distribution requires the addition of a parameter lines for the natural log of the effective sample size multiplier (\\(\\theta\\)) at the end of the selectivity parameter section in the control file. See the Dirichlet parameter in the control file for information regarding setup.\nThe Parameter Select option needs be used to specify which data sources should be weighted together or separate.\n\n2 = Dirichlet Multinomial Error (saturation).\n\nThis parameterization of the Dirichlet-multinomial Error has not been tested, so this option should be used with caution. The Dirichlet Multinomial Error data weighting approach will calculate the effective sample size based on equation 12 from Thorson et al. (2017) where the estimated parameter will now be in terms of \\(\\beta\\). The application of this method should follow the same steps detailed above for option 1.\n\n3 = Multivariate Tweedie.\n\n\n\n7.14.0.6 Parameter Select\n\nValue that indicates the groups of composition data for estimation of the Dirichlet or Multivariate Tweedie parameter for weighting composition data.\n\n0 = Default; and\n1-N = Only used for the Dirichlet option. Set to a sequence of numbers from 1 to N where N is the total number of combinations of fleet and age/length. That is, if you have 3 fleets with length data, but only 2 also have age data, you would have values 1 to 3 in the length comp setup and 4 to 5 in the age comp setup. You can also have a data weight that is shared across fleets by repeating values in Parameter Select. Note that there can be no skipped numbers in the sequence from 1 to N, otherwise the model will exit on error when reading in the input files.\n\n\n\n7.14.0.7 Minimum Sample Size\n\nThe minimum value (floor) for all sample sizes. This value must be at least 0.001. Conditional age-at-length data may have observations with sample sizes less than 1. SS3 v.3.24 had an implicit minimum sample size value of 1.\n\n\n7.14.0.8 Additional information on Dirichlet Parameter Number and Effective Sample Sizes\n\nIf the Dirichlet-multinomial error distribution is selected, indicate here which of a list of Dirichlet-multinomial parameters will be used for this fleet. So each fleet could use a unique Dirichlet-multinomial parameter, or all could share the same, or any combination of unique and shared. The requested number of Dirichlet-multinomial parameters are specified as parameter lines in the control file immediately after the selectivity parameter section. Please note that age-compositions Dirichlet-multinomial parameters are continued after length-compositions, so a model with one fleet and both data types would presumably require two new Dirichlet-multinomial parameters.\nThe Dirichlet estimates the effective sample size as \\(N_{eff}=\\frac{1}{1+\\theta}+\\frac{N\\theta}{1+\\theta}\\) where \\(\\theta\\) is the estimated parameter and \\(N\\) is the input sample size. Stock Synthesis estimates the log of the Dirichlet-multinomial parameter such that \\(\\hat{\\theta}_{\\text{fishery}} = e^{-0.6072} = 0.54\\) where assuming \\(N=100\\) for the fishery would result in an effective sample size equal to 35.7.\nThis formula for effective sample size implies that, as the Stock Synthesis parameter ln(DM_theta) goes to large values (i.e., 20), then the adjusted sample size will converge to the input sample size. In this case, small changes in the value of the ln(DM_theta) parameter has no action, and the derivative of the negative log-likelihood is zero with respect to the parameter, which means the Hessian will be singular and cannot be inverted. To avoid this non-invertible Hessian when the ln(DM_theta) parameter becomes large, turn it off while fixing it at the high value. This is equivalent to turning off down-weighting of fleets where evidence suggests that the input sample sizes are reasonable.\nFor additional information about the Dirichlet-multinomial please see Thorson et al. (2017) and the detailed Data Weighting section."
  },
  {
    "objectID": "qmds/html.html#length-composition-data",
    "href": "qmds/html.html#length-composition-data",
    "title": "RAW HTML CONTENT",
    "section": "7.15 Length Composition Data",
    "text": "7.15 Length Composition Data\nComposition data can be entered as proportions, numbers, or values of observations by length bin based on data expansions.\nThe data bins do not need to cover all observed lengths. The selection of data bin structure should be based on the observed distribution of lengths and the assumed growth curve. If growth asymptotes at larger lengths, having additional length bins across these sizes may not contribute information to the model and may slow model run time. Additionally, the lower length bin selection should be selected such that, depending on the size selection, to allow for information on smaller fish and possible patterns in recruitment. While set separately users should ensure that the length and age bins align. It is recommended to explore multiple configurations of length and age bins to determine the impact of this choice on model estimation.\nSpecify the length composition data as:\n\n\n\n\n28\nNumber of length bins for data\n\n\n\n\n26 28 30 ... 80\nVector of length bins associated with the length data\n\n\n\n\nNote: the vector of length bins above will aggregate data from outside the range of values as follows:\n\n\n\n\n\nbin 1\nbin 2\nbin 3\n...\nbin 27\nbin 28\n\n\n\n\n\n\n\nbin vector\n26\n28\n30\n...\n78\n80\n\n\n\n\n\nbin contains\n0–27.99\n28–29.99\n30–30.99\n...\n78–79.99\n80+\n\n\n\n\n\n\n\nExample of a single length composition observation:\n\n\n\n\n\n\n\n\n\nYear\nMonth\nFleet\nSex\nPartition\nNsamp\ndata vector\n\n\n1986\n1\n1\n3\n0\n20\n&lt;female then male data&gt;\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n-9999\n0\n0\n0\n0\n0\n&lt;0 repeated for each element of the data vector above&gt;\n\n\n\n\n\n7.15.0.1 Sex\n\nIf model has only one sex defined in the set-up, all observations must have sex set equal to 0 or 1 and the data vector by year will equal the number of the user defined data bins. This also applies to the age data.\nIn a 2 sex model, the data vector always has female data followed by male data, even if only one of the two sexes has data that will be used. The below description applies to a 2 sex model:\n\nSex = 0 means combined male and female (must already be combined and information placed in the female portion of the data vector) (male entries must exist for correct data reading, then will be ignored).\nSex = 1 means female only (male entries must exist for correct data reading, then will be ignored).\nSex = 2 means male only (female entries must exist and will be ignored after being read).\nSex = 3 means data from both sexes will be used and they are scaled so that they together sum to 1.0; i.e. sex ratio is preserved.\n\n\n\n7.15.0.2 Partition\n\nPartition indicates samples from either discards,retained, or combined.\n\n0 = combined;\n1 = discard; and\n2 = retained.\n\n\n\n7.15.0.3 Excluding Data\n\n\n\nIf the value of year is negative, then that observation is not transferred into the working array. This feature is the easiest way to include observations in a data file but not to use them in a particular model scenario.\nIf the value of fleet in the length or age composition observed data line is negative, then the observation is processed and its expected value and log likelihood are calculated, but this log likelihood is not included in the total log likelihood. This feature allows the user to see the fit to a provisional observation without having that observation affect the model.\n\n\n\n7.15.0.4 Note\n\nWhen processing data to be input into SS3, all observed fish of sizes smaller than the first bin should be added to the first bin and all observed fish larger than the last bin should be condensed into the last bin.\nThe number of length composition data lines no longer needs to be specified in order to read the length (or age) composition data. Starting in SS3 3.30, the model will continue to read length composition data until an pre-specified exit line is read. The exit line is specified by entering -9999 at the end of the data matrix. The -9999 indicates to the model the end of length composition lines to be read.\nEach observation can be stored as one row for ease of data management in a spreadsheet and for sorting of the observations. However, the 6 header values, the female vector and the male vector could each be on a separate line because ADMB reads values consecutively from the input file and will move to the next line as necessary to read additional values.\nThe composition observations can be in any order and replicate observations by a year for a fleet are allowed (unlike survey and discard data). However, if the super-period approach is used, then each super-periods’ observations must be contiguous in the data file."
  },
  {
    "objectID": "qmds/html.html#age-composition-option",
    "href": "qmds/html.html#age-composition-option",
    "title": "RAW HTML CONTENT",
    "section": "7.16 Age Composition Option",
    "text": "7.16 Age Composition Option\nThe age composition section begins by reading the number of age bins. If the value 0 is entered for the number of age bins, then skips reading the bin structure and all reading of other age composition data inputs.\n\n\n\n\n17\nNumber of age bins; can be equal to 0 if age data are not used; do not include a vector of agebins if the number of age bins is set equal to 0.\n\n\n\n\n\n\n\n7.16.1 Age Composition Bins\nIf a positive number of age bins is read, then reads the bin definition next.\n\n\n\n\n1 2 3 ... 20 25\nVector of ages\n\n\n\n\n\n\nThe bins are in terms of observed age (here age) and entered as the lower edge of each bin. Each ageing imprecision definition is used to create a matrix that translates true age structure into age structure. The first and last age’ bins work as accumulators. So in the example any age 0 fish that are caught would be assigned to the age = 1 bin.\n\n\n7.16.2 Ageing Error\nHere, the capability to create a distribution of age (e.g., age with possible bias and imprecision) from true age is created. One or many ageing error definitions can be created. For each, the model will expect an input vector of mean age and a vector of standard deviations associated with the mean age.\n\n\n\n\n2\nNumber of ageing error matrices to generate\n\n\n\n\n\n\n\n\n\n\n\n\nExample with no bias and very little uncertainty at age:\n\n\nAge-0\nAge-1\nAge-2\n...\nMax Age\n\n\n\n-1\n-1\n-1\n...\n-1\n#Mean Age\n\n\n0.001\n0.001\n0.001\n...\n0.001\n#SD\n\n\n\n\n\n\n\n\n\n\nExample with no bias and some uncertainty at age:\n\n\n0.5\n1.5\n2.5\n...\nMax Age + 0.5\n#Mean Age\n\n\n0.5\n0.65\n0.67\n...\n4.3\n#SD Age\n\n\n\n\n\n\n\n\n\n\nExample with bias and uncertainty at age:\n\n\n0.5\n1.4\n2.3\n...\nMax Age + Age Bias\n#Mean Age\n\n\n0.5\n0.65\n0.67\n...\n4.3\n#SD Age\n\n\n\n\nIn principle, one could have year or laboratory specific matrices for ageing error. For each matrix, enter a vector with mean age for each true age; if there is no ageing bias, then set age equal to true age + 0.5. Alternatively, -1 value for mean age means to set it equal to true age plus 0.5. The addition of +0.5 is needed so that fish will get assigned to the intended integer age. The length of the input vector is equal to the population maximum age plus one (0-max age), with the first entry being for age 0 fish and the last for fish of population maximum age even if the maximum age bin for the data is lower than the population maximum age. The following line is a a vector with the standard deviation of age for each true age with a normal distribution assumption.\nThe model is able to create one ageing error matrix from parameters, rather than from an input vector. The range of conditions in which this new feature will perform well has not been evaluated, so it should be considered as a preliminary implementation and subject to modification. To invoke this option, for the selected ageing error vector, set the standard deviation of ageing error to a negative value for age 0. This will cause creation of an ageing error matrix from parameters and any age or size-at-age data that specify use of this age error pattern will use this matrix. Then in the control file, add a full parameter line below the cohort growth deviation parameter (or the movement parameter lines if used) in the mortality growth parameter section. These parameters are described in the control file section of this manual.\nCode for ageing error calculation can be found in SS_miscfxn.tpl, search for function \"get_age_age\" or \"SS_Label_Function 45\".\n\n\n7.16.3 Age Composition Specification\nIf age data are included in the model, the following set-up is required, similar to the length data section.\n\n\n\nSpecify bin compression and error structure for age composition data for each fleet:\n\n\n\n\nMin.\nConstant\nCombine\n\nComp.\n\nMin.\n\n\nTail\nadded\nmales &\nCompress.\nError\nParam.\nSample\n\n\nCompress.\nto prop.\nfemales\nBins\nDist.\nSelect\nSize\n\n\n0\n0.0001\n1\n0\n0\n0\n1\n\n\n0\n0.0001\n1\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\nSpecify method by which length bin range for age obs will be interpreted:\n\n\n1\nBin method for age data\n\n\n\n1 = value refers to population bin index\n\n\n\n2 = value refers to data bin index\n\n\n\n3 = value is actual length (which must correspond to population length bin\n\n\n\nboundary)\n\n\n\n\n\n\n\n\n\nAn example age composition observation:\n\n\nYear\nMonth\nFleet\nSex\nPartition\nAge Err\nLbin lo\nLbin hi\nNsamp\nData Vector\n\n\n1987\n1\n1\n3\n0\n2\n-1\n-1\n79\n&lt;enter data values&gt;\n\n\n-9999\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\nSyntax for Sex, Partition, and data vector are same as for length. The data vector has female values then male values, just as for the length composition data.\n\n7.16.3.1 Age Error\n\nAge error (Age Err) identifies which ageing error matrix to use to generate expected value for this observation.\n\n\n7.16.3.2 Lbin Low and Lbin High\n\nLbin lo and Lbin hi are the range of length bins that this age composition observation refers to. Normally these are entered with a value of -1 and -1 to select the full size range. Whether these are entered as population bin number, length data bin number, or actual length is controlled by the value of the length bin range method above.\n\nEntering value of 0 or -1 for Lbin lo converts Lbin lo to 1;\nEntering value of 0 or -1 for Lbin hi converts Lbin hi to Maxbin;\nIt is strongly advised to use the -1 codes to select the full size range. If you use explicit values, then the model could unintentionally exclude information from some size range if the population bin structure is changed.\nIn reporting to the comp_report.sso, the reported Lbin_lo and Lbin_hi values are always converted to actual length.\n\n\n\n7.16.3.3 Excluding Data\n\nAs with the length composition data, a negative year value causes the observation to not be read into the working matrix, a negative value for fleet causes the observation to be included in expected values calculation, but not in contribution to total log likelihood, a negative value for month causes start-stop of super-period."
  },
  {
    "objectID": "qmds/html.html#conditional-age-at-length",
    "href": "qmds/html.html#conditional-age-at-length",
    "title": "RAW HTML CONTENT",
    "section": "7.17 Conditional Age-at-Length",
    "text": "7.17 Conditional Age-at-Length\nUse of conditional age-at-length will greatly increase the total number of age composition observations and associated model run time but there can be several advantages to inputting ages in this fashion. First, it avoids double use of fish for both age and size information because the age information is considered conditional on the length information. Second, it contains more detailed information about the relationship between size and age so provides stronger ability to estimate growth parameters, especially the variance of size-at-age. Lastly, where age data are collected in a length-stratified program, the conditional age-at-length approach can directly match the protocols of the sampling program.\nHowever, simulation research has shown that the use of conditional age-at-length data can result in biased growth estimates in the presence of unaccounted for age-based movement when length-based selectivity is assumed (H. H. Lee et al. 2017), when other age-based processes (e.g., mortality) are not accounted for (H. Lee et al. 2019), or based on the age sampling protocol (Piner, Lee, and Maunder 2016). Understanding how data are collected (e.g., random, length-conditioned samples) and the biology of the stock is important when using conditional age-at-length data for a fleet.\nIn a two sex model, it is best to enter these conditional age-at-length data as single sex observations (sex = 1 for females and = 2 for males), rather than as joint sex observations (sex = 3). Inputting joint sex observations comes with a more rigid assumption about sex ratios within each length bin. Using separate vectors for each sex allows 100% of the expected composition to be fit to 100% observations within each sex, whereas with the sex = 3 option, you would have a bad fit if the sex ratio were out of balance with the model expectation, even if the observed proportion at age within each sex exactly matched the model expectation for that age. Additionally, inputting the conditional age-at-length data as single sex observations isolates the age composition data from any sex selectivity as well.\nConditional age-at-length data are entered within the age composition data section and can be mixed with marginal age observations for other fleets of other years within a fleet. To treat age data as conditional on length, Lbin_lo and Lbin_hi are used to select a subset of the total size range. This is different than setting Lbin_lo and Lbin_hi both to -1 to select the entire size range, which treats the data entered on this line within the age composition data section as marginal age composition data.\n\n\n\n\n\n\nAn example conditional age-at-length composition observations:\n\n\nYear\nMonth\nFleet\nSex\nPartition\nAge Err\nLbin lo\nLbin hi\nNsamp\nData Vector\n\n\n1987\n1\n1\n1\n0\n2\n10\n10\n18\n&lt;data values&gt;\n\n\n1987\n1\n1\n1\n0\n2\n12\n12\n24\n&lt;data values&gt;\n\n\n1987\n1\n1\n1\n0\n2\n14\n14\n16\n&lt;data values&gt;\n\n\n1987\n1\n1\n1\n0\n2\n16\n16\n30\n&lt;data values&gt;\n\n\n-9999\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\nIn this example observation, the age data is treated as on being conditional on the 2 cm length bins of 10–11.99, 12–13.99, 14–15.99, and 16–17.99cm. If there are no observations of ages for a specific sex within a length bin for a specific year, that entry may be omitted."
  },
  {
    "objectID": "qmds/html.html#mean-length-or-body-weight-at-age",
    "href": "qmds/html.html#mean-length-or-body-weight-at-age",
    "title": "RAW HTML CONTENT",
    "section": "7.18 Mean Length or Body Weight-at-Age",
    "text": "7.18 Mean Length or Body Weight-at-Age\nThe model also accepts input of mean length-at-age or mean body weight-at-age. This is done in terms of observed age, not true age, to take into account the effects of ageing imprecision on expected mean size-at-age. If the value of the Age Error column is positive, then the observation is interpreted as mean length-at-age. If the value of the Age Error column is negative, then the observation is interpreted as mean body weight-at-age and the abs(Age Error) is used as Age Error.\n\n\n\n\n1\nUse mean size-at-age observation (0 = none, 1 = read data matrix)\n\n\nAn example observation:\n\n\n\n\n\n\n\nAge\n\nData Vector\nSample Size\n\n\nYr\nMonth\nFleet\nSex\nPart.\nErr.\nIgnore\n(Female - Male)\n(Female - Male)\n\n\n1989\n7\n1\n3\n0\n1\n999\n&lt;Mean Size values&gt;\n&lt;Sample Sizes&gt;\n\n\n...\n\n\n\n\n\n\n\n\n\n\n-9999\n0\n0\n0\n0\n0\n0\n0 0 0 0 0 0 0\n0 0 0 0 0 0 0\n\n\n\n\n\n7.18.0.1 Note\n\n\n\nNegatively valued mean size entries with be ignored in fitting. This feature allows the user to see the fit to a provisional observation without having that observation affect the model.\nA number of fish value of 0 will cause mean size value to be ignored in fitting. This feature allows the user to see the fit to a provisional observation without having that observation affect the model.\nNegative value for year causes observation to not be included in the working matrix. This feature is the easiest way to include observations in a data file but not to use them in a particular model scenario.\nEach sexes’ data vector and N fish vector has length equal to the number of age bins.\nThe \"Ignore\" column is not used (set aside for future options) but still needs to have default values in that column (any value).\nWhere age data are being entered as conditional age-at-length and growth parameters are being estimated, it may be useful to include a mean length-at-age vector with nil emphasis to provide another view on the model’s estimates.\nAn experiment that may be of interest might be to take the body weight-at-age data and enter it to the model as empirical body weight-at-true age in the wtatage.ss file, and to contrast results to entering the same body weight-at-age data here and to attempt to estimate growth parameters, potentially time-varying, that match these body weight data.\nIf using mean size-at-age data, please see the lambda usage notes regarding issues for model fitting depending upon other data within the model."
  },
  {
    "objectID": "qmds/html.html#environmental-data",
    "href": "qmds/html.html#environmental-data",
    "title": "RAW HTML CONTENT",
    "section": "7.19 Environmental Data",
    "text": "7.19 Environmental Data\nThe model accepts input of time series of environmental data. Parameters can be made to be time-varying by making them a function of one of these environmental time series. In v.3.30.16 the option to specify the centering of environmental data by either using the mean of the by mean and the z-score.\n\n\n\n\nParameter values can be a function of an environmental data series:\n\n\n\n\n1\nNumber of environmental variables\n\n\nThe environmental data can be centered by subtracting the mean and dividing by stdev (z-score, -1) or\n\n\nby subtracting the mean fo the environmental variable (-2) based on the year column value.\n\n\nCOND &gt; 0 Example of 2 environmental observations:\n\n\n\nYear\nVariable\nValue\n\n\n\n1990\n1\n0.10\n\n\n\n1991\n1\n0.15\n\n\n\n-1\n1\n1\n\n\n\n-2\n2\n1\n\n\n\n-9999\n0\n0\n\n\n\n\nThe final two lines in the example above indicate in that variable series 1 will be centered by subtracting the mean and dividing by the standard deviation (indicated by the -1 value in the year column). The environmental variable series 2 will be centered by subtracting the mean of the time series (indicated by the -2 value in the year column). The input in the \"value\" column for both of the final two lines specifying the centering of the time series is ignored by the model. The control file also will need to be modified to in the long parameter line column \"env-var\" for the selected parameter. This feature was added in v.3.30.16.\n\n7.19.0.1 Note\n\n\n\nAny years for which environmental data are not read are assigned a value of 0.0. None of the current link functions contain a link parameter that acts as an offset. Therefore, you should subtract the mean from your data. This lessens the problem with missing observations, but does not eliminate it. A better approach for dealing with missing observations is to use a different approach for the environmental effect on the parameter. Set up the parameter to have random deviations for all years, then enter the zero-centered environmental information as a special survey of type 35 and set up the catchability of that survey to be a link to the deviation vector. This is a more complex approach, but it is superior in treatment of missing values and superior in allowing for error in the environmental relationship.\nUsers can assign environmental conditions for the initial equilibrium year by including environmental data for one year before the start year. However, this works only for recruitment parameters, not biology or selectivity parameters.\nEnvironmental data can be read for up to 100 years after the end year of the model. Then, if the recruitment-environment link has been activated, the future recruitments will be influenced by any future environmental data. This could be used to create a future \"regime shift\" by setting historical values of the relevant environmental variable equal to zero and future values equal to 1, in which case the magnitude of the regime shift would be dictated by the value of the environmental linkage parameter. Note that only future recruitment and growth can be modified by the environmental inputs; there are no options to allow environmentally-linked selectivity in the forecast years."
  },
  {
    "objectID": "qmds/html.html#generalized-size-composition-data",
    "href": "qmds/html.html#generalized-size-composition-data",
    "title": "RAW HTML CONTENT",
    "section": "7.20 Generalized Size Composition Data",
    "text": "7.20 Generalized Size Composition Data\nThe generalized approach to size composition information was designed initially to provide a means to include weight frequency data. However, the uses are broader, such as allowing for size composition data with different data bins. The user can define as many generalized size composition methods as necessary.\n\nEach method has a specified number of bins.\nEach method has \"units\" so the frequencies can be in units of biomass or numbers.\nEach method has \"scale\" so the bins can be in terms of weight or length (including ability to convert bin definitions in pounds or inches to kg or cm).\nThe composition data is input as females then males, just like all other composition data in SS3. In a two-sex model, the new composition data can be combined sex, single sex, or both sex.\nThe generalized size composition data can be from the combined discard and retained, discard only or retained only.\nThere are two options for treating fish that in population size bins are smaller than the smallest size frequency bin.\n\nOption 1: By default, these fish are excluded (unlike length composition data where the small fish are automatically accumulated up into the first bin.)\nOption 2: If the first size bin is given as a negative value, then accumulation is turned on and the absolute value of the entered value is used as the lower edge of the first size bin.\n\n\n\n\n\n\nExample entry:\n\n\n\n\n2\n\nNumber (N) of size frequency methods to be read. If this value is 0, then omit all entries below. A value of -1 (or any negative value) triggers expanded optional inputs below that allow for either Dirichlet of two parameter Multivariate (MV) Tweedie likelihood for fitting these data.\n\n\nCOND &lt; 0 - Number of size frequency\n\n\n2\nNumber of size frequency methods to read\n\n\nEND COND &lt; 0\n\n\n25 15\nNumber of bins per method\n\n\n2 2\nUnits per each method (1 = biomass, 2 = numbers)\n\n\n3 3\nScale per each method (1 = kg, 2 = lbs, 3 = cm, 4 = inches)\n\n\n1e-9 1e-9\nMin compression to add to each observation (entry for each method)\n\n\n2 2\nNumber of observations per weight frequency method\n\n\nCOND &lt; 0 - Number of size frequency\n\n\n1 1\nComposition error structure (0 = multinomial, 1 = Dirichlet using Theta*n, 2 = Dirichlet using beta, 3 = MV Tweedie)\n\n\n1 1\nParameter select consecutive index for Dirichlet or MV Tweedie composition error\n\n\nEND COND &lt; 0\n\n\n\n\n\n\n\n\nThen enter the lower edge of the bins for each method. The two row vectors shown\n\n\n\n\nbelow contain the bin definitions for methods 1 and 2 respectively:\n\n\n\n\n-26\n28\n30\n32\n34\n36\n38\n40\n42\n...\n60\n62\n64\n68\n72\n76\n80\n90\n\n\n\n\n-26\n28\n30\n32\n34\n36\n38\n40\n42\n44\n46\n48\n50\n52\n54\n\n\n\n\n\n\nExample input is shown below. Note that the format is identical to the length composition data, including sex and partition options, except for the addition of the first column, which indicates the size frequency method.\n\n\n\n\n\n\n\n\n\n\nSample\n&lt;composition\n\n\nMethod\nYear\nMonth\nFleet\nSex\nPart\nSize\nfemales then males&gt;\n\n\n1\n1975\n1\n1\n3\n0\n43\n&lt;data&gt;\n\n\n1\n1977\n1\n1\n3\n0\n43\n&lt;data&gt;\n\n\n1\n1979\n1\n1\n3\n0\n43\n&lt;data&gt;\n\n\n1\n1980\n1\n1\n3\n0\n43\n&lt;data&gt;\n\n\n\n\n\n7.20.0.1 Note\n\n\n\nThere is no tail compression for generalized size frequency data.\nSuper-period capability is as for the length and age composition data.\nBy choosing units = 2 and scale = 3 with identical bins and a negative first bin to turn accumulation of small fish on, the size composition method is identical to the length composition method.\nBin boundaries do not need to align with the population length bin boundaries. The model interpolates as necessary.\nSize bins cannot be defined as narrower than the population bin width.\nThe transition matrix can depend upon weight-at-length which differs between sexes and can vary seasonally. Thus, the transition matrix is calculated internally for each sex and each season."
  },
  {
    "objectID": "qmds/html.html#tag-recapture-data",
    "href": "qmds/html.html#tag-recapture-data",
    "title": "RAW HTML CONTENT",
    "section": "7.21 Tag-Recapture Data",
    "text": "7.21 Tag-Recapture Data\nEach released tag group is characterized by an area, time, sex and age at release. Each recapture event is characterized by a time and fleet (since fleets operate in only one area, it is not necessary to specify the area of recapture). Fleets with tagging data must be fishing fleets (e.g., fleet type 1 or 2).\nInside the model, the tagged cohort is apportioned across all growth patterns in a given area at a given time (with options to apportion to only one sex or to both). The tag cohort by growth pattern then behaves according to the movement and mortality of the growth pattern. The number of tagged fish is modeled as a negligible fraction of the total population, so a tagging event does not move fish from an untagged group to a tagged group. Instead, tagged fish are seeded into the population with no impact at all on the total population abundance or mortality.\nPredominant age at release for each tag group must be assigned; this requirement keeps SS3 efficient. By assigning a tag group to a single age rather than distributing it across all possible ages according to the size composition of the release group, the tag group can be tracked as a single cohort through the age by time matrix with minimal overhead to the rest of the model. Tags are released at the beginning of a season and recaptures follow the timing of the fleet that made the recapture.\n\n\n\n\nExample set-up for tagging data:\n\n\n\n\n1\n\nDo tags - 0/1/2. If this value is 0, then omit all entries below.\n\n\n\n\nIf value is 2, read 1 additional input.\n\n\nCOND &gt; 0 All subsequent tag-recapture entries must be omitted if \"Do Tags\" = 0\n\n\n\n3\nNumber of tag groups\n\n\n\n7\nNumber of recapture events\n\n\n\n2\nMixing latency period: N periods to delay before comparing observed\n\n\n\n\nto expected recoveries (0 = release period).\n\n\n\n10\nMax periods (seasons) to track recoveries, after which tags enter\n\n\n\n\naccumulator\n\n\nCOND = 2\n\n\n\n2\nMinimum recaptures. The number of recaptures &gt;= mixperiod must be\n\n\n\n\n&gt;= min tags recaptured specified to include tag group in log likelihood\n\n\n\nRelease Data\n\n\n\nTG\nArea\nYear\nSeason\n&lt;tfill&gt;\nSex\nAge\nN Release\n\n\n\n1\n1\n1980\n1\n999\n0\n24\n2000\n\n\n\n2\n1\n1995\n1\n999\n1\n24\n1000\n\n\n\n3\n1\n1985\n1\n999\n2\n24\n10\n\n\n\nRecapture Data\n\n\n\nTG\n\nYear\n\nSeason\n\nFleet\nNumber\n\n\n\n1\n\n1982\n\n1\n\n1\n7\n\n\n\n1\n\n1982\n\n1\n\n2\n5\n\n\n\n1\n\n1985\n\n1\n\n2\n0\n\n\n\n2\n\n1997\n\n1\n\n1\n6\n\n\n\n2\n\n1997\n\n2\n\n1\n4\n\n\n\n3\n\n1986\n\n1\n\n1\n7\n\n\n\n3\n\n1986\n\n2\n\n1\n5\n\n\n\n\n\n7.21.0.1 Note\n\n\n\nThe release data must be entered in tag group order.\n&lt;tfill&gt; values are place holders and are replaced by program generated values for model time.\nAnalysis of the tag-recapture data has one negative log likelihood component for the distribution of recaptures across areas and another negative log likelihood component for the decay of tag recaptures from a group over time. Note the decay of tag recaptures from a group over time suggests information about mortality is available in the tag-recapture data. More on this is in the control file documentation.\nDo tags option 2 adds an additional input compared to do tags option 1, minimum recaptures. Minimum recaptures allows the user to exclude tag groups that have few recaptures after the mixing period from the likelihood. This may be useful when few tags from a group have been recaptured as an alternative to manually removing the groups with these low numbers of recaptured tags from the tagging data.\nWarning for earlier versions of SS3: A shortcoming in the recapture calculations when also using Pope’s F approach was identified and corrected in version 3.30.14."
  },
  {
    "objectID": "qmds/html.html#stock-morph-composition-data",
    "href": "qmds/html.html#stock-morph-composition-data",
    "title": "RAW HTML CONTENT",
    "section": "7.22 Stock (Morph) Composition Data",
    "text": "7.22 Stock (Morph) Composition Data\nIt is sometimes possible to observe the fraction of a sample that is composed of fish from different stocks. These data could come from genetics, otolith microchemistry, tags, or other means. The growth pattern feature allows definition of cohorts of fish that have different biological characteristics and which are independently tracked as they move among areas. SS3 now incorporates the capability to calculate the expected proportion of a sample of fish that come from different growth patterns, \"morphs\". In the inaugural application of this feature, there was a 3 area model with one stock spawning and recruiting in area 1, the other stock in area 3, then seasonally the stocks would move into area 2 where stock composition observations were collected, then they moved back to their natal area later in the year.\n\n\n\n\nStock composition by growth pattern (morph) data can be entered in as follows:\n\n\n\n\n1\nDo morph composition, if zero, then do not enter any further input below.\n\n\nCOND = 1\n\n\n\n3\nNumber of observations\n\n\n\n2\nNumber of morphs\n\n\n\n0.0001\nMinimum Compression\n\n\n\nYear\nMonth\nFleet\nNull\nNsamp\nData by N Morphs\n\n\n\n1980\n1\n1\n0\n36\n0.4\n0.6\n\n\n\n1981\n1\n1\n0\n40\n0.44\n0.54\n\n\n\n1982\n1\n1\n0\n50\n0.37\n0.63\n\n\n\n\n\n7.22.0.1 Note\n\n\n\nThe number of stocks entered with these data must match the number of growth patterns (morphs) in the control file.\nEach data line for unique observations should enter data for morph 1 first followed sequentially for each morph included in the model.\nThe expected value is combined across sexes. The entered data values will be normalized to sum to one within SS3.\nThe \"null\" flag is included here in the data input section and is a reserved spot for future features.\nNote that there is a specific value of minimum compression to add to all values of observed and expected.\nWarning for earlier versions of SS3: A flaw was identified in the calculation of accumulation by morph. This has been corrected in version 3.30.14. Older versions were incorrectly calculating the catch by morph using the expectation around age-at-length which already was accounting for the accumulation by morph."
  },
  {
    "objectID": "qmds/html.html#selectivity-empirical-data-future-feature",
    "href": "qmds/html.html#selectivity-empirical-data-future-feature",
    "title": "RAW HTML CONTENT",
    "section": "7.23 Selectivity Empirical Data (future feature)",
    "text": "7.23 Selectivity Empirical Data (future feature)\nIt is sometimes possible to conduct field experiments or other studies to provide direct information about the selectivity of a particular length or age relative to the length or age that has peak selectivity, or to have a prior for selectivity that is more easily stated than a prior on a highly transformed selectivity parameter. This section provides a way to input data that would be compared to the specified derived value for selectivity. This is a placeholder at this time, required to include in the data file and will be fully implemented soon.\n\n\n\n\nSelectivity data feature is under development for a future option and is not yet implemented.\n\n\nThe input line still must be specified in as follows:\n\n\n0\nDo data read for selectivity (future option)\n\n\n\n\n\n\n\n\nEnd of Data File\n\n\n\n\n999\n#End of data file marker"
  },
  {
    "objectID": "qmds/html.html#excluding-data-2",
    "href": "qmds/html.html#excluding-data-2",
    "title": "RAW HTML CONTENT",
    "section": "7.24 Excluding Data",
    "text": "7.24 Excluding Data\nData that are before the model start year or greater than the retrospective year are not moved into the internal working arrays at all. So if you have any alternative observations that are used in some model runs and not in others, you can simply give them a negative year value rather than having to comment them out. The first output to data.ss_new has the unaltered and complete input data. Subsequent reports to data.ss_new produce expected values or bootstraps only for the data that are being used. Additional information on bootstrapping is available in Bootstrap Data Files Section.\nData that are to be included in the calculations of expected values, but excluded from the calculation of negative log likelihood, are flagged by use of a negative value for fleet number."
  },
  {
    "objectID": "qmds/html.html#data-super-periods",
    "href": "qmds/html.html#data-super-periods",
    "title": "RAW HTML CONTENT",
    "section": "7.25 Data Super-Periods",
    "text": "7.25 Data Super-Periods\nThe super-period capability allows the user to introduce data that represent a blend across a set of time steps and to cause the model to create an expected value for this observation that uses the same set of time steps. The option is available for all types of data and a similar syntax is used.\nAll super-period observations must be contiguous in the data file. All but one of the observations in the sequence will have a negative value for fleet ID so the data associated with these dummy observations will be ignored. The observed values must be combined outside of the model and then inserted into the data file for the one observation with a positive fleet number.\nSuper-periods are started with a negative value for month, and then stopped with a negative value for month, observations within the super-period are designated with a negative fleet field. The standard error or input sample size field is now used for weighting of the expected values. An error message is generated if the super-period does not contain one observation with a positive fleet field.\nAn expected value for the observation will be computed for each selected time period within the super-period. The expected values are weighted according to the values entered in the standard error (or input sample size) field for all observations except the single observation holding the combined data. The expected value for that year gets a relative weight of 1.0. So in the example below, the relative weights are: 1982, 1.0 (fixed); 1983, 0.85; 1985, 0.4; 1986, 0.4. These weights are summed and rescaled to sum to 1.0, and are output in the echoinput.sso file.\nNot all time steps within the extent of a super-period need be included. For example, in a three season model, a super-period could be set up to combine information from season 2 across 3 years, e.g., skip over the season 1 and season 3 for the purposes of calculating the expected value for the super-period. The key is to create a dummy observation (negative fleet value) for all time steps, except 1, that will be included in the super-period and to include one real observation (positive fleet value; which contains the real combined data from all the specified time steps).\n\n\n\n\nSuper-period example:\n\n\n\n\nYear\nMonth\nFleet\nObs\nSE\nComment\n\n\n1982\n-2\n3\n34.2\n0.3\nStart super-period. This observation has positive fleet value, so is expected to contain combined data from all identified periods of the super-period. The standard error (SE) entered here is use as the SE of the combined observation. The expected value for the survey in 1982 will have a relative weight of 1.0 (default) in calculating the combined expected value.\n\n\n1983\n2\n-3\n55\n0.3\nIn super-period; entered observation is ignored. The expected value for the survey in 1983 will have a relative weight equal to the value in the standard error field (0.3) in calculating the combined expected value.\n\n\n1985\n2\n-3\n88\n0.40\nNote that 1984 is not included in the super-period Relative weight for 1985 is 0.4\n\n\n1986\n-2\n-3\n88\n0.40\nEnd super-period\n\n\n\n\nA time step that is within the time extent of the super-period can still have its own separate observation. In the above example, the survey observation in 1984 could be entered as a separate observation, but it must not be entered inside of the contiguous block of super-period observations. For composition data (which allow for replicate observations), a particular time steps’ observations could be entered as a member of a super-period and as a separate observation.\nThe super-period concept can also be used to combine seasons within a year with multiple seasons. This usage could be preferred if fish are growing rapidly within the year so their effective age selectivity is changing within year as they grow; fish are growing within the year so fishery data collected year round have a broader size-at-age modes than a mid-year model approximation can produce; and it could be useful in situations with very high fishing mortality."
  },
  {
    "objectID": "qmds/html.html#overview-of-control-file",
    "href": "qmds/html.html#overview-of-control-file",
    "title": "RAW HTML CONTENT",
    "section": "8.1 Overview of Control File",
    "text": "8.1 Overview of Control File\nThese listed model features are denoted in the control file in the following order:\n\nNumber of growth patterns and platoons\nDesign matrix for assignment of recruitment to area/settlement event/growth pattern\nDesign matrix for movement between areas\nDefinition of time blocks that can be used for time-varying parameters\nControls far all time-varying parameters\n\nSpecification for growth and fecundity\nNatural mortality growth parameters, weight-at-length, maturity, and fecundity, for each sex\nHermaphroditism parameter line (if used)\nRecruitment distribution parameters for each area, settlement event, and growth pattern\nCohort growth deviation\nMovement between areas (if used)\nAge error parameter line (if used)\nCatch multiplier (if used)\nFraction female\nSetup for any mortality-growth parameters are time-varying\nSeasonal effects on biology parameters\n\nSpawner-recruitment parameters\nSetup for any stock recruitment parameters are time-varying\nRecruitment deviations\n\nF ballpark value in specified year\nMethod for calculating fishing mortality (F)\nInitial equilibrium F for each fleet\n\nCatchability (Q) setup for each fleet and survey\nCatchability parameters\nSetup for any catchability parameters are time-varying\n\nLength selectivity, retention, discard mortality setup for each fleet and survey\nAge selectivity setup for each fleet and survey\nParameters for length selectivity, retention, discard mortality for each fleet and survey\nParameters for age selectivity, retention, discard mortality for each fleet and survey\nSetup for any selectivity parameters that are time-varying\n\nTag-recapture parameters\n\nVariance adjustments\nLambdas for likelihood components\n\nThe order in which they appear in the control file has grown over time rather opportunistically, so it may not appear particularly logical at this time, especially various aspects of recruitment distribution and growth. When the same information is entered via the SS3 GUI, it is organized more logically and then written in this form to the text control file."
  },
  {
    "objectID": "qmds/html.html#parameter-line-elements",
    "href": "qmds/html.html#parameter-line-elements",
    "title": "RAW HTML CONTENT",
    "section": "8.2 Parameter Line Elements",
    "text": "8.2 Parameter Line Elements\nThe primary role of the control file is to define the parameters to be used by the model. The general syntax of the 14 elements of a long parameter line is described here. If used, time-varying parameter lines use only the first seven elements of a parameter line and will be referred to as a short parameter line. Three types of time-varying properties can be applied to a base parameter: blocks or trend, environmental linkage, and random deviation. Each parameter line contains:\n\n\n\n\n\n\n\n\n\n\nColumn\nElement\nDescription\n\n\n\n\n1\nLO\nMinimum value for the parameter\n\n\n2\nHI\nMaximum value for the parameter\n\n\n3\nINIT\nInitial value for the parameter. If the phase (described below) for the parameter is negative the parameter is fixed at this value. If the ss.par file is read, it overwrites these INIT values.\n\n\n4\nPRIOR\nExpected value for the parameter. This value is ignored if the prior type is 0 (no prior) or 1 (symmetric beta). If the selected prior type (described below) is lognormal, this value is entered in log space.\n\n\n5\nPRIOR SD\nStandard deviation for the prior, used to calculate likelihood of the current parameter value. This value is ignored if prior type is 0. The standard deviation is in regular space regardless of the prior type.\n\n\n6\nPRIOR TYPE\n0 = none,\n\n\n\n\n1 = symmetric beta;\n\n\n\n\n2 = full beta;\n\n\n\n\n3 = lognormal without bias adjustment;\n\n\n\n\n4 = lognormal with bias adjustment;\n\n\n\n\n5 = gamma; and\n\n\n\n\n6 = normal.\n\n\n7\nPHASE\nPhase in which parameter begins to be estimated. A negative value causes the parameter to retain its INIT value (or value read from the ss.par file).\n\n\n8\nEnv var & Link\nCreate a linkage to an input environmental time-series\n\n\n9\nDev link\nInvokes use of the deviation vector in the linkage function\n\n\n10\nDev min yr\nBeginning year for the deviation vector\n\n\n11\nDev max yr\nEnding year for the deviation vector\n\n\n12\nDev phase\nPhase for estimation for elements in the deviation vector\n\n\n13\nBlock\nTime block or trend to be applied\n\n\n14\nBlock function\nFunctional form for the block offset.\n\n\n\n\nNote that relative to SS3 v.3.24, the order of PRIOR SD and PRIOR TYPE have been switched and the PRIOR TYPE options have been renumbered.\nThe full parameter line (14 in length) syntax for the mortality-growth, spawn-recruitment, catchability, and selectivity sections provides additional controls to give the parameter time-varying properties. If a parameter (a full parameter line of length 14) is set up to be time-varying (i.e., parameter time blocks, annual deviations), short parameter lines, the first 7 elements, are required to be specified immediately after the main parameter block (i.e., mortality-growth parameter section). Additional information regard time-varying parameters and how to implement them is in the Using Time-Varying Parameters section."
  },
  {
    "objectID": "qmds/html.html#terminology-1",
    "href": "qmds/html.html#terminology-1",
    "title": "RAW HTML CONTENT",
    "section": "8.3 Terminology",
    "text": "8.3 Terminology\nThe term COND appears in the \"Typical Value\" column of this documentation (it does not actually appear in the model files), it indicates that the following section is omitted except under certain conditions, or that the factors included in the following section depend upon certain conditions. In most cases, the description in the definition column is the same as the label output to the ss_new files."
  },
  {
    "objectID": "qmds/html.html#beginning-of-control-file-inputs",
    "href": "qmds/html.html#beginning-of-control-file-inputs",
    "title": "RAW HTML CONTENT",
    "section": "8.4 Beginning of Control File Inputs",
    "text": "8.4 Beginning of Control File Inputs\n\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n\nComments beginning with #C at the top of the file will be retained and included in output.\n\n\n\n0\n\n0 = Do not read the weight-at-age (wtatage.ss) file;\n\n\n\n\n1 = Read the weight-at-age (wtatage.ss) file, also read and use the growth parameters; and\n\n\n\n\n2 = Future option to read the weight-at-age (wtatage.ss) file, then omit reading and using growth parameters and all length-based data.\n\n\n\n\nAdditional information on the weight-at-age file and the expected formatting can be found in the Empirical Weight-at-Age section.\n\n\n\n\nNumber (N) of growth patterns (GP), also referred to as morphs:\n\n\n\n\nThese are collections of fish with unique biological characteristics (growth, mortality, weight-length, reproduction). The GP x Sex x Settlement Events constitute unique growth patterns that are tracked in SS3. They are assigned these characteristics at birth and retain them throughout their lifetime. At recruitment, growth pattern members are distributed across areas (if any) and they retain their biological characteristics even if they move to another area in which a different cohort with different biological characteristics might predominate. For example, one could assign a fast-growing growth pattern to recruit predominately in a southern areas and a slow-growing growth pattern to a northern area. The natural mortality and growth parameters are specified for each growth pattern in the mortality-growth parameter section in the order of females growth pattern 1 to growth pattern N followed by males growth pattern 1 to growth pattern N in a two sex model.\n\n\n1\n\nNumber of platoons within a growth pattern/morph:\n\n\n\n\nThis allows exploration of size-dependent survivorship. A value of 1 will not create additional platoons. Odd-numbered values (i.e., 3, 5) will break the overall morph into that number of platoons creating a smaller, larger, and mean growth platoon. The higher the number of platoons the slower the model will run, so values above 5 not advised. The fraction of each morph assigned to each platoon is custom-input or designated to be a normal approximation. When multiple platoons are designated, an additional input is the ratio of between platoon to within platoon variability in size-at-age. This is used to partition the total growth variability. For the platoons, their size-at-age is calculated as a factor (determined from the between-within variability calculation) times the size-at-age of the central morph which is determined from the growth parameters for that Growth Pattern x Sex.\n\n\nCOND &gt; 1\nFollowing 2 lines are conditional on N platoons &gt; 1.\n\n\n\n0.7\nPlatoon within/between standard deviation ratio. Ratio of the amount of variability in length-at-age between platoons to within platoons.\n\n\n\n0.2 0.6 0.2\nDistribution among platoons. Enter either a custom vector or enter a vector of length N with the first value of -1 to get a normal approximation: (0.15, 0.70, 0.15) for 3 platoons, or 5 platoons (0.031, 0.237, 0.464, 0.237, 0.031).\n\n\n\n\n\n8.4.1 Weight-at-Age\nThe capability to read empirical body weight-at-age for the population and each fleet was added starting in v.3.04, in lieu of generating these weights internally from the growth parameters, weight-at-length, and size-selectivity. The values are read from a separate file named, wtatage.ss. This file is only required to exist if this option is selected. See the Empirical Weight-at-Age section for additional information on file formatting for empirical weight-at-age.\n\n\n8.4.2 Settlement Timing for Recruits and Distribution\nIn older versions of SS3 one value of spawning biomass was calculated annually at the beginning of one specified spawning season and this spawning biomass produced one annual total recruitment value. The annual recruitment value was then distributed among seasons, areas, and growth types according to other model parameters.\nAdditional control of the seasonal timing was added in v.3.30 and now there now is an explicit elapsed time between spawning and recruitment. Spawning still occurs, just once per year, which defines a single spawning biomass for the stock-recruitment curve but its timing can be at any specified time, not just the beginning of a season. Recruitment of the progeny from an annual spawning can now enter the population in one or more settlement events, at some point after spawning as defined by the user.\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n1\n\nRecruitment distribution method. This section controls which combinations of growth pattern x area x settlement will get a portion of the total recruitment coming from each spawning. Options:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 = no longer available (used the SS3 v.3.24 or earlier setup);\n\n\n\n\n\n\n\n\n\n\n3 = each settle entity; and\n\n\n\n\n\n\n\n\n1\n\nSpawner-Recruitment (not implement yet, but required), options:\n\n\n\n\n1 = global; and\n\n\n\n\n2 = by area (by area is not yet implemented; there is a conceptual challenge to doing the equilibrium calculation when there is fishing).\n\n\n\n\n\n\n\n\n1\n\nNumber of recruitment settlement assignments. Must be at least 1 even if only 1 settlement and 1 area because the timing of that settlement must be specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\nFuture feature, not implement yet but required.\n\n\nGrowth Pattern\nMonth\nArea\nAge at settlement\n\n\n1\n5.5\n1\n0\n\n\n\nThe above example specifies settlement to mid-May (month 5.5). Note that normally the calendar age at settlement is 0 if settlement happens between the time of spawning and the end of that year, and at age 1 if settlement is in the year after spawning.\nBelow is an example set-up where there are multiple settlement events, with one occurring the following year after spawning:\n\n\n\n\n3\nNumber of recruitment settlement events\n\n\n0\nUnused option\n\n\nGrowth Pattern\nMonth\nArea\nAge (for each settlement assignment)\n\n\n1\n11.0\n1\n0\n\n\n1\n12.0\n1\n0\n\n\n1\n1.0\n1\n1\n\n\n\n\nDetails regarding settlement of recruits and timing:\n\nRecruitment happens in specified settlement events (growth pattern, month, area).\nNumber of unique settlement timings is calculated at runtime.\nNow there is explicit elapsed time between spawning and recruitment.\nGrowth and natural mortality of the platoon begins at time of settlement, which is its real age 0.0 for growth; but pre-settlement fish exist from the beginning of the season of settlement, so can be caught if selected.\nAge at recruitment now user-controlled (should be 0 if in year of spawning).\nAll fish become integer age 1 (for age determination) on their first January 1st.\nRecruitment can occur &gt;12 months after spawning which is achieved by setting the settlement age to a value greater than 1.0.\n\nThe distribution of recruitment among these settlement events is controlled by recruitment apportionment parameters. There must be a parameter line for each growth pattern, then for each area, then for each settlement. All of these are required, but only those growth pattern x area x settlements designated to receive recruits in the recruitment design matrix will have the parameter used in the recruitment distribution calculation. For the recruitment apportionment, the parameter values are the natural log of apportionment weight. The sum of all apportionment weights is calculated for each growth pattern x area x settlements that have been designated to receive recruits in the recruitment design matrix. Then the apportionment weights are scaled to sum to 1.0 so that the total recruitment from the spawning event is distributed among the cells designated to receive recruitment. Additionally, these distribution parameters can be time-varying, so the fraction of the recruits that occur in a particular growth pattern, area, or settlement can change from year to year. To specify annual variation in the distribution or recruits by area add a start and end year in the deviation min year and max year columns. Similar to the apportionment of recruits by area, one should be fixed while the other area(s) can deviate relative to the one area. If annual deviations are specified then two additional short parameter lines will be required to specify the standard error and the autocorrelation for each area with deviations.\n\n\n\n\n8.4.2.1 Recruitment Distribution and Parameters\n\nRecruits are apportioned according to:\n\\[\\text{apportionment}_i = \\frac{e^{p_i}}{\\sum_{i=1}^{N}e^{p_i}}\\]\nwhere \\(p_i\\) is the proportion of recruits to area \\(i\\) and \\(N\\) is the number of settlement events. These parameters are defined in the mortality-growth parameter section.\nTips for fixing or estimating the recruitment apportionment:\n\nSet the value for one of these parameters, \\(p_i\\), to 0.0 and not estimate it so that other parameters will be estimated (if not fixed) relative to its fixed value.\nGive the estimated parameters a min-max so they have a good range relative to the base parameter (i.e., of min = -5 and max = 5).\nIn order to get a different distribution of recruitments in different years, you will need to make at least one of the recruitment distribution parameters time-varying.\n\nIn a seasonal model, all cohorts graduate to the age of 1 when they first reach January 1, even if the seasonal structure of the model has them being spawned in the late fall. In general, this means that the model operates under the assumption that all age data have been adjusted so that fish are age 0 at the time of spawning and all fish graduate to the next age on January 1. This can be problematic if the ageing structures deposit a ring at another time of year. Consequently, you may need to add or subtract a year to some of your age data to make it conform to the model expected data structure, or more ideally you may need to define the calendar year within the model to start at the beginning of the season at which ring deposition occurs. Talk with your ageing lab about their criteria for seasonal ring deposition.\nSeasonal recruitment is coded to work smoothly with growth. If the recruitment occurring in each season is assigned the same growth pattern, then each seasonal cohort’s growth trajectory is simply shifted along the age/time axis. At the end of the year, the early born cohorts will be larger, but all are growing with the same growth parameters, so all will converge in size as they approach their common maximum length (e.g., no seasonal effects on growth).\nAt the time of settlement, fish are assigned a size equal to the lower edge of the first population size bin and they grow linearly until they reach the age A1. A warning is generated if the first population length bin is greater than 10 cm as this seems an unreasonably large value for a larval fish. A1 is in terms of real age elapsed since birth. All fish advance to the next integer age on January 1, regardless of birth season. For example, consider a 2 season model with some recruitment in each season and with each season’s recruits coming from the same GP. At the end of the first year, the early born fish will be larger but both of the seasonal cohorts will advance to an integer age of 1 on Jan 1 of the next year. The full growth curve is still calculated below A1, but the size-at-age used is the linear replacement. Because the linear growth trajectory can never go negative, there is no need for the additive constant to the standard deviation (necessary for the growth model used in SS2 V1.x), but the option to add a constant has been retained in the model.\n\n\n\n8.4.3 Movement\nHere the movement of fish between areas are defined. This is a box transfer with no explicit adjacency of areas, so fish can move from any area to any other area in each time step. While not incorporated yet, there is a desire for future versions of SS3 to have the capability to allow sex-specific movement, and also to allow some sort of mirroring so that sexes and growth patterns can share the same movement parameters if desired.\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n\n\n\n\n\n\n2\nEnter Number of movement definitions.\n\n\n\n1.0\nFirst age that moves. This value is a real number, not an integer, to allow for an in-year start to movement in a multi-season model. It is the real age at the beginning of a season, even though movement does not occur until the end of the season. For example, in a setup with two 6-month seasons a value of 0.5 will cause the age 0 fish to not move when they complete their first 6 month season of life, and then to move at the end of their second season because they start movement capability when they reach the age of 0.5 years (6 months).\n\n\n\n1 1 1 2 4 10\nMovement definitions: season, growth pattern, source area, destination, age1, and age2. The example shown here has 1 growth patterns and 2 areas with fish moving between the two areas. The rate of movement will be controlled by the movement parameters later defined in the mortality-growth parameter section. Here the age1 and age2 specify the range over which the movement parameters are interpolated with movement constant below age1 and above age2.\n\n\n\n1 2 2 1 4 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo parameters will be entered later for each growth pattern, area pair, and season.\n\nMovement is constant at the first parameter (P1) below the specified minimum age for movement change, constant at the second parameter (P2) above maximum age for movement change, and linearly interpolated for intermediate ages.\nA movement rate parameter can be set to use the same value as the corresponding parameter for the first defined movement pattern by entering a parameter value of -9999 and a negative phase value.\nFor each source area, the implicit movement parameter value is 0.0 (movement within a single area). However, this default value can be replaced if the stay movement is selected to have an explicit pair of parameters (e.g., specify movement rate for area 1 to area 1) and will require additional parameter lines.\nA constant movement rate across all ages can be accomplished by either:\n\nSetting both movement ages to 0, not estimating the first movement parameter, and using a second movement parameter to cover all ages from 0 to the maximum number of ages.\nSetting movement ages to any value, estimating the first movement parameter, and setting the second movement parameter to have a value of -9998 with a negative phase.\n\nThe parameter is exponentiated so that a movement parameter value of 0 becomes 1.0.\nFor each source area, all movement rates are then summed and divided by this sum so that 100% of the fish are accounted for in the movement calculations. \\[\\text{rate}_i = \\frac{e^{p_i}}{\\sum_{j=1}^{N}e^{p_i}}\\]\nAt least one movement parameter must be fixed so that all other movement parameters are estimated relative to it. This is achieved naturally by not specifying the stay rate parameter so it has a fixed value of 0.0.\nThe resultant movement rates are multiplied by season duration in a seasonal model.\n\n\n\n8.4.4 Time Blocks\n\n\n\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n3\n\nNumber of block patterns. These patterns can be referred to in the parameter sections to create a separate parameter value for each block.\n\n\n\n\n\n\n\n\n\n\n\n\nCOND &gt; 0:\nFollowing inputs are omitted if the number of block patterns equals 0.\n\n\n\n3 2 1\nBlocks per pattern:\n\n\n\n\n\n\n\n\n1975 1985 1986 1990 1995 2001\nBeginning and ending years for blocks in design 1; years not assigned to a block period retain the baseline value for a parameter that uses this pattern.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1987 1990 1995 2001\nBeginning and ending years for blocks in design 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n1999 2002\nBeginning and ending years for blocks in design 3.\n\n\n\n\n\n\n\n\n8.4.5 Auto-generation\nAuto-generation is a useful way to automatically create the required short time-varying parameter lines which will be written in the control.ss_new file. These parameter lines can then be copied into the control file and modified as needed. As example, if you want to add a block to natural mortality, modify the block and block function entry of the mortality parameter line, ensure that auto-generation is set to 0 (for the biology section at least) and run the model without estimation. The control.ss_new file will now show the required block parameter line specification for natural mortality and this line can be copied into the main control file. Note, that if auto-generation is on (set to 0), the model will not expect to read the time-varying parameters in that section of the control file and will error out if they are present\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n1\n\nEnvironmental/Block/Deviation adjust method for all time-varying parameters.\n\n\n\n\n1 = warning relative to base parameter bounds; and\n\n\n\n\n3 = no bound check. Logistic bound check form from previous SS3 versions (e.g., SS3 v.3.24) is no longer an option.\n\n\n1 1 1 1 1\nAuto-generation of time-varying parameter lines. Five values control auto-generation for parameter block sections: 1-biology, 2-spawn-recruitment, 3-catchability, 4-tag (future), and 5-selectivity.\n\n\n\n\nThe accepted values are:\n\n\n\n\n0 = auto-generate all time-varying parameters (no time-varying parameters are expected);\n\n\n\n\n1 = read each time-varying parameter line as exists in the control file; and\n\n\n\n\n2 = read each line and auto-generate if read if the time-varying parameter value for LO = -12345. Useful to generate reasonable starting values."
  },
  {
    "objectID": "qmds/html.html#biology",
    "href": "qmds/html.html#biology",
    "title": "RAW HTML CONTENT",
    "section": "8.5 Biology",
    "text": "8.5 Biology\n\n8.5.1 Natural Mortality\nNatural mortality (M) options include some options that are referenced to integer age and other options to real age since settlement. If using an option that references M to real age since settlement, M varies by age and will change by season (e.g., cohorts born early in the year will have different M than cohorts born later in the year).\n\n8.5.1.1 Lorenzen Natural Mortality\n\nLorenzen natural mortality is based on the concept that natural mortality is driven by physiological and ecological processes and varies over the life cycle of a fish. So, natural mortality is scaled by the length of the fish. In this implementation, a reference age and M value are read in, and other ages will have an M scaled to its body size-at-age. However, if platoons are used, all will have the same M as their growth pattern. Lorenzen M calculation will be updated if the starting year growth parameters are active, but if growth parameters vary during the time-series, the M is not further updated. Be careful in using Lorenzen when there is time-varying growth.\n\n\n8.5.1.2 Age-specific M Linked to Age-Specific Length and Maturity\n\nThis is an experimental option available as of 3.30.17.\nA general model for age- and sex-specific natural mortality expands a model developed by Mark N. Maunder et al. (2010) and Mark N. Maunder (2011) and is based on the following some assumptions:\n\nM for younger fish is due mainly to processes that are functions of the size of the individuals (e.g., predation);\nM increases after individuals become reproductively mature;\nMaturity follows a logistic curve; and\nM caused by senescence is either small or occurs at an age for which there are few fish alive, so it is not influential.\n\nThe model is based on combining the observation that M is inversely proportional to length for young fish (Kai Lorenzen 2000) and the logistic model from Lehodey, Senina, and Murtugudde (2008) for older fish. Natural mortality for a given sex and age is: \\[M_{a,s} = M_{juv,s}\\frac{L_{a,s}}{L_{mat*,s}}^{\\lambda} + \\frac{M_{mat,s}-M_{juv,s}\\frac{L_{a,s}}{L_{mat*,s}}^{\\lambda}}{1+e^{\\beta_s(L_{a,s}- L_{50,s})}},\\]\nwhere \\(M_{juv,s}\\) (juvenile natural mortality), \\(\\lambda\\) (power), \\(L_{mat*,s}\\) ( first mature length of fish), and \\(M_{mat,s}\\) (the mature instantaneous natural mortality rate by sex, are user inputs in long parameter lines. For suboption 1, \\(L_{50}\\) and and \\(\\beta\\) (slope) parameters taken from the maturity relationship within the model, which must use maturity-fecundity option 1. For suboption 3, the \\(L_{50,s}\\) (the length at which 50% of fish are mature) and \\(\\beta\\) (slope) parameters are specified in long parameter lines by the user.\nNote that juvenile natural mortality, \\(M_{juv,s}\\), and first mature length of fish, \\(L_{mat*,s}\\), inputs are by sex (and growth pattern), but it is recommended to share them across sex by using the offset option. Using offset option 2 (males offset from females) causes male parameters to be an offset to the female parameters, so a parameter value of 0.0 for a male parameter will fix the parameter as same as the female parameter. Alternatively, using offset option 1 and setting males to 0.0 and not estimating the parameter fixes the parameter at the value of the female parameter (the section on fixing male parameters the same as female parameters has more details). This fulfills an additional assumption: M caused by reproduction may differ by sex, but juvenile M is independent of sex.\nThe length for a given age and sex, \\(L_{a,s}\\) is calculated within the model.\nSome suggested defaults for user-provided parameter inputs are:\n\n\\(\\lambda = -1.5\\) from Gulland (1987)\n\\(M_{mat,s}=\\frac{5.4}{t_{max,s}}\\) from Hamel (submitted) if \\(t_{max}\\) is available, otherwise \\(M_{mat,s} = 4.118K_{s}^{0.73}L_{inf,s}^{-0.33}\\) as in Then et al. (2015)\n\\(M_{juv,s} = 3W_{mat}^{-0.288}\\) from K. Lorenzen (1996)\n\n\n\n8.5.1.3 Age-range Lorenzen\n\nThe original implementation of Lorenzen natural mortality in Stock Synthesis uses a reference age and its associated natural mortality as inputs to determine the Lorenzen curve. However, sometimes this information is not known. The age-range Lorenzen instead uses a range of ages and the average natural mortality over them to calculate a Lorenzen natural mortality curve.\nLike the original Lorenzen options, ages will have an M scaled to its body size-at-age and care should be taken when there are multiple growth patterns or time-varying growth.\n\n\n8.5.1.4 Natural Mortality Options\n\n\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n1\n\nNatural Mortality Options:\n\n\n\n\n0 = A single parameter;\n\n\n\n\n1 = N breakpoints;\n\n\n\n\n2 = Lorenzen;\n\n\n\n\n3 = Read age specific M and do not do seasonal interpolation;\n\n\n\n\n4 = Read age specific and do seasonal interpolation, if appropriate;\n\n\n\n\n5 = age-specific M linked to age-specific length and maturity (experimental);\n\n\n\n\n6 = Age-range Lorenzen.\n\n\nCOND = 0\nNo additional natural mortality controls.\n\n\nCOND = 1\n\n\n\n\n4\nNumber of breakpoints. Then read a vector of ages for these breakpoints. Later, per sex x GP, read N parameters for the natural mortality at each breakpoint.\n\n\n2.5 4.5 9.0 15.0\nVector of age breakpoints.\n\n\nCOND = 2\n\n\n\n\n4\nReference age for Lorenzen natural mortality: read one additional integer value that is the reference age. Later read one long parameter line for each sex x growth pattern that will be the M at the reference age.\n\n\nCOND = 3 or 4\nDo not read any natural mortality parameters in the mortality growth parameter section. With option 3, these M values are held fixed for the integer age (no seasonality or birth season considerations). With option 4, there is seasonal interpolation based on real age, just as in options 1 and 2.\n\n\n\n0.20 0.25 ... 0.20 0.23 ...\nAge-specific M values where in a 2 sex model the first row is female and the second row is male. If there are multiple growth patterns female growth pattern 1-N is read first followed by males 1-N growth pattern.\n\n\nCOND = 5\nage-specific M linked to age-specific length and maturity suboptions.\n\n\n\n\n1 = Requires 4 long parameter lines per sex x growth pattern using maturity. Must be used with maturity option 1;\n\n\n\n\n2 = reserved for future option;\n\n\n\n\n3 = Requires 6 long parameter lines per sex x growth pattern\n\n\nCOND = 6\nRead two additional integer values that are the age range for average M. Later, read one long parameter line for each sex x growth pattern that will be the average M over the reference age range.\n\n\n\n0\nMinimum age of average M range for calculating Lorenzen natural mortality.\n\n\n\n10\nMaximum age of average M range for calculating Lorenzen natural mortality.\n\n\n\n\n\n\n8.5.2 Growth\n\n8.5.2.1 Timing\n\nWhen fish recruit at the real age of 0.0 at settlement, they have body size equal to the lower edge of the first population size bin. The fish then grow linearly until they reach a real age equal to the input value “growth-at-age for L1” and have a size equal to the parameter value for L1 (the minimum length parameter). As they age further, they grow according the selected growth equation. The growth curve is calibrated to go through the size L2 parameter when they reach the age of maximum length.\n\n\n8.5.2.2 Maximum Length (Linf)\n\nIf “Growth at age for L2” is set equal to 999, then the size at the L2 parameter is used as Linf.\n\n\n8.5.2.3 von Bertalanffy growth function\n\nThe von Bertalanffy growth curve is parameterized as:\n\\[L_t = L_\\infty + (L_{1}-L_\\infty)e^{-k(a-A_{1})}\\]\nwith parameters \\(L_{1}\\), \\(L_\\infty\\), and \\(k\\). The \\(L_\\infty\\) is calculated as:\n\\[L_\\infty = L_{1} + \\frac{(L_2 - L_1)}{e^{-k(A2-A1)}}\\]\nbased on the input values of fixed age for first size-at-age (\\(A_1\\)) and fixed age for second size-at-age (\\(A_2\\)).\n\n\n8.5.2.4 Schnute/Richards growth function\n\nThe Richards (1959) growth model as parameterized by Schnute (1981) provides a flexible growth parameterization that allows for not only asymptotic growth but also linear, quadratic or exponential growth. The Schnute/Richards growth is invoked by entering option 2 in the growth type field. The Schnute/Richards growth function uses the standard growth parameters (e.g, Lmin, Linf, and \\(k\\)) and a fourth parameter that is read after reading the von Bertalanffy growth coefficient parameter (\\(k\\)). When this fourth parameter has a value of 1.0, it is equivalent to the standard von Bertalanffy growth curve. When this function was first introduced , it was required that A0 parameter be set to 0.0.\nThe Schnute/Richards growth model is parameterized as:\n\\[L_t = L_{MIN}^b + (L_\\infty^b-L_{MIN}^b)\\frac{1-e^{-k(t-A_{1})}}{1-e^{-k(A_2-A_1)}}^{1/b}\\]\nwith parameters \\(L_{MIN}\\), \\(L_{MAX}\\), \\(k\\), and \\(b\\).\nThe Richards model has \\(b\\) &lt; 0, the von Bertalanffy model has \\(b\\) = 1. The general case of \\(b\\) &gt; 0 was called the \"generalized von Bertalanffy\" by Schnute (1981). The Gompertz has \\(b\\) = 0, where the equation is undefined as written above and must be replaced with:\n\\[L_t = y_1e\\Big[log(y_2/y-1)\\frac{1-e^{-k(t-A_1)}}{1-e{-k(A_2-A_1)}}\\Big]\\]\nThus, if \\(b\\) will be estimated as a free parameter, it might be necessary to include options for constraining it to different ranges.\n\n\n8.5.2.5 Mean size-at-maximum age\n\nThe mean size of fish in the max age age bin depends upon how close the growth curve is to Linf by the time it reaches max age and the mortality rate of fish after they reach max age. Users specify the mortality rate to use in this calculation during the initial equilibrium year. This must be specified by the user and should be reasonably close to M plus initial F. In SS3 v.3.30, this uses the von Bertalanffy growth out to 3 times the maximum population age and decays the numbers at age by exp(-value set here). For subsequent years of the time series, the model should update the size-at-maximum age according to the weighted average mean size of fish already at maximum age and the size of fish just graduating into maximum age. Unfortunately, this updating is only happening in years with time-varying growth. This will hopefully be fixed in a the future version.\n\n\n8.5.2.6 Age-specific K\n\nThis option creates age-specific K multipliers for each age of a user-specified age range, with independent multiplicative factors for each age in the range and for each growth pattern / sex. The null value is 1.0 and each age’s K is set to the next earlier age’s K times the value of the current age’s multiplier. Each of these multipliers is entered as a full parameter line, so inherits all time-varying capabilities of full parameters. The lower end of this age range cannot extend younger than the specified age for which the first growth parameter applies. This is a beta model feature, so examine output closely to assure you are getting the size-at-age pattern you expect. Beware of using this option in a model with seasons within year because the K deviations are indexed solely by integer age according to birth year. There is no offset for birth season timing effects, nor is there any seasonal interpolation of the age-varying K.\n\n\n\n\n\n8.5.2.7 Growth cessation\n\nA growth cessation model was developed for the application to tropical tuna species (Mark N. Maunder et al. 2018). Growth cessation allows for a linear relationship between length and age, followed by a marked reduction of growth after the onset of sexual maturity by assuming linear growth for the youngest individuals and then a logistic function to model the decreasing growth rate at older ages.\n\n\n\nExample growth specifications:\n\n\n\n\nTypical Value\nDescription and Options\n\n\n1\n\nGrowth Model:\n\n\n\n\n1 = von Bertalanffy (3 parameters);\n\n\n\n\n2 = Schnute’s generalized growth curve (aka Richards curve) with 3 parameters. Third parameter has null value of 1.0;\n\n\n\n\n3 = von Bertalanffy with age-specific K multipliers for specified range of ages, requires additional inputs below following the placeholder for future growth feature;\n\n\n\n\n4 = age specific K. Set base K as K for age = nages and working backwards and the age-specific K = K for the next older age * multiplier, requires additional inputs below following the placeholder for future growth feature;\n\n\n\n\n5 = age specific K. Set base K as K for nages and work backwards and the age-specific K = base K * multiplier, requires additional inputs below following the placeholder for future growth feature;\n\n\n\n\n6 = not implemented;\n\n\n\n\n7 = not implemented; and\n\n\n\n\n8 = growth cessation. Decreases the K for older fish. If implemented, the Amin and Amax parameters, the next two lines, need to be set at 0 and 999 respectively. The mortality-growth parameter section requires the base K parameter line which is interpreted as the steepness of the logistic function that models the reduction in the growth increment by age followed by a second parameter line which is the parameter related to the maximum growth rate.\n\n\n1\n\nGrowth Amin (A1): Reference age for first size-at-age (post-settlement) parameter.\n\n\n25\n\nGrowth Amax (A2): Reference age for second size-at-age parameter (999 to use as L infinity).\n\n\n0.20\n\nExponential decay for growth above maximum age (plus group: fixed at 0.20 in SS3 v.3.24; should approximate initial Z). Alternative Options:\n\n\n\n\n-998 = Disable growth above maximum age (plus group) similar to earlier versions of SS3 (prior to SS3 v.3.24); and\n\n\n\n\n-999 = Replicate the simpler calculation done in SS3 v.3.24.\n\n\n0\n\nPlaceholder for future growth feature.\n\n\nCOND = 3\nGrowth model: age-specific K\n\n\n2\n\nNumber of K multipliers to read;\n\n\n\n5\nMinimum age for age-specific K; and\n\n\n\n7\nMaximum age for age-specific K.\n\n\nCOND = 4 or 5\nGrowth model: age-specific K\n\n\n2\n\nNumber of K multipliers to read;\n\n\n\n7\nMaximum age for age-specific K;\n\n\n\n6\nSecond age for age-specific K; and\n\n\n\n5\nMinimum age for age-specific K.\n\n\n0\n\nStandard deviation added to length-at-age: Enter 0.10 to mimic SS2 V1.xx. Recommend using a value of 0.0.\n\n\n1\n\nCV Pattern (cannot be time-varying)\n\n\n\n\n0: CV=f(LAA), so the 2 parameters are in terms of CV of the distribution of length-at-age and the interpolation between these 2 parameters is a function of mean length-at-age;\n\n\n\n\n1: CV=f(A), so interpolation is a function of age;\n\n\n\n\n2: SD=f(LAA), so parameters define the standard deviations of length-at-age and interpolation is a function of mean length-at-age;\n\n\n\n\n3: SD=f(A); and\n\n\n\n\n4: Lognormal distribution of size-at-age. Input parameters will specify the standard deviation of natural log size-at-age (e.g., entered values will typically be between 0.05 and 0.15). A bias adjustment is applied so the lognormal distribution of size-at-age will have the same mean size as when a normal distribution is used.\n\n\n\n\n\n\n8.5.3 Maturity-Fecundity\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n2\n\nMaturity Option:\n\n\n\n\n1 = length logistic;\n\n\n\n\n2 = age logistic;\n\n\n\n\n3 = read maturity-at-age for each female growth pattern;\n\n\n\n\n4 = read a fecundity x maturity-at-age vector for all ages;\n\n\n\n\n5 = disabled; and\n\n\n\n\n6 = read vector of length-based maturity values.\n\n\n\n\nNote: need to read 2 parameter lines (maturity at 50% and maturity slope) even if option 3 or 4 is selected.\n\n\nCOND = 3 or 4\nMaturity Option\n\n\n0 0.05 0.10 ...\nVector of age-specific maturity or fecundity. One row of length Nages + 1 based on the maximum population age for each female growth pattern.\n\n\nCOND = 6\nMaturity Option\n\n\n0 0.05 0.10 ...\nVector of length-specific maturity or fecundity, based on the population length bins. One row of length equal to the number of population length bins (defined in the data file) for each female growth pattern.\n\n\n1\n\nFirst Mature Age: all ages below the first mature age will have maturity set to zero. This value is overridden if maturity option is 3 or 4 or if empirical weight-at-age (wtatage.ss) is used, but still must exist here.\n\n\n1\n\nFecundity Option (irrelevant if maturity option is 4 or wtatage.ss is used):\n\n\n\n\n1 = to interpret the 2 egg parameters as linear eggs/kg on body weight (current default), so fecundity = \\(wt * (a+b*wt)\\), so value of a=1, b=0 causes eggs to be equivalent to spawning biomass;\n\n\n\n\n2 = to set fecundity= \\(a*L^ b\\);\n\n\n\n\n3 = to set fecundity= \\(a*W^ b\\), so values of a=1, b=1 causes fecundity to be equivalent to spawning biomass;\n\n\n\n\n4 = fecundity = \\(a+b*L\\); and\n\n\n\n\n5 = eggs = \\(a+b*wt\\).\n\n\n\n\n\n8.5.4 Hermaphroditism\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n0\n\nHermaphroditism Option:\n\n\n\n\n0 = not used;\n\n\n\n\n1 = invoke female to male age-specific function; and\n\n\n\n\n-1 = invoke male to female age-specific function.\n\n\n\n\nNote: this creates the annual, age-specific fraction that change sex, it is not the fraction that is each sex.\n\n\nCOND = 1 or\n\n\n\nCOND = -1\nRead 2 lines below if hermaphroditism is selected. Also read 3 parameters after reading the male (option 1) or female (option -1) weight-length parameter.\n\n\n\n-1.2\nHermaphroditism Season:\n\n\n\n\n-1 to do transition at the end of each season (after mortality and before movement); and\n\n\n\n\n&lt;positive integer&gt; to select just one season.\n\n\n\n\nIf fractional part included (optional), indicates first age that transitions (otherwise, age 1 assumed).\n\n\n\n0.5\nInclude males in spawning biomass;\n\n\n\n\n0 = no males in spawning biomass;\n\n\n\n&gt;0\n&lt;1 = fraction of male biomass to include in spawning biomass; and\n\n\n\n\n1 = simple addition of males to females.\n\n\n\nThe hermaphroditism option requires three full parameter lines in the mortality growth section:\n\nA parameter line for inflection point (in age) at when fish may begin to change sex.\nThe standard deviation.\nThe asymptote of the maximum proportion that will transition to the other sex\n\nThese parameter lines are entered directly after the weight-at-length parameters for males.\n\n\n\n\n\n8.5.5 Natural Mortality and Growth Parameter Offset Method\nThe most common set-up for natural mortality and growth parameters for two-sex models is direct assignment (option 1) which allows for independent estimation (or fixing) of natural mortality and growth parameters by sex. Within the direct assignment option there is functionality to set male parameters equal to the corresponding female parameter if the male INIT value is set to 0 and the phase is negative.\nAlternatively, there may be situations where a user wants to create direct linkages between natural mortality and growth parameters between sexes (options 2 or 3). If the parameter offset option 2 is selected, the control file still requires that all male natural mortality and growth parameters lines to be included. The natural mortality and growth parameters (e.g., k, Lmin, Lmax, CV1, CV2) for sex &gt; 1 (typically male fish) have a value that is an exponential offset to the female natural mortality and growth parameters, e.g., \\(M_{\\text{male}} = M_{\\text{female}}*exp(M_{\\text{male offset}})\\). An offset parameter can be fixed at 0.0, at a non-zero value, or estimated.\nParameter offset option 3 has an offset feature for the growth CV and for natural mortality. For the growth CV, the parameter for CV at old age is an exponential offset from the parameter for CV at young age, e.g., \\(CV_{\\text{old}} = CV_{\\text{young}}*exp(CV_{\\text{offset}})\\).This allows for CV old to track an estimated CV young parameter. For natural mortality, if there is more than 1 natural mortality parameter, then parameters 2 and higher for the same sex and growth pattern are exponential offsets from the first natural mortality parameter. Note that it is an old feature designed to work with natural mortality option 1 (breakpoints). It may work with natural mortality options 3 and 4, but this has not been tested.\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n1\n\nParameter Offset Method:\n\n\n\n\n1 = direct assignment;\n\n\n\n\n2 = for each growth pattern by sex, parameter defines offset from sex 1, offsets are in exponential terms, so for example: \\(M_{\\text{old male}} = M_{\\text{old female}}*exp(M_{\\text{old male}})\\); and\n\n\n\n\n3 = for each growth pattern by sex, parameter defines offset from growth pattern 1 sex 1. For females, given that “natM option” is breakpoint and there are two breakpoints, parameter defines offset from early age (e.g., \\(M_{\\text{old female}} = M_{\\text{young female}}*exp(M_{\\text{old female}}\\)). For males, given that \"natM option\" is breakpoint and there are two breakpoints, parameter is defined as offset from females AND from early age (e.g., \\(M_{\\text{old male}} = M_{\\text{young female}}*exp(M_{\\text{young male}})*exp(M_{\\text{old male}})\\)).\n\n\n\n\n\n8.5.6 Catch Multiplier\nThese parameter lines are only included in the control file if the catch multiplier field in the data file is set to 1 for a fleet. The model expected catch \\(C_{exp}\\) by fleet is estimated by:\n\\[C_{exp} = \\frac{C_{obs}}{c_{mult}}\\]\nwhere \\(C_{obs}\\) is the input catch by fleet (observed catch) within the data file and \\(c_{mult}\\) is the estimated (or fixed) catch multiplier. It has year-specific, not season-specific, time-varying capabilities. In the catch likelihood calculation, expected catch is multiplied by the catch multiplier by year and fishery to get \\(C_{obs}\\) before being compared to the observed retained catch as modified by the \\(c_{mult}\\).\n\n\n8.5.7 Ageing Error Parameters\nThese parameters are only included in the control file if one of the ageing error definitions in the data file has requested this feature (by putting a negative value for the ageing error of the age zero fish of one ageing error definition). As of version 3.30.12, these parameters now have time-varying capability. Seven additional full parameter lines are required. The parameter lines specify:\n\nAge at which the estimated pattern begins (just linear below this age), this is the start age.\nBias at start age (as additive offset from unbiased age).\nBias at maximum (as additive offset from unbiased age).\nPower function coefficient for interpolating between those 2 values (value of 0.0 produces linear interpolation in the bias).\nStandard deviation at start age.\nStandard deviation at max age.\nPower function coefficient for interpolating between those 2 values.\n\nCode for implementing vectors of mean age and standard deviation of age can be located online within the SS_miscfxn.tpl file, search for function \"get_age_age\" or \"SS_Label_Function 45\".\n\n\n8.5.8 Sex ratio\nThe last line in the mortality-growth parameter section allows the user to fix or estimate the sex ratio between female and male fish at recruitment. The parameter is specified in the fraction of female fish and is applied at settlement. The default option is a sex ratio of 0.50 with this parameter not being estimated. Any composition data input as type = 3, both sexes, will be informative to the sex ratio because it scales females and males together, not separately, for this data type input. Estimation of the sex ratio is a new feature and should be done with care with the user checking that the answer is reflective of the data.\nAs of v.3.30.12, this parameter now has time-varying capability similar to other parameters in the mortality-growth section.\n\n\n8.5.9 Predator Fleet Mortality\nThe ability to define a predator fleet was first implemented in v.3.30.18. A parameter line for predator mortality is only required if a predator fleet has been defined in the data file. For each fleet that is designated as a predator, a new parameter line is created in the mortality-growth (MGparm) section in the control file. This parameter will have the label M2_pred1, where the \"1\" is the index for the predator (not the index of the fleet being used as a predator). More than one predator can be included. If the model has &gt; 1 season, it is normal to expect M2 to vary seasonally. Therefore, only if the number of seasons is greater than 1, follow each M2 parameter with number of season parameters to provide the seasonal multipliers. These are simple multipliers times M2, so at least one of these needs to have a non-estimated value. The set of multipliers can be used to set M2 to only operate in one season if desired. If there is more than one predator fleet, each will have its own seasonal multipliers. If there is only 1 season in the model, then no multiplier lines are included.\nM2 is age-specific, but not sex or morph specific. The value of the M2 parameter will be distributed across ages according to the selectivity for this fleet. In this example note that \"pred1\" refers to the first predator in the model, note the fleet number in which that predator has been 3 configured. The resultant age-specific M2 is added to the base M to create a total age-specific M that operates in the model exactly as M has always operated.\nBecause M2 is a MGparm, it can be time-varying like any other MGparm. This is important because M2, as a component added to base M, will probably always need to be time-varying by blocks, random walk or linkage to external driver. A time series of M2 from an external source could be input by setting the M2 parameter to have a base value of 0.0 and linking to the time series in the environmental data section of the data file using an additive link. In addition, the relationship should have a fixed slope of 1.0 such that M2(y) = 0.0 + 1.0 * M2_env_input(y).\nNote that all existing reports of natural mortality are the total (base M + M2) natural mortality. The M2 parameter is active in the virgin year and initial equilibrium year, where the value of M2 in the start year is used. In the future, separate control of M2 for the initial equilibrium will be provided. M2 is part of the total M used in the SPR and MSY benchmark calculations. M2 is active in the forecast era, so be attentive to its configuration if it is time-varying. Testing to date shows that this M2 feature can replicate previous results using bycatch fleets.\nNote that predator calculations probably will fail if tried with F Method=1 (Pope’s), even though the Pope calculation is built into the hybrid F approach.\n\n\n8.5.10 Read Biology Parameters\n\nNext\n\n, the model reads the mortality-growth (MG) parameters in generally the following order (may vary based on selected options):\n\n\n\n\n\n\n\n\nParameter\n\nDescription\n\n\n\n\nParameter\n\nDescription\n\n\n\nFemale natural mortality and growth parameters in the following order by growth pattern.\n\n\n\n\nM\nNatural mortality for female growth pattern 1, where the number of natural mortality parameters depends on the option selected.\n\n\nCOND if M option = 1\n\n\n\n\nN breakpoints\nN-1 parameter lines as an exponential offsets from the previous reference age.\n\n\n\nLmin\nLength at Amin (units in cm) for female, growth pattern 1.\n\n\n\nLmax\nLength at Amax (units in cm) for female, growth pattern 1.\n\n\n\nVBK\nvon Bertanlaffy growth coefficient (units are per year) for females, growth pattern 1.\n\n\nCOND if growth type = 2\n\n\n\n\nRichards Coefficient\nOnly include this parameter if Richards growth function is used. If included, a parameter value of 1.0 will have a null effect and produce a growth curve identical to von Bertalanffy.\n\n\nCOND if growth type &gt;=3\nAge-Specific K\n\n\n\nN parameter lines equal to the number K deviations for the ages specified above.\n\n\n\nCV young\nVariability for size at age &lt;= Amin for females, growth pattern 1. Note that CV cannot vary over time, so do not set up env-link or a deviation vector. Also, units are either as CV or as standard deviation, depending on assigned value of CV pattern.\n\n\n\nCV old\nVariability for size at age &gt;= Amax for females, growth pattern 1. For intermediate ages, do a linear interpolation of CV on means size-at-age. Note that the units for CV will depend on the CV pattern and the value of mortality-growth parameter as offset. The CV value cannot vary over time.\n\n\n\nWtLen scale\nCoefficient to convert length in cm to weight in kg for females.\n\n\n\nWtLen exp\nExponent in to convert length to weight for females.\n\n\n\nMat-50%\nMaturity logistic inflection (in cm or years) where female maturity-at-length (or age) is a logistic function: \\(M_{l} = 1/(1+exp(\\alpha*(l_{a} - \\beta)))\\). The \\(\\alpha\\) is the slope, \\(l_{a}\\) is the size-at-age, and \\(\\beta\\) is the inflection of the maturity curve. Value ignored for maturity option 3, 4, and 6.\n\n\n\nMat-slope\nLogistic slope (must have negative value). Value ignored for maturity option 3, 4, and 6.\n\n\n\nEggs-alpha\nTwo fecundity parameters; usage depends on the selected fecundity option. Must be included here even if vector is read in the control section above.\n\n\n\nEggs-beta\n\n\n\nCOND: growth pattern &gt; 1\nRepeat female parameters in the above order for growth pattern 2.\n\n\nMales\nMale natural mortality and growth parameters in the following order by growth pattern.\n\n\n\nM\nNatural mortality for male GP1, where the number of natural mortality parameters depends on the option selected.\n\n\nCOND if M option = 1\n\n\n\n\nN breakpoints\nN-1 parameter lines as an exponential offsets from the previous reference age.\n\n\n\nLmin\nLength at Amin (units in cm) for male, growth pattern 1. In a two sex model, fixing the INIT value a 0 will assume the same Lmin as the female parameter value.\n\n\n\nLmax\nLength at Amax (units in cm) for male, growth pattern 1. In a two sex model, fixing the INIT value a 0 will assume the same Lmax as the female parameter value. \n\n\n\nVBK\nvon Bertanlaffy growth coefficient (units are per year) for males, growth pattern 1. In a two sex model, fixing the INIT value a 0 will assume the same k as the female parameter value.\n\n\nCOND if growth type = 2\n\n\n\n\nRichards Coefficient\nOnly include this parameter if Richards growth function is used. If included, a parameter value of 1.0 will have a null effect and produce a growth curve identical to Bertalanffy.\n\n\nCOND if growth type = 3\nAge-Specific K\n\n\n\nN parameter lines equal to the number K deviations for the ages specified above.\n\n\n\nCV young\nVariability for size at age &lt;= Amin for males, GP1. Note that CV cannot vary over time, so do not set up env-link or a deviation vector. Also, units are either as CV or as standard deviation, depending on assigned value of CV pattern.\n\n\n\nCV old\nVariability for size at age &gt;= Amax for males, growth pattern 1. For intermediate ages, do a linear interpolation of CV on means size-at-age. Note that the units for CV will depend on the CV pattern and the value of mortality-growth parameters as offset.\n\n\n\nWtLen scale\nCoefficient to convert length in cm to weight in kg for males.\n\n\n\nWtLen exp\nExponent to convert length to weight for males.\n\n\nCOND: growth pattern &gt; 1\nRepeat male parameters in the above order for growth pattern 2.\n\n\nCOND: Hermaphroditism\n3 parameters lines define a normal distribution for the transition rate of females to males (or vice versa).\n\n\n\nInflect Age\nHermaphrodite inflection age.\n\n\n\nStDev\nHermaphrodite standard deviation (in age) .\n\n\n\nAsmp Rate\nHermaphrodite asymptotic rate.\n\n\nCOND: Recruitment Distribution\n3 parameters lines defining recruitment distribution. See Recruitment Distribution and Parameters for more details about recruitment apportionment parameterization.\n\n\nMethod = 2\n\n\n\n\n\n\n\n\nRecruitment Dist. GP\nRecruitment apportionment by growth pattern, if multiple growth patterns, multiple entries required.\n\n\nRecruitment Dist. Area\nRecruitment apportionment by area, if multiple areas, multiple entries required.\n\n\nRecruitment Dist. Month\nRecruitment apportionment by month, if multiple months, multiple entries required.\n\n\nCOND: Recruitment Distribution\n1 parameter line for each settlement event defining the distribution of recruitment among them. See Recruitment Distribution and Parameters for more details about recruitment apportionment parameterization.\n\n\nMethod = 3\n\n\n\n\n\n\n\n\nRecruitment Dist. 1\nRecruitment apportionment parameter for the 1st settlement event.\n\n\nRecruitment Dist. 2\nRecruitment apportionment parameter for the 2nd settlement event.\n\n\nCohort growth deviation\nSet equal to 1.0 and do not estimate; it is deviations from this base that matter.\n\n\n2 x N selected movement pairs\nMovement parameters\n\n\nCOND: The following lines are only required when the associated features are turned on.\n\n\n\nAgeing Error\nTurned on in the data file.\n\n\n\nCatch Multiplier\nFor each fleet selected for this option in the data file.\n\n\nFraction female\n\nFraction\n\nfemale at the time of recruitment by growth pattern, if multiple growth patterns, multiple entries required.\n\n\nCOND: The following lines are only required when predator fleets are invoked.\n\n\n\nM2 Predator\nTurned on in the data file.\n\n\n\nExample format for mortality-growth parameter section with 2 sexes, 2 areas. Parameters marked with COND are conditional on selecting that feature:\n\n\n\n\n\n\nPrior\n&lt;other\nBlock\n\n\n\nLO\nHI\nINIT\nValue\nentries&gt;\nFxn\nParameter Label\n\n\n\n\n\nPrior\n&lt;other\nBlock\n\n\n\n0\n0.50\n0.15\n0.1\n...\n0\n#NatM_p_1_Fem_GP_1\n\n\n0\n45\n21\n36\n...\n0\n#L_at_Amin_Fem_GP_1\n\n\n40\n90\n70\n70\n...\n0\n#L_at_Amax_Fem_GP_1\n\n\n0\n0.25\n0.15\n0.10\n...\n0\n#VonBert_K_Fem_GP_1\n\n\n0.10\n0.25\n0.15\n0.20\n...\n0\n#CV_young_Fem_GP_1\n\n\n0.10\n0.25\n0.15\n0.20\n...\n0\n#CV_old_Fem_GP_1\n\n\n-3\n3\n2e-6\n0\n...\n0\n#Wtlen_1_Fem\n\n\n-3\n4\n3\n3\n...\n0\n#Wtlen_2_Fem\n\n\n50\n60\n55\n55\n...\n0\n#Mat50%_Fem\n\n\n-3\n3\n-0.2\n-0.2\n...\n0\n#Mat_slope_Fem\n\n\n-5\n5\n0\n0\n...\n0\n#Eggs/kg_inter_Fem\n\n\n-50\n5\n0\n0\n...\n0\n#Eggs/kg_slope_wt_Fem\n\n\n0\n0.50\n0.15\n0.1\n...\n0\n#NatM_p_1_Mal_GP_1\n\n\n0\n45\n21\n36\n...\n0\n#L_at_Amin_Mal_GP_1\n\n\n40\n90\n70\n70\n...\n0\n#L_at_Amax_Mal_GP_1\n\n\n0\n0.25\n0.15\n0.10\n...\n0\n#VonBert_K_Mal_GP_1\n\n\n0.10\n0.25\n0.15\n0.20\n...\n0\n#CV_young_Mal_GP_1\n\n\n0.10\n0.25\n0.15\n0.20\n...\n0\n#CV_old_Mal_GP_1\n\n\n-3\n3\n2e-6\n0\n...\n0\n#Wtlen_1_Mal\n\n\n-3\n4\n3\n3\n...\n0\n#Wtlen_2_Mal\n\n\n0\n0\n0\n0\n...\n0\n#RecrDist_GP_1\n\n\n0\n0\n0\n0\n...\n0\n#RecrDist_Area_1\n\n\n0\n0\n0\n0\n...\n0\n#RecrDist_Area_2\n\n\n0\n0\n0\n0\n...\n0\n#RecrDist_Settlement_1\n\n\n0.2\n5\n1\n1\n...\n0\n#Cohort_Grow_Dev\n\n\n-5\n5\n-4\n1\n...\n0\n#Move_A_seas1_GP1_from_1to2 (COND)\n\n\n-5\n5\n-4\n1\n...\n0\n#Move_B_seas1_GP1_from_1to2 (COND)\n\n\n-99\n99\n1\n0\n...\n0\n#AgeKeyParm1 (COND)\n\n\n-99\n99\n0.288\n0\n...\n0\n#Age_Key_Parms 2 to 5 (COND)\n\n\n-99\n99\n0.715\n0\n...\n0\n#Age_Key_Parm6 (COND)\n\n\n0.2\n3.0\n1.0\n0\n...\n0\n#Catch_mult_fleet1 (COND)\n\n\n0.001\n0.999\n0.5\n0.5\n...\n0\n#Frac_Female_GP_1\n\n\n-1.0\n2\n0\n0\n...\n0\n#PredM2_4\n\n\n\n\n\n\n\n8.5.10.1 Setting Male Parameters Equal to Females\n\nThe model allows a short-cut for males to use the same parameter values as female fish for natural mortality, length minimum (Length at Amin), maximum length (Length at Amax), coefficient at younger ages (CV1), coefficient at older ages (CV2), and the growth coefficient (K) when using offset option = 1 (the offset section has information on the options available). If the INIT parameter value for males is set equal to 0.0 and the phase set to negative, not estimated, each of these male parameters will use the corresponding female parameter value for the males.\n\n\n\n8.5.11 Time-varying Parameters\nPlease see the Time-Varying Parameter Specification and Setup section for details on how to set up time varying parameters. In short, additional short parameter lines will be needed after the long parameter lines. There are some additional considerations for time-varying growth.\n\n\n8.5.12 Seasonal Biology Parameters\nSeasonal effects are available for weight-length parameters, maturity, fecundity, and for the growth parameter K. The seasonal parameter values adjust the base parameter value for that season. \\[P'=P*exp(\\text{seas\\_value})\\]\n\n\n\nControl file continued:\n\n\n\n\nValue\n\nDescription\n\n\n\nSeasonality for selected biology parameters (not a conditional input). Read 10 integers to specify which biology parameters have seasonality: female-wtlen1, female-wtlen2, maturity1, maturity2, fecundity1, fecundity2, male-wtlen1, male-wtlen2, L1, K. Reading a positive value selects that factor for seasonality.\n\n\n\nCOND: If any factors have seasonality, then read N seasons parameters that define the\n\n\nseasonal offsets from the base parameter value.\n\n\n&lt;short parameter line(s)&gt;\nRead N seasons short parameter lines for each factor selected for seasonality. The parameter values define an exponential offset from the base parameter value."
  },
  {
    "objectID": "qmds/html.html#spawner-recruitment",
    "href": "qmds/html.html#spawner-recruitment",
    "title": "RAW HTML CONTENT",
    "section": "8.6 Spawner-Recruitment",
    "text": "8.6 Spawner-Recruitment\nThe spawner-recruitment section starts by specification of the functional relationship that will be used.\n\n\n\nControl file continued:\n\n\n\n\nValue\nLabel\nDescription\n\n\n3\nSpawner-\nThe options are:\n\n\n\nRecruitment\n2: Ricker: 2 parameters: ln(R0) and steepness (h);\n\n\n\nRelationship\n3: Standard Beverton-Holt, 2 parameters: ln(R0) and steepness;\n\n\n\n\n4: Ignore steepness and no bias adjustment. Use this in conjunction with very low emphasis on recruitment deviations to get CAGEAN-like unconstrained recruitment estimates, 2 parameters, but only uses the first one;\n\n\n\n\n5: Hockey stick:3 parameters: ln(R0), steepness, and \\(R_{\\text{min}}\\)) for ln(R0), fraction of virgin SSB at which inflection occurs, and the R level at SSB=0.0;\n\n\n\n\n6: Beverton-Holt with flat-top beyond Bzero, 2 parameters: ln(R0) and steepness;\n\n\n\n\n7: Survivorship function: 3 parameters: ln(R0), \\(z_{frac}\\), and \\(\\beta\\), suitable for sharks and low fecundity stocks to assure recruits are &lt;= population production;\n\n\n\n\n8: Shepherd re-parameterization: 3 parameters: ln(R0), steepness, and shape parameter, \\(c\\) (added to version 3.30.11 and is in beta mode); and\n\n\n\n\n9: Ricker re-parameterization: 3 parameters: ln(R0), steepness, and Ricker power, \\(\\gamma\\) (added to version 3.30.11 and is in beta mode).\n\n\n1\nEquilibrium recruitment\nUse steepness in initial equilibrium recruitment calculation\n\n\n\n\n0 = none; and\n\n\n\n\n1 = use steepness (h).\n\n\n0\nFuture Feature\nReserved for the future option to make realized \\(\\sigma_R\\) a function of the stock-recruitment curve.\n\n\n\n\n8.6.0.1 Equilibrium Recruitment\n\nIn principle, steepness should always be used when calculating equilibrium recruitment. This was not the default in early versions of Stock Synthesis, so has not come into common practice. The original logic, from early 1990s version of Stock Synthesis for long-lived U.S. west coast groundfish, was that fishing had not yet gone on long enough to have reduced spawning biomass enough to reduce expected recruitment noticeably for the chosen initial equilibrium year.\nSteepness should be used in the equilibrium calculation whenever you believe that the initial equilibrium catch was large enough to have reduced expected recruitment below R0. Note that when using this option, the initial equilibrium catch cannot be greater than MSY. SS3 uses the identical code for initial equilibrium catch and for MSY calculation, so it is axiomatic that MSY will be &lt;= initial equilibrium catch. So, SS3 will estimate a R0 large enough to make MSY &lt; initial equilibrium catch. Alternatively, you can elect to not use this option and instead add many years to the beginning of the time series with that same level of initial catch. In this case, it is not equilibrium, so the catch can reduce the population below BMSY.\n\n\n8.6.1 Spawner-Recruitment Functions\nThe number of age-0 fish is related to spawning biomass according to a stock-recruitment relationship. There are a number of options for the shape of the spawner-recruitment relationship: Beverton-Holt, Ricker, Hockey-Stick, and a survival-based stock recruitment relationship.\n\n\n\n\n8.6.1.1 Beverton-Holt\n\nThe Beverton-Holt Stock Recruitment curve is calculated as: \\[{R_y = \\frac{4hR_0SB_y}{SB_0(1-h)+SB_y(5h-1)}e^{-0.5b_y\\sigma^2_R+\\tilde{R}_y}\\qquad \\tilde{R}_y\\sim N(0;\\sigma^2_R)}\\]\nwhere \\(R_0\\) is the unfished equilibrium recruitment, \\(SB_0\\) is the unfished equilibrium spawning biomass (corresponding to \\(R_0\\)), \\(SB_y\\) is the spawning biomass at the start of the spawning season during year \\(y\\), \\(h\\) is the steepness parameter, \\(b_y\\) is the bias adjustment fraction applied during year \\(y\\), is the standard deviation among recruitment deviations in natural log space, and is the lognormal recruitment deviation for year \\(y\\). The bias-adjustment factor (Methot and Taylor 2011) ensures unbiased estimation of mean recruitment even during data-poor eras in which the maximum likelihood estimate of the recruitment deviation is near 0.0.\n\n\n\n\n\n8.6.1.2 Ricker\n\nThe Ricker Stock Recruitment curve is calculated as: \\[{R_y = \\frac{R_0SB_y}{SB_0}e^{h(1-SB_y/SB_0)}e^{-0.5b_y\\sigma^2_R+\\tilde{R}_y}\\qquad \\tilde{R}_y\\sim N(0;\\sigma^2_R)}\\]\nwhere the stock recruitment parameters have the same meaning as described above for the Beverton-Holt.\n\n\n\n\n\n8.6.1.3 Hockey-Stick\n\nThe hockey-stick recruitment curve is calculated as: \\[{R_y = join(R_{\\text{min}}R_0+R_0\\frac{SB_y}{hSB_0}(1-R_{\\text{min}}))+R_0(1-join)e^{-0.5b_y\\sigma^2_R+\\tilde{R}_y}\\qquad \\tilde{R}_y\\sim N(0;\\sigma^2_R)}\\] where \\(R_{\\text{min}}\\) is the minimum recruitment level predicted at a spawning size of zero and is set by the user in the control file, \\(h\\) is defined as the fraction of \\(SB_0\\) below which recruitment declines linearly, and \\(join\\) is defined as: \\[{ join = \\bigg[1+e^{1000*\\frac{(SB_0-hSB_0)}{SB_0}}\\bigg]^{-1} }\\]\n\n\n\n\n\n8.6.1.4 Survivorship\n\nThe survivorship stock recruitment relationship based on Taylor et al. (2013) is a stock-recruitment model that enables explicit modeling of survival between embryos and age 0 recruits, and allows the description of a wide range of pre-recruit survival curves. The model is especially useful for low fecundity species that produce relatively few offspring per litter and exhibit a more direct connection between spawning output and recruitment than species generating millions of eggs.\nSurvival-based recruitment is constrained so that the recruitment rate cannot exceed fecundity. The relationship between survival and spawning output is based on parameters which are on a natural log scale. These are: \\[z_0=-ln(S_0)\\] which is the negative of the natural log of the equilibrium survival \\(S_0\\), and can be thought of as a pre-recruit instantaneous mortality rate at equilibrium, and \\[z_{\\text{min}}=-ln(S_{\\text{max}})=z_0(1-z_{\\text{frac}})\\] which is the negative of the natural log of the maximum pre-recruit survival rate (\\(S_{\\text{max}}\\), the limit as spawning output approaches 0), and is parameterized as a function of \\(z_{\\text{frac}}\\) (which represents the reduction in mortality as a fraction of \\(z_0\\)) so the expression is well defined over the parameter range 0&lt;\\(z_{\\text{frac}}\\)&lt;1.\nRecruitment at age 0 for each year in the time series is calculated as: \\[{ R_y = SB_ye^{\\Big(-z_0 + (z_0-z_{min})\\big(1-(SB_y/SB_0)^\\beta \\big)\\Big)}e^{\\tilde{R}_y}\\qquad \\tilde{R}_y\\sim N(0;\\sigma^2_R)}\\] where \\(SB_y\\) is the spawning output in year y, \\(\\beta\\) is a parameter controlling the shape of density-dependent relationship between relative spawning depletion \\(SB_y/SB_0\\) and pre-recruit survival (with limit \\(\\beta\\) &lt; 1), \\(\\tilde{R}_y\\) is the recruitment in year \\(y\\), and \\(\\sigma_R\\) is the standard deviation of recruitment in natural log space.\nAs implemented in Stock Synthesis, the parameters needed to apply the stock-recruitment relationship based on the pre-recruit survival are ln(\\(R_0\\)), \\(z_{\\text{frac}}\\), and \\(\\beta\\). The value of ln(\\(R_0\\)) defines the equilibrium quantities of \\(SB_0\\), \\(S_0\\), and \\(z_0\\) for a given set of biological inputs (either estimated of fixed), regardless of the values of \\(z_{\\text{frac}}\\) and \\(\\beta\\).\nThe interpretation of the quantity \\(z_0=-ln(S_0)\\) as pre-recruit instantaneous mortality rate at unfished equilibrium is imperfect because the recruitment in a given year is calculated as a product of the survival fraction \\(S_y\\) and the spawning output \\(SB_y\\) for that same time period so that there is not a 1-year lag between quantification of eggs or pups and recruitment at age 0, which is when recruits are calculated within Stock Synthesis. However, if age 0 or some set of youngest ages is not selected by any fishery of survey, then density dependent survival may be assumed to occur anywhere before the first appearance of any cohort in the data or model expectations. In such cases, the upper limit on survival up to age \\(a\\) is given by \\(S_{\\text{max}}e^{-aM}\\).\nNevertheless, interpreting \\(z_0\\) as an instantaneous mortality helps with the understanding of \\(z_{\\text{frac}}\\). This parameter controls the magnitude of the density-dependent increase in survival associated with a reduction in spawning output. It represents the fraction by which this mortality-like rate is reduced as spawning output is reduced from \\(SB_0\\) to 0. This is approximately equal to the increase in survival as a fraction of the maximum possible increase in survival. That is: \\[z_{\\text{frac}}=\\frac{ln(S_{\\text{max}})-ln(S_0)}{-ln(S_0)} \\approx \\frac{S_{\\text{max}}-S_0}{1-S_0}\\] For example, if \\(S_0 = 0.4\\), \\(z_{\\text{frac}}=0.8\\), then the resulting fraction increase in survival is \\((S_{\\text{max}}-S_0)/(1-S_0)=0.72\\).\nThe parameter \\(\\beta\\) controls the point where survival changes fastest as a function of spawning depletion. A value of \\(\\beta\\) = 1 corresponds to a linear change in natural log survival and an approximately linear relationship between survival and spawning depletion. Values of \\(\\beta\\)&lt;1 have survival increasing fastest at low spawning output (concave decreasing survival) whereas \\(\\beta\\)&gt;1 has the increase in survival occurring fastest closer to the unfished equilibrium (convex decreasing survival).\nThe steepness (\\(h\\)) of the spawner-recruit curve (defined as recruitment relative to R0 at a spawning depletion level of 0.2) based on pre-recruit survival can be derived from the parameters discussed above according to the relationship and associated inequality: \\[h = 0.2e^{z_0z_{\\text{frac}}(1-0.2^\\beta)}&lt;0.2e^{z_0}=\\frac{1}{5S_0}=\\frac{SB_0}{5R_0}\\]\nUnlike the Beverton-Holt stock-recruitment relationship, recruitment can increase above \\(R_0\\) for stocks that are below \\(SB_0\\) and thus the steepness is not fundamentally constrained below 1. However, in many cases, steepness will be limited well below 1 by the inequality above, which implies an inverse relationship between the maximum steepness and equilibrium survival. Specifically, the inequality above bounds steepness below 1 for all cases where \\(S_0\\)&gt;0.2, which are those with the lowest fecundity, an intuitively reasonable result. For example, with \\(S_0\\)=0.4, the steepness is limited below 0.5, regardless of the choice of \\(z_{\\text{frac}}\\) or \\(\\beta\\). This natural limit on steepness may be one of the most valuable aspects of this stock-recruitment relationship.\nCode for the survival based recruitment can be found in SS_recruit.tpl, search for \"SS_Label_43.3.7 survival based\".\n\n\n8.6.1.5 Shepherd\n\nThe Shepherd stock recruit curve is calculated as: \\[R_y = \\bigg(\\frac{SB_y}{SB_0}\\bigg)\\frac{5h_{adj}R_0(1-0.2^c)}{(1-5h_{adj}0.2^c)+(5h_{adj}-1)(\\frac{SB_y}{SB_0})^c}e^{-0.5b_y\\sigma^2_R+\\tilde{R}_y}\\qquad \\tilde{R}_y\\sim N(0;\\sigma^2_R)\\] where c is the shape parameter for the stock recruitment curve, and \\(h_{adj}\\) is the transformed steepness parameter defined as: \\[h_{adj}=0.2+\\bigg(\\frac{h-0.2}{0.8}\\bigg)\\bigg(\\frac{1}{5*0.2^c}-0.2\\bigg)\\]\nMore details can be found in Punt and Cope (2019).\n\n\n8.6.1.6 Ricker Re-parameterization\n\nThe Ricker stock recruit curve re-parameterized version. More details can be found in Punt and Cope (2019). \\[R_y = R_0*(1-temp)*e^{ln(5h)(1-SB_y/SB_0)^{\\gamma}/0.8^{\\gamma}}\\] where \\(\\gamma\\) is the Ricker shape parameter and \\(temp\\) is defined as: \\[temp = \\begin{cases} 1-SB_y/SB_0 & \\text{if $1-SB_y/SB_0 &gt;$ 0 }\\\\ 0.001 & \\text{if $1-SB_y/SB_0 \\leq$ 0} \\end{cases}\\] where \\(temp\\) stabilizes recruitment at \\(R_0\\) if \\(SB_y &gt; SB_0\\).\n\n\n\n8.6.2 Spawner-Recruitment Parameter Setup\nRead the required number of long parameter set-up lines (e.g., LO, HI, INIT, PRIOR, PRIOR TYPE, SD, PHASE, ..., and BLOCK TYPE). These parameters are:\n\n\n\nValue\nLabel\nDescription\n\n\n\n\n8.5\nln(R0)\nLog of virgin recruitment level.\n\n\n0.60\nSteepness\nSteepness of spawner recruitment relationship, bound by 0.2 and 1.0 for the Beverton-Holt.\n\n\nCOND:\nIf SRR = 5, 7, or 8\n\n\n\n3rd Parameter\nOptional depending on which spawner-recruitment relationship function is used.\n\n\n0.60\n\\(\\sigma_R\\)\nStandard deviation of natural log recruitment. This parameter has two related roles. It penalizes deviations from the spawner-recruitment curve, and it defines the offset between the arithmetic mean spawner-recruitment curve (as calculated from ln(R0) and steepness) and the expected geometric mean (which is the basis from which the deviations are calculated. Thus the value of \\(\\sigma_R\\) must be selected to approximate the true average recruitment deviation. See Tuning \\(\\sigma_R\\) section below for additional guidance on how to tune \\(\\sigma_R\\).\n\n\n0\nRegime Parameter\nThis replaces the R1 offset parameter. It can have a block for the initial equilibrium year, so can fully replicate the functionality of the previous R1 offset approach. The SR regime parameter is intended to have a base value of 0.0 and not be estimated. Similar to cohort-growth deviation, it serves simply as a base for adding time-varying adjustments. This concept is similar to the old environment effect on deviates feature in SS3 v.3.24 and earlier.\n\n\n0\nAutocorrelation\nAutocorrelation in recruitment.\n\n\n\nExample set-up of the spawner-recruitment section:\n\n\n\n\nLO\nHI\nINIT\nPRIOR\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n\n\n3\n31\n8.81\n10.3\n...\n0\n#SR_LN(R0)\n\n\n0.2\n1\n0.61\n0.70\n...\n0\n#SR_BH_steep\n\n\n0\n2\n0.60\n0.80\n...\n0\n#SR_sigmaR\n\n\n-5\n5\n0\n0\n...\n0\n#SR_regime\n\n\n-99\n99\n0\n0\n...\n0\n#SR_autocorr\n\n\n\n\n\n\n8.6.3 Spawner-Recruitment Time-Varying Parameters\nThe R0, steepness, and regime shift parameters can be time-varying by blocks, trends, environmental linkages, or random deviations. Details on how to specify time-varying parameters can be found in the Time-Varying Parameter Specification and Setup section. However, not all of these options make sense for all parameters; please see additional details in the section on Time-Varying Stock-Recruitment Considerations.\n\n\n8.6.4 Tuning \\(\\sigma_R\\)\n The \\(\\sigma_R\\) value is generally not estimatable and it is recommended practice to tune input \\(\\sigma_R\\) values based on the variance in estimated recruitments post running SS3. The R package r4ss designed to read and visualize SS3 model results provides recommendations on adjusting \\(\\sigma_R\\) values in the sigma_R_info object in the list created by the r4ss::SS_output() function. An alternative \\(\\sigma_R\\) value is provided based on equation:\n\\[\\sigma_R^2 = Var(\\hat{r}) + \\overline{SE(\\hat{r}_y)}^2\\]\nSimulation studies conducted by (Methot and Taylor 2011) compared three methods of tuning \\(\\sigma_R\\) with the above approach generally performed best since it accounts for variability among recruitment deviations and the uncertainty in their estimates.\n\n\n8.6.5 Recruitment Deviation Setup\n\n\n\nControl file continued:\n\n\n\n\nValue\nLabel\nDescription\n\n\n1\nDo Recruitment Deviations\nThis selects the way in which recruitment deviations are coded:\n\n\n\n\n0: None (so all recruitments come from spawner recruitment curve).\n\n\n\n\n1: Deviation vector (previously the only option): the deviations are encoded as a deviation vector, so ADMB enforces a sum-to-zero constraint.\n\n\n\n\n2: Simple deviations: the deviations do not have an explicit constraint to sum to zero, although they still should end up having close to a zero sum. The difference in model performance between options (1) and (2) has not been fully explored to date.\n\n\n\n\n3: Deviation vector (added in v.3.30.13) where the estimated recruitment is equal to the R0 adjusted for blocks multiplied by a simple deviation vector of unconstrained deviations. The negative log likelihood from the deviation vector is equal to the natural log of the estimated recruitment divided by the expected recruitment by year adjusted for the spawner-recruit curve, regimes, environmental parameters, and bias-adjustment. The negative log likelihood between option 2 and 3 is approximately equal.\n\n\n\n\n4: Similar to option 3 but includes a penalty based on the sum of the deviations (added in v.3.30.13).\n\n\n1971\nMain recruitment deviations begin year\nIf begin year is less than the model start year, then the early deviations are used to modify the initial age composition. However, if set to be more than the population maximum age before start year, it is changed to equal to the maximum age before start year.\n\n\n2017\nMain recruitment deviations end year\nIf recruitment deviations end year is later than retro year, it is reset to equal retro year. The final year to estimate main recruitment deviations should be set to a year where information about young fish in the data becomes limited. As example, if the model end year is 2020 and the fleet/survey only starts observing fish of age 2+, the last year to estimate main recruitment deviations could be set to 2018. Years after the main period but before the end model year will be estimated as late deviations\n\n\n3\nMain recruitment deviations phase.\n\n\n\n1\nAdvanced\n0: Use default values for advanced options\n\n\n\nOptions\n1: Read values for the 11 advanced options.\n\n\nCOND = 1 Beginning of advanced options\n\n\n\n1950\nEarly Recruitment Deviation Start Year:\n\n\n\n\n0: skip (default);\n\n\n\n\n+year: absolute year (must be less than begin year of main recruitment deviations); and\n\n\n\n\n-integer: set relative to main recruitment deviations start year.\n\n\n\n\nNote: because this is a deviation vector, it should be long enough so that recruitment deviations for individual years are not unduly constrained.\n\n\n\n6\nEarly Recruitment Deviation Phase:\n\n\n\n\nNegative value: default value to not estimate early deviations.\n\n\n\n\nUsers may want to set to a late phase if there is not much early data.\n\n\n\n0\nForecast Recruitment Phase:\n\n\n\n\n0 = Default value.\n\n\n\n\nForecast recruitment deviations always begin in the first year after the end of the main recruitment deviations. Recruitment in the forecast period is deterministic derived from the specified stock-recruitment relationship. Setting their phase to 0 causes their phase to be set to max lambda phase +1 (so that they become active after rest of parameters have converged.). However, it is possible here to set an earlier phase for their estimation, or to set a negative phase to keep the forecast recruitment deviations at a constant level.\n\n\n\n1\nForecast Recruitment Deviations Lambda:\n\n\n\n\n1 = Default value.\n\n\n\n\nThis lambda is for the log likelihood of the forecast recruitment deviations that occur before endyr + 1. Use a larger value here if solitary, noisy data at end of time series cause unruly recruitment deviation estimation.\n\n\n\n1956\nLast year with no bias adjustment.\n\n\n\n1970\nFirst year with full bias adjustment.\n\n\n\n2001\nLast year with full bias adjustment.\n\n\n\n2002\nFirst recent year with no bias adjustment.\n\n\n\n\nThese four entries control how the bias adjustment is phased in and then phased back out when the model is searching for the maximum log likelihood. Bias adjustment is automatically turned off when in MCMC mode. For intervening years between the first and second years in this list, the amount of bias adjustment that will be applied is linearly phased in. The first year with full bias adjustment should be a few years into the data-rich period so that the model will apply the full bias-correction only to those recruitment deviations that have enough data to inform the model about the full range of recruitment variability. Defaults for the four year values: start year - 1000, start year - Nages, main recruitment deviation final year, end year + 1. See Recruitment Likelihood with Bias Adjustment for more information.\n\n\n\n0.85\nMax Bias Adjustment:\n\n\n\n\n&gt; 0: value for the maximum bias adjustment during the MLE mode.\n\n\n\n\n-1: bias adjustment set to 1.0 for all years, including forecast, with estimated recruitment deviations (similar to MCMC).\n\n\n\n\n-2: bias adjustment set to 1.0 for all years from start to end model year, bias adjustment set to 0 for the forecast period.\n\n\n\n\n-3: bias adjustment set to 0 for all model and forecast years.\n\n\n\n0\nPeriod For Recruitment Cycles:\n\n\n\n\nUse this when the model is configured to model seasons as years and there is a need to impose a periodicity to the expected recruitment level. If value is &gt;0, then read that number of full parameter lines below define the recruitment cycle. See more information on setting up a seasons as years model in the continuous seasonal recruitment section.\n\n\n\n-5\nMinimum Recruitment Deviation: Min value for recruitment deviation.\n\n\n\n\nNegative phase = Default value.\n\n\n\n5\nMaximum Recruitment Deviation: Max value for recruitment deviation.\n\n\n\n\nLate Phase = Default value (e.g., 5)\n\n\n\n5\nNumber of Explicit Recruitment Deviations to Read:\n\n\n\n\n0: Default, do not read any recruitment deviations; Integer: read this number of recruitment deviations.\n\n\nCOND = If N recruitment cycle is &gt; 0, enter N full parameter lines below.\n\n\n\n&lt;parameter line&gt;\nFull parameter line for each of the N periods of recruitment cycle.\n\n\nCOND = If the number of recruitment deviations to read is &gt; 0, then enter a total number of lines\n\n\nequal to that specified above. A year and a deviation value is expected as:\n\n\n\n2010 1.27\nEnter year and deviation.\n\n\n\n2011 -0.45\nTwo example recruitment deviations being read. Note: the model will rescale the entire vector of recruitment deviation after reading these deviations, so by reading two positive values, all other recruitment deviations will be scaled to a small negative value to achieve a sum to zero condition before starting model estimation.\n\n\n\n2012 -0.25\n\n\n\n\n2013 0.67\n\n\n\n\n2014 0.20\n\n\n\n\n2015 -0.11\n\n\n\nEnd of advanced options\n\n\n\n\n8.6.5.1 Recruitment Eras\n\nConceptually, the model treats the early, data-poor period, the main data-rich period, and the recent/forecast time period as three eras along a continuum. The user has control of the break year between eras. Each era has its own vector. The early era is defined as a vector (prior to V3.10 this was a deviation vector) so it can have zeros during the earliest years not informed by data and then a few years with non-zero values without imposing a zero-centering on this collection of deviations. The main era can be a vector of simple deviations, or a deviation vector but it is normally implemented as a deviation vector so that the spawner-recruitment function is its central tendency. The last era does not force a zero-centered deviation vector so it can have zeros during the actual forecast and non-zero values in last few years of the time series. The early and last eras are optional, but their use can help prevent balancing a preponderance of negative deviations in early years against a preponderance of positive deviations in later years. When the 3 eras are used, it would be typically to turn on the main era during an early model phase, turn on the early era during a later phase, then have the last era turn on in the final phase.\n\n\n\n\n\n8.6.5.2 Recruitment Likelihood with Bias Adjustment\n\nFor each year in the total recruitment deviation time series (early, mid, late/forecast) the contribution of that year to the log likelihood is equal to: \\(dev^2/(2.0*\\sigma^2_R)+offset*ln(\\sigma_R)\\) where offset is the recruitment bias adjustment between the arithmetic and geometric mean of expected recruitment for that year. With this approach, years with a zero or small offset value do not contribute to the second component. \\(\\sigma_R\\) may be estimable when there is good data to establish the time series of recruitment deviations.\nThe implemented recruitment bias adjustment is based upon the work documented in Methot and Taylor (2011) and following the work of Mark N. Maunder and Deriso (2003). The concept is based upon the following logic. The \\(\\sigma_R\\) represents the true variability of recruitment in the population. It provides the constraining penalty for the estimates of recruitment deviations and it is not affected by data. Where data that are informative about recruitment deviations are available, the total variability in recruitment, \\(\\sigma_R\\), is partitioned into a signal (the variability among the recruitment estimates) and the residual, the variance of each recruitment estimate calculated as:\n\\[SE(\\hat{r}_y)^2 + SD(\\hat{r})^2=\\Bigg( \\bigg( \\frac{1}{\\sigma^2_d}+\\frac{1}{\\sigma^2_R}\\bigg)^{-1/2}\\Bigg)^2+\\Bigg( \\frac{\\sigma^2_R}{(\\sigma^2_R+\\sigma^2_d)^{1/2}}\\Bigg)^2=\\sigma^2_R\\]\nWhere there are no data, no signal can be estimated and the individual recruitment deviations collapse towards 0.0 and the variance of each recruitment deviation approaches \\(\\sigma_R\\). Conversely, where there highly informative data about the recruitment deviations, then the variability among the estimated recruitment deviations will approach \\(\\sigma_R\\) and the variance of each recruitment deviation will approach zero. Perfect data will estimate the recruitment time series signal perfectly. Of course, we never have perfect data so we should always expect the estimated signal (variability among the recruitment deviations) to be less than the true population recruitment variability.\nThe correct offset (bias adjustment) to apply to the expected value for recruitment is based on the concept that a time series of estimated recruitments should be mean unbiased, not median unbiased, because the biomass of a stock depends upon the cumulative number of recruits, which is dominated by the large recruitments. The degree of offset depends upon the degree of recruitment signal that can be estimated. Where no recruitment signal can be estimated, the median recruitment is the same as the mean recruitment, so no offset is applied. Where lognormal recruitment signal can be estimated, the mean recruitment will be greater than the median recruitment. The value:\n\\[b_y=\\frac{E\\Big( SD(\\hat{r}_y)\\Big)^2}{\\sigma^2_R}=1-\\frac{SE(\\hat{r}_y)^2}{\\sigma^2_R}\\]\nof the offset then depends upon the partitioning of \\(\\sigma_R\\) into between and within recruitment variability. The most appropriate degree of bias adjustment can be approximated from the relationship among \\(\\sigma_R\\), recruitment variability (the signal), and recruitment residual error. Because the quantity and quality of data varies during a time series, the user can control the rate at which the offset is ramped in during the early, data-poor years, and then ramped back to zero for the forecast years. On output, the mean bias adjustment during the early and main eras is calculated, comparing this value to the RMSE of estimated recruitment deviations in the report.sso file. A warning is generated if the RMSE is small and the bias adjustment is larger than 2.0 times the ratio of \\(RMSE^2\\) to \\(\\sigma^2_R\\). Additional information on recruitment bias adjustment can be found in the Recruitment Variability and Bias Correction section.\nIn MCMC mode, the model still draws recruitment deviations from the lognormal distribution, so the full offset is used such that the expected mean recruitment from this lognormal distribution will stay equal to the mean from the spawner-recruitment curve. When the model reaches the MCMC and MCEVAL phases, all bias adjustment values are set to 1.0 for all active recruitment deviations because the model is now re-sampling from the full lognormal distribution of each recruitment.\n\n\n8.6.5.3 Recruitment Cycle\n\nWhen the model is configured such that seasons are modeled as years, the concept of season within year disappears. However, there may be reason to still want to model a repeating pattern in expected recruitment to track an actual seasonal cycle in recruitment. If the recruitment cycle factor is set to a positive integer, this value is interpreted as the number of time units in the cycle and this number of full parameter lines will be read. The cyclic effect is modeled as an exp(p) factor times R0, so a parameter value of 0.0 has nil effect. In order to maintain the same number of total recruits over the duration of the cycle, a penalty is introduced so that the cumulative effect of the cycle produces the same number of recruits as Ncycles * R0. Because the cyclic factor operates as an exponential, this penalty is different than a penalty that would cause the sum of the cyclic factors to be 0.0. This is done by adding a penalty to the parameter likelihood.\n\n\n8.6.5.4 Initial Age Composition\n\nA non-equilibrium initial age composition is achieved by setting the first year of the recruitment deviations before the model start year. These pre-start year recruitment deviations will be applied to the initial equilibrium age composition to adjust this composition before starting the time series. The model first applies the initial F level to an equilibrium age composition to get a preliminary N-at-age vector and the catch that comes from applying the F’s to that vector, then it applies the recruitment deviations for the specified number of younger ages in this vector. If the number of estimated ages in the initial age composition is less than maximum age, then the older ages will retain their equilibrium levels. Because the older ages in the initial age composition will have progressively less information from which to estimate their true deviation, the start of the bias adjustment should be set accordingly."
  },
  {
    "objectID": "qmds/html.html#fishing-mortality-method",
    "href": "qmds/html.html#fishing-mortality-method",
    "title": "RAW HTML CONTENT",
    "section": "8.7 Fishing Mortality Method",
    "text": "8.7 Fishing Mortality Method\nThere are four methods available for calculation of fishing mortality (F): 1) Pope’s approximation, 2) Baranov’s continuous F with each F as a model parameter, 3) a hybrid F method, and 4) a fleet-specific parameter hybrid F approach (introduced in version 3.30.18).\nA new fleet-specific parameter hybrid F approach was introduced in version 3.30.18 and is now the recommended approach for most models. With this approach, some fleets can stay in hybrid F mode while others transition to parameters. For example, bycatch fleets must start with parameters in phase 1, while other fishing fleets can use hybrid F or start with hybrid and transition to parameters at a fleet-specific designated phase. We believe this new method 4 is a superior super-set to current methods 2 (all use parameters and all can start hybrid then switch to parameters) and method 3 (all hybrid for all phases). However, during testing specific situations were identified when this approach may not be the best selection. If there is uncertainty around annual input catch values (e.g., se = 0.15) and some fleets have discard data being fit to as well, the treatment of F as parameters (method 2) may allow for better model fits to the data.\nThe hybrid F method does a Pope’s approximation to provide initial values for iterative adjustment of the Baranov continuous F values to closely approximate the observed catch. Prior to version 3.30.18, the hybrid method (method 3) was recommended in most cases. With the hybrid method, the final values are in terms of continuous F, but do not need to be specified as full parameters. In a 2 fishery model, low F case (e.g., similar to natural mortality or lower), the hybrid method is just as fast as the Pope approximation and produces identical results.\nHowever, when F is very high, the problem becomes quite computationally stiff for Pope’s approximation and the hybrid method so convergence in ADMB may slow due to more sensitive gradients in the log likelihood. In these high F cases it may be better to use F option 2, continuous F as full parameters. It is also advisable to allow the model to start with good values for the F parameters. This can be done by specifying a later phase (&gt;1) under the conditional input for F method = 2 where early phases will use the hybrid method, then switch to F as parameter in later phases and transfer the hybrid F values to the parameter initial values.\nF as parameter is also preferred for situations where catch is known imprecisely and it is acceptable to have a solution in which the final F values do not reproduce the input catch levels exactly.\nThe calculation of the catch log-likelihood for the Baranov F method (method 2) and the Hybrid F method (method 3) is identical with the only difference between the methods is how the expected catch is calculated. With F method 2, F is a parameter (or set of parameters for all the fleets). The model uses that parameter(s) to go straight to calculating expected catch for each fleet, then to the log-likelihood. F method 2 includes option to do F method 3 in early phases and then transition to F method 2 to finish (which can be faster in high F, many fleet situations). With F method 3, the model does a Pope’s (F method 1) to get each F in right ballpark and then adjusts it through a few iterations to get closer to matching the observed catch. It then finishes through the F method 2 code to calculate the final iteration of expected catch and then log-likelihood. At high F and/or with many fleets it may not get to an exact match to the observed catch.\nFinally, the newer fleet-specific hybrid F method (F method 4) provides more flexibility in that some fleets can do hybrid and other fleets (particularly high F fleets) can do parameters starting at a fleet-specific phase. This new approach is considered the prefer F method.\n\n\n\nControl file continued:\n\n\n\n\nTypical Value\nDescription and Options\n\n\n0.2\n\nF ballpark\n\n\n\n\nThis value is compared to the sum of the F’s for the specified year (defined on the next line). The sum is over all seasons and areas. In older versions of SS3, the lambda was automatically set to 0.0 in the final phase, the user now has control over whether to reduce the lambda in later phases.\n\n\n-1990\n\nF ballpark year\n\n\n\n\nNegative value disable F ballpark.\n\n\n3\n\nF Method\n\n\n\n\n1 = Pope’s (discrete);\n\n\n\n\n2 = Baranov (continuous) F as a parameter;\n\n\n\n\n3 = Hybrid F; and\n\n\n\n\n4 = Fleet-specific parameter/hybrid F (recommended).\n\n\n2.9\n\nMaximum F\n\n\n\n\nThis maximum is applied within each season and area. A value of 0.9 is recommended for F method 1, and a value of about 4 is recommended for F method 2 and 3.\n\n\nCOND: F method = 1, no additional input for Pope’s approximation.\n\n\nCOND: F method = 2:\n\n\n\n0.10 1 0\nInitial F value, phase, and the number of F detail setup lines to read (example has 0).\n\n\n\n\nFor phases prior to the phase of the F value becoming active, the hybrid option will be used and the F values so calculated become the starting values for the F parameters when this phase is reached.\n\n\nIf F method = 2 and N for F detail is &gt; 0\n\n\n1 1980 1 0.20 0.05 4\nFleet, year, season, F, SE, phase - these values override the catch standard error values in the data file and the overall starting F value and phase read just above.\n\n\nCOND: F method = 4\n\n\n\n\nRead list of fleets needing parameters, starting F values, and phases. To treat a fleet F as hybrid only select a phase of 99. A parameter line is not required for all fleets and if not specified will be treated as hybrid across all phases, except for bycatch fleets which are required to have an input parameter line. Use a negative phase to set F as constant (i.e., not estimated) in v. 3.30.19 and higher.\n\n\nFleet\nParameter Value\nPhase\n\n\n1\n0.05\n1\n\n\n2\n0.01\n1\n\n\n-9999\n1\n1\n\n\nCOND: F method = 3 or 4\n\n\n\n4\nNumber of tuning iterations in hybrid fleets. A value of 3 is sufficient with a single fleet and low Fs. A value of 5 or so may be needed to match the catch near exactly when there are many fleets and high F.\n\n\n\n\n\n\n\n8.7.1 Initial Fishing Mortality\nRead a short parameter setup line for each fishery and season when there is non-zero eqilibirium catch in a season for the fleet (equilibrium catches are input in the catch section of the data file). The parameters are the fishing mortalities for the initial equilibrium catches. Do not try to estimate parameters for fisheries with zero initial equilibrium catch - no parameter line is needed fleets and seasons with zero equilibirium catch.\nIf there is catch, then give a starting value greater than zero and it generally is best to estimate the parameter in phase 1. The initial F parameter lines are ordered as shown in the example below - by season, then within a season, by fleet.\nIt is possible to use the initial F method to achieve an estimate of the initial equilibrium Z in cases where the initial equilibrium catch is unknown. To do this requires 2 changes to the input files:\n\nData File: Include a positive value for the initial equilibrium catch for at least one fleet, often with a higher standard error depending upon the situation.\nControl File: Add an initial F parameter line (short parameter line) for each fleet and season with initial equilibrium catch to be estimated immediately after the Fishing Mortality set-up. It will be influenced by the early age and size comps which should have some information about the early levels of Z.\n\n\n\n\nAn example setup with two fishery fleets and two seasons with initial equilibrium catches:\n\n\n\n\n0.1\n# F ballpark\n\n\n-2001\n# F ballpark year (negative value to disable)\n\n\n3\n# F method: 1=Pope; 2=Baranov; 3=Hybrid\n\n\n3\n# Maximum F value\n\n\n4\n# Number of iterations for tuning F in hybrid method\n\n\n# Initial F parameters\n\n\nLO\nHI\nINIT\nPrior\nPr. SD\nPr. Type\nPhase\nLabel\n\n\n0\n3\n0.1\n0\n99\n0\n1\n#InitF_seas_1_fleet_1\n\n\n0\n3\n0.1\n0\n99\n0\n1\n#InitF_seas_1_fleet_2\n\n\n0\n3\n0.1\n0\n99\n0\n1\n#InitF_seas_2_fleet_1\n\n\n0\n3\n0.1\n0\n99\n0\n1\n#InitF_seas_2_fleet_2"
  },
  {
    "objectID": "qmds/html.html#catchability",
    "href": "qmds/html.html#catchability",
    "title": "RAW HTML CONTENT",
    "section": "8.8 Catchability",
    "text": "8.8 Catchability\nCatchability is the scaling factor that relates a model quantity to the expected value for some type of data (index). Typically this is used to converted selected numbers or biomass for a fleet into the expected value for a survey or CPUE by that fleet. In SS3, the concept has been extended so that, for example, a time series of an environmental factor could be treated as a survey of the time series of deviations for some parameter. This flexibility means that a family of link functions beyond simple proportionality is needed.\nFor each fishery and survey with an index, enter a row with the entries as described below:\n\nFleet Number\nLink type or index of deviation vector: An assumed functional form between Q, the expected value, and the survey observation.\n\n1 = simple Q, proportional assumption about Q: \\(y=q*x\\).\n2 = mirror simple Q - this will mirror the Q value from another fleet. Mirror in Q must refer to a lower number fleet relative to the fleet with the mirrored Q (example: fleet 3 mirror fleet 2). Requires a Q parameter line for the fleet but will not be used.\n3 = Q with power, 2 parameters establish a parameter for non-linearity in survey-abundance linkage. Assumes proportional with offset and power function: \\(y=qx^c\\) where \\(q = exp(lnQ_{base}))\\) thus the \\(c\\) is not related to expected biomass but vulnerable biomass to Q. Therefore, \\(c\\) \\(&lt;\\) 0 leads to hyper-stability and \\(c &gt; 0\\) leads to hyper-depletion.\n4 = mirror Q with offset (2 parameter lines required). The mirrored Q with offset for with be reported as base Q + offset value. Mirror in Q must refer to a lower number fleet relative to the fleet with the mirrored Q. See mirrored Q with offset below for example set up.\nIf the parameter is for an index of a deviation vector (index units = 35), use this column to enter the index of the deviation vector to which the index is related.\n\nExtra input for link information (i.e., mirror fleet)\n\n&gt;0 = mirror the Q from another (lower numbered survey designated by referencing the fleet number)\nIf a depletion survey, option 34, approach is being applied the following values in this column determines how phases are adjusted:\n\n0 = add 1 to phases of all parameters. Only R0 active in new phase 1. Mimics the default option of previous model versions;\n1 = only R0 active in phase 1. Then finish with no other parameters becoming active; useful for data-limited draws of other fixed parameters. Essentially, this option allows the model to mimic DB-SRA; and\n2 = no phase adjustments, can be used when profiling on fixed R0.\n\n\nEstimate extra standard error for an index\n\n0 = skip (typical); and\n1 = estimate a parameter that will contain an additive constant to be added to the input standard deviation of the survey variability.\n\nBias adjustment - adjusts for log-normal bias when using an informative prior on Q.\n\n0 = no bias adjustment applied; and\n1 = apply bias adjustment. Bias correction will be applied to the estimated Q value.\n\nQ float\n\n0 = no float, parameter is estimated; and\n1 = float, analytical solution is used, but parameter line still required.\nAdditional information regarding the use of and appropriate application of float in Q can be found in the Float Q section below.\n\n\n\n\n\nFor a setup with a single survey, the Q setup matrix could be:\n\n\n\n\nFleet\nLink\nLink\nExtra\nBias\n\n\n\n\nNum.\nType\nInfo\nSD\nAdjust\nFloat\nLabel\n\n\n3\n1\n0\n1\n1\n0\n#Survey\n\n\n-9999\n0\n0\n0\n0\n0\n#End Read\n\n\n\n\n\n\nLO\nHI\nINIT\n&lt;other entries&gt;\nPHASE\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n\n\n-5\n5\n-0.12\n...\n1\n...\n0\n#Survey1 LnQ base\n\n\n0\n0.5\n0.1\n...\n-1\n...\n0\n#Survey1 Extra SD\n\n\n\nIf the Q base parameter specifies that it is time-varying by the annual deviation method, short parameter lines to specify the specifications of the deviation vector come after all the base Q parameters.\n\n\n\n\n8.8.1 Mirrored Q with offset\nBelow is an example setup for fleets with a mirrored Q and offset from another fleet (link type = 4):\n\n\n\nFor a setup with a single survey, the Q setup matrix could be:\n\n\n\n\nFleet\nLink\nLink\nExtra\nBias\n\n\n\n\nNum.\nType\nInfo\nSD\nAdjust\nFloat\nLabel\n\n\n1\n1\n0\n1\n0\n0\n#Fleet 1\n\n\n2\n4\n1\n0\n0\n0\n#Fleet 2\n\n\n-9999\n0\n0\n0\n0\n0\n#End Read\n\n\n\n\n\n\nA long parameter line is expected for each link parameter (i.e. Q) and for the\n\n\n\n\nLO\nHI\nINIT\n&lt;other entries&gt;\nPHASE\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n-7\n5\n0.51\n...\n1\n...\n0\n#Fleet 1 LnQ base\n\n\n0\n0.5\n0.1\n...\n-1\n...\n0\n#Fleet 1 Extra SD\n\n\n-7\n5\n-6\n...\n-1\n...\n0\n#Fleet 2 LnQ base\n\n\n-8\n5\n-7\n...\n-1\n...\n0\n#Fleet 2 Mirror Q offset\n\n\n\n\n\n\n\n\n8.8.2 Float Q\nThe use and development of float in Q has evolved over time within SS3. The original approach in earlier versions of SS3 (version 3.24 and before) is that with Q \"float\" the units of the survey or fishery CPUE were treated as dimensionless so the Q was adjusted within each model iteration to maintain a mean difference of 0.0 between observed and expected (usually in natural log space). In contrast, Q as a parameter (float = 0) one had the ability to interpret the absolute scaling of Q and put a prior on it to help guide the model solution. Also, with Q as a parameter the code allowed for Q to be time-varying.\nThen midway through the evolution of the SS3 v.3.24 code lineage a new Q option was introduced based on user recommendations. This option allowed Q to float and to compare the resulting Q value to a prior, hence the information in that prior would pull the model solution in direction of a floated Q that came close to the prior.\nCurrently, in 3.30, that float with prior capability is fully embraced. All fleets that have any survey or CPUE options need to have a catchability specification and get a base Q parameter in the list. Any of these Q’s can be either:\n\nFixed: by not floating and not estimating.\nFloating: not estimated as an active parameter, set phase to negative, and not capable of being time-varying. Can have a prior, or not. Future versions may allow Q to be time-varying and then rescaling that Q vector according to the float logic but this is not yet currently implemented.\nEstimated as active parameter: so not floating. Can be time-varying and can have a prior.\n\nQ relates the units of the survey or CPUE to the population abundance, not the population density per unit area. But many surveys and most fishery CPUE is a proportional to mean fish density per unit area. This does not have any impact in a one area model because the role of area is absorbed into the value of Q. In a multi-area model, one may want to assert that the relative difference in CPUE between two areas is informative about the relative abundance between the areas. However, CPUE is a measure of fish density per unit area, so one may want to multiply CPUE by area before putting the data into the model so that asserting the same Q for the two areas will be informative about relative abundance.\nIn SS3 v.3.30.13, a new catchability option has been added that allows Q to be mirrored and to add an offset to ln(Q) of the primary area when calculating the ln(Q) for the dependent area. The offset is a parameter and, hence, can be estimated and have a prior. This option allows the CPUE data to stay in density units and the effect of relative stock area is contained in the value of the ln(Q) offset.\n\n\n8.8.3 Catchabilty Time-Varying Parameters\nTime-Varying catchability can be used. Details on how to specify time-varying parameters can be found in the Time-Varying Parameter Specification and Setup section.\n\n\n8.8.4 Q Conversion Issues Between SS3 v.3.24 and v.3.30\nIn SS3 v.3.24 it was common to use the deviation approach implemented as if it was survey specific blocks to create a time-varying Q for a single survey. In some cases, only one year’s deviation was made active in order to implement, in effect, a block for Q. The transition executable (sstrans.exe) cannot convert this, but an analogous approach is available in SS3 v.3.30 because true blocks can now be used, as well as environmental links and annual deviations. Also note that deviations in SS3 v.3.24 were survey specific (so no parameter for years with no survey). In SS3 v.3.30, deviations are always year-specific, so you might have a deviation created for a year with no survey."
  },
  {
    "objectID": "qmds/html.html#selectivity-and-discard",
    "href": "qmds/html.html#selectivity-and-discard",
    "title": "RAW HTML CONTENT",
    "section": "8.9 Selectivity and Discard",
    "text": "8.9 Selectivity and Discard\nFor each fleet and survey, read a definition line for size selectivity and retention.\n\n\n\n\nExample Setup for Selectivity:\n\n\n\n\nSize Selectivity:\n\n\nPattern\nDiscard\nMale\nSpecial\nLabel\n\n\n1\n2\n0\n0\n#Fishery1\n\n\n1\n0\n0\n0\n#Survey1\n\n\n0\n0\n0\n0\n#Survey2\n\n\nAge Selectivity:\n\n\nPattern\nDiscard\nMale\nSpecial\nLabel\n\n\n11\n0\n0\n0\n#Fishery1\n\n\n11\n0\n0\n0\n#Survey1\n\n\n11\n0\n0\n0\n#Survey2\n\n\n\n\n\n8.9.0.1 Pattern\n\nSpecify the size/age selectivity pattern. See the Selectivity Pattern section for user options.\n\n\n8.9.0.2 Discard\n\nDiscard options:\n\nOption = 0: no discarding by fleet.\nOption = 1: program will read 4 retention parameters after reading the specified number of selectivity parameters and all discarded fish are assumed dead.\nOption = 2: program will read 4 retention parameters and 4 discard mortality parameters.\nOption = 3: no additional parameters are read and all fish are assumed discarded and dead.\nOption = 4: program will read 7 retention parameters for dome-shaped retention and 4 discard mortality parameters.\nOption = negative fleet number: will mirror the retention and discard mortality pattern of the lower numbered fleet.\n\n\n\n8.9.0.3 Male\n\nMale specific selectivity options:\n\nOption = 0: no male specific selectivity parameters required. Male and female selectivity will be the same.\nOption = 1: program will read 4 additional parameters to define the male selectivity relative to the female selectivity. Anytime the male selectivity is caused to be greater than 1.0; the entire male/female matrix of selectivity values is scaled by the max so that the realized max is 1.0. Note, this may cause gradient problems.\nOption 2: main selectivity parameters define male selectivity and female selectivity is estimated as an offset from male selectivity. This alternative is preferable if female selectivity is less than male selectivity.\nOption 3: only available if the selectivity pattern is 1, 20, or 24 and it causes the male selectivity parameters to be offset from the female parameters, rather than the male selectivity being an offset from the female selectivity.\nOption 4: similar to Option 3 only with the female parameters offset from the male parameters.\n\n\n\n8.9.0.4 Special\n\nSpecial (0/value): This value is used in different ways depending on the context. If the selectivity type is to mirror another selectivity type, then put the index of that source fleet or survey here. It must refer to a lower numbered fleet/survey. If the selectivity type is 6 (linear segment), then put the number of segments here. If the selectivity type is 7, then put a 1 here to keep selectivity constant above the mean average size for old fish of morph 1. If selectivity type is 27 (cubic spline), then put the number of nodes (knots) here.\n\n\n8.9.0.5 Age Selectivity\n\nFor each fleet and survey, read a definition line for age selectivity. The 4 values to be read are the same as for the size-selectivity.\nAs of SS3 v.3.30.15, for some selectivity patterns the user can specify the minimum age of selected fish. Most selectivity curves by default select age 0 fish (i.e., inherently specify the minimum age of selected fish as 0). However, it is fairly common for the age bins specified in the data file to start at age 1. This means that any age 0 fish selected are pooled up into the age 1’ bin, which will have a detrimental effect on fitting age-composition data. In order to prevent the selection of age 0 (or older) fish, the user can specify the minimum selected age for some selectivity patterns (12, 13, 14, 16, 18, 26, or 27) in versions of SS3 v.3.30.15 and later. For example, if the minimum selected age is 1 (so that age 0 fish are not selected), selectivity pattern type can be specified as 1XX, where XX is the selectivity pattern. A more specific example is if selectivity is age-logistic and the minimum selected age desired is 1, the selectivity pattern would be specified as 112 (the regular age-logistic selectivity pattern is option 12). The user can also select higher minimum selected ages, if desired; for example, 212 would be the age-logistic selectivity pattern with a minimum selected age of 2 (so that age 0 and 1 fish are not selected).\n\n\n8.9.1 Reading the Selectivity and Retention Parameters\nRead the required number of parameter setup lines as specified by the definition lines above. The complete order of the parameter setup lines is:\n\nSize selectivity for fishery 1;\nRetention for fishery 1 (if discard specified);\nDiscard Mortality for fishery 1 (if discard specified);\nMale offsets for size selectivity for fishery 1 (if offsets used);\n;\nAge selectivity for fishery 1;\nRetention for fishery 1 (if discard specified);\nDiscard Mortality for fishery 1 (if discard specified);\nMale offsets for age selectivity for fishery 1 (if offsets used); and\n.\n\n\n\n\nThe list of parameters to be read from the above setup would be:\n\n\n\n\nLO\nHI\nINIT\nPRIOR\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n19\n80\n53.5\n50\n...\n0\n#SizeSel p1 fishery 1\n\n\n0.01\n60\n18.9\n15\n...\n0\n#SizeSel p2 fishery 1\n\n\n20\n70\n38.6\n40\n...\n0\n#Retain_L_infl_fishery 1\n\n\n0.1\n10\n6.5\n1\n...\n0\n#Retain_L_width_fishery 1\n\n\n0.001\n1\n0.98\n1\n...\n0\n#Retain_L_asymptote_logit_fishery 1\n\n\n-10\n10\n1\n0\n...\n0\n#Retain_L_maleoffset_fishery 1\n\n\n0.1\n1\n0.6\n0.6\n...\n0\n#DiscMort_L_infl_fishery 1\n\n\n-2\n2\n0\n0\n...\n0\n#DiscMort_L_width_fishery 1\n\n\n20\n70\n40\n40\n...\n0\n#DiscMort_L_level_old_fishery 1\n\n\n0.1\n10\n1\n1\n...\n0\n#DiscMortt_L_male_offset_fishery 1\n\n\n19\n80\n53.5\n50\n...\n0\n#SizeSel p1 survey 1\n\n\n0.01\n60\n18.9\n15\n...\n0\n#SizeSel p2 survey 1\n\n\n0\n40\n0\n5\n...\n0\n#AgeSel p1 fishery 1\n\n\n0\n40\n40\n5\n...\n0\n#AgeSel p2 fishery 1\n\n\n0\n40\n0\n5\n...\n0\n#AgeSel p1 survey 1\n\n\n0\n40\n40\n5\n...\n0\n#AgeSel p2 survey 1\n\n\n0\n40\n0\n5\n...\n0\n#AgeSel p1 survey 2\n\n\n0\n40\n0\n5\n...\n0\n#AgeSel p2 survey 2\n\n\n\n\n\n\n\n\n8.9.2 Selectivity Patterns\nThe currently defined selectivity patterns, and corresponding required number of parameters, are:\n\n\n\nSIZE BASED SELECTIVITY\n\n\n\n\nPattern\nN Parameters\nDescription\n\n\n0\n0\nSelectivity = 1.0 for all sizes.\n\n\n1\n2\nLogistic selectivity.\n\n\n2\n6\nOlder version of selectivity pattern 24 for backward compatibility in treatment of sex-specific scaling\n\n\n5\n2\nMirror another selectivity. The two parameters select bin range.\n\n\n6\n2 + special\nNon-parametric\n\n\n8\n8\nDouble logistic, with defined peak, uses smooth joiners; special=1 causes constant selectivity above \\(L_{inf}\\) for morph 1. Recommend using pattern 24 instead.\n\n\n9\n6\nSimple double logistic with no defined peak.\n\n\n11\n2\nSelectivity = 1.0 for a specified length-bin range.\n\n\n15\n0\nMirror another selectivity (same as for age selectivity).\n\n\n21\n2\nNon-parametric size selectivity\n\n\n22\n4\nDouble normal; similar to CASAL.\n\n\n23\n6\nSame as the double normal pattern 24 except the final selectivity is now directly interpreted as the terminal selectivity value. Cannot be used with Pope’s F method because maximum selectivity may be greater than 1.\n\n\n24\n6\nDouble normal with defined initial and final selectivity level - Recommended option.\n\n\n25\n3\nExponential-logistic.\n\n\n27\n3 + 2*N nodes\nCubic spline with N nodes.\n\n\n42\n5 + 2*x\nSelectivity pattern 27 but with 2 additional scaling parameters.\n\n\n43\n4 + x\nSelectivity pattern 6 but with 2 additional scaling parameters.\n\n\n\n\n\n\nAGE BASED SELECTIVITY\n\n\n\n\nPattern\nN Parameters\nDescription\n\n\n0\n0\nSelectivity = 1.0 for ages 0+.\n\n\n10\n0\nSelectivity = 1.0 for all ages beginning at age 1. If it is desired that age-0 fish be selected, then use pattern #11 and set minimum age to 0.0.\n\n\n11\n2\nSelectivity = 1.0 for a specified age range.\n\n\n12\n2\nLogistic selectivity.\n\n\n13\n8\nDouble logistic, IF joiners. Use discouraged. Use pattern 18 instead.\n\n\n14\nnages + 1\nSeparate parameter for each age (empirical), value at age is \\(\\frac{1}{1+exp(-x)}\\).\n\n\n15\n0\nMirror another age-specific selectivity pattern.\n\n\n16\n2\nColeraine single Gaussian.\n\n\n17\nnages + 1 or special + 1\nEmpirical as a random walk from previous age.\n\n\n18\n8\nDouble logistic, with defined peak, uses smooth joiners.\n\n\n19\n6\nSimple double logistic with no defined peak.\n\n\n20\n6\nDouble normal with defined initial and final level. Recommended option.\n\n\n26\n3\nExponential logistic.\n\n\n27\n3 + 2*N nodes\nCubic spline in age based on N nodes.\n\n\n41\n2 + nages + 1\nSelectivity pattern 17 but with 2 additional scaling parameters.\n\n\n42\n5 + 2*N nodes\nSelectivity pattern 27 but with 2 additional scaling parameters.\n\n\n44\n4 + nages\nSimilar to age selectivity pattern 17 but with separate parameters for males and with revised controls.\n\n\n45\n4 + nages\nSimilar to age selectivity pattern 14 but with separate parameters for males and with revised controls.\n\n\n\n\n8.9.2.1 Special Selectivity Options\n\nSpecial selectivity options (type 30 in size based selectivity) are no longer specified within the control file. Specifying the use of one of these selectivity types is now done within the data file by selecting the survey \"units\" (see the section on Index units).\n\n\n\n8.9.3 Selectivity Pattern Details\n\n8.9.3.1 Pattern 1 (size) and 12 (age) - Simple Logistic\n\nLogistic selectivity for the primary sex (if selectivity varies by sex) is formulated as: \\[S_l = \\frac{1.0}{1+exp(-ln(19)(L_l - p1)/p2)}\\] where \\(L_l\\) is the length bin. If age based selectivity is selected then the length bin is replaced by the age vector. If sex specific selectivity is specified the non-primary sex the p1 and p2 parameters are estimated as offsets. Note that with a large p2 parameter, selectivity may not reach 1.0 at the largest size bin. The parameters are:\n\np1 - size/age at inflection; and\np2 - width for 95% selection; a negative width causes a descending curve.\n\n\n\n8.9.3.2 Pattern 2 (size) - Older version of selectivity pattern 24 for backward compatibility\n\nDiffers from pattern 24 only in the treatment of sex-specific offset parameter 5. See note in Section  for more information. Pattern 24 was changed in version 3.30.19 with the old parameterization now provided in Pattern 2.\n\n\n8.9.3.3 Pattern 5 (size) - Mirror Selectivity\n\nTwo parameters select the min and max bin number (not min max size) of the source selectivity pattern. If first parameter has value &lt;=0, then interpreted as a value of 1 (e.g., first bin). If second parameter has value &lt;=0, then interpreted as maximum length bin (e.g., last bin specified in the data file). The mirrored selectivity pattern must have be from a lower fleet number (e.g., already specified before the mirrored fleet).\n\n\n8.9.3.4 Pattern 6 (size) - Non-parametric Selectivity\n\nNon-parametric size selectivity uses a set of linear segments. The first way point is at Length = p1 and the last way point is at Length = p2. The total number of way points is specified by the value of the Special factor in the selectivity set-up, so the N intervals is one less than the number of way points. Intermediate way points are located at equidistant intervals between p1 and p2. Parameters 3 to N are the selectivity values at the way points, entered as logistic, e.g., \\(1/(1+exp(-x))\\). Ramps from 10 cm to p3 if L &lt; p1. Constant at Np if L &gt; p2. Note that prior to version 3.03 the way points were specified in terms of bin number, rather than length.\n\n\n8.9.3.5 Pattern 8 (size) and 18 (age) - Double Logistic\n\nUsers are discouraged from using the double logistic selectivity. The double normal selectivity pattern (size pattern 24, age pattern 20) provides similar functionality but with only 6 parameters.\n\np1 - peak: size (age) for peak. Should be an integer and should be at bin boundary and not estimated. But options 7 and 18 may allow estimation.\np2 - initial: selectivity at length bin=1 (minimum length) or age=0.\np3 - inflection: a logit transform \\((1/(1+exp(-x))\\) is used so that the transformed value will be between 0 and 1. So a p1 value of -1.1 will be transformed to 0.25 and used to set the selectivity equal to 0.5 at a size (age) equal to 0.25 of the way between minimum length and peak.\np4 - slope1: ln(slope) of left side (ascending) selectivity.\np5 - final: logit transform for selectivity at maximum length (or maximum age).\np6 - inflection2: logit transform for size (age) at right side selectivity equal to half way between peak + peak width and maximum length (or maximum age).\np7 - slop2: ln(slope) of right side (descending) selectivity.\np8 - peak width: width of flattop.\n\n\n\n8.9.3.6 Pattern 11 (size or age) - Selectivity = 1.0 for range\n\nLength- or age-selectivity can be set equal to 1.0 for a range of lengths or ages. If the selectivity is length-based the input parameters should match the population length bin number that will have selectivity = 1.0. A simple example how this works is as follows:\n\n\n\nPopulation Length Bin #\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nPopulation Length (cm)\n10\n12\n14\n16\n18\n20\n22\n24\n\n\n\n\np1 The first length-bin to set selectivity equal to 1.0. Using the above bin structure if p1 = 3 then selectivity = 1.0 mid-bin length of 15 cm.\np2 The final length-bin to set selectivity equal to 1.0. Using the above bin structure if p2 = 7 then selectivity = 1.0 mid-bin length of 23 cm.\nAll length bins before and after p1 and p2 will be set near zero (1e-010).\n\nThe age-selectivity approach follows that detailed above for length-selectivity but is more intuitive since parameter p1 and p2 is set in terms of population age.\n\n\n8.9.3.7 Pattern 14 (age) - Revise Age\n\nAge-selectivity pattern 14 to allow selectivity-at-age to be the same as selectivity at the next younger age. When using this option, the range on each parameter should be approximately -5 to 9 to prevent the parameters from drifting into extreme values with nil gradient. The age-based selectivity is calculated as \\(a = 1\\) to \\(a = Amax + 1\\):\n\\[S_a = \\frac{1}{1+exp(-(p_{a+1} + (9 - max(p_a))))}\\]\n\n\n8.9.3.8 Pattern 17 (age) - Random Walk\n\nThis selectivity pattern provides for a random walk in ln(selectivity). For each age \\(a \\geq A_{\\text{min}}\\), where \\(A_{\\text{min}}\\) is the minimum age for which selectivity is allowed to be non-zero, there is a selectivity parameter, \\(p_a\\), controlling the changing selectivity from age \\(a-1\\) to age \\(a\\).\nThe selectivity at age \\(a\\) is computed as:\n\\[S_a = \\exp (S'_a - S'_{\\text{max}}),\\] where\n\\[S'_a = \\sum_{i = a_{\\text{min}}}^A p_i\\] and\n\\[S'_{\\text{max}} = \\mbox{max} \\{S'_a\\}.\\]\nSelectivity is fixed at \\(S_a = 0\\) for \\(a &lt; A_{\\text{min}}\\).\nThis formulation has the properties that the maximum selectivity equals 1, positive values of \\(p_a\\) are associated with increasing selectivity between ages \\(a-1\\) and \\(a\\), and negative values are associated with decreasing selectivity between those ages and \\(p_a = 0\\) gives constant selectivity.\nThe condition that maximum selectivity equals 1 results in one fewer degree of freedom than the number of estimated \\(p_a\\). Therefore, at least one parameter should be fixed at an arbitrary value, typically \\(p_{A_{\\text{min}}}=0\\).\nThe number of parameters lines required to the control file for pattern 17 is nages + 1, unless a greater than zero value is included in the special column. If special is greater than 0, then special + 1 is the number of parameter lines needed in the control file. The value of special should be less than or equal to nages.Input to the special column is used to reduce the number of parameters lines needed (selectivity is constant above the age represented by the last parameter value read when using special).\nIn typical usage:\n\nFirst parameter (for age 0) could have a value of -1000 so that the age 0 fish would get a selectivity of 0.0;\nSecond parameter (for age 1) could have a value of 0.0 and not be estimated, so age 1 is the reference age against which subsequent changes occur.\nNext parameters get estimated values. To assure that selectivity increases for the younger ages, the parameter min for these parameters could be set to 0.0 or a slightly negative value.\nIf dome-shaped selectivity is expected, then the parameters for older ages could have a range with the max set to 0.0 so they cannot increase further.\nTo keep selectivity at a particular age the same as selectivity at the next younger age, set its parameter value to 0.0 and not estimated. This allows for all older ages to have the same selectivity.\nTo keep a constant rate of change in selectivity across a range of ages, use the -999 flag to keep the same rate of change in ln(selectivity) as for the previous age.\n\nCode for implementing random walk selectivity can be found in SS_selex.tpl, search for \"SS_Label_Info_22.7.17\".\n\n\n8.9.3.9 Pattern 22 (size) - Double Normal with Plateau\n\n\n\np1 - peak1: beginning size for the plateau (in cm).\np2 - peak2: ending size for the plateau. Calculated as a fraction of the distance between peak1 and 99% of the lower edge of the last size bin in the model. Transformed as (1/(1+exp(-p2)). So a value of 0 results in peak2 being halfway between peak1 and 99% of the last bin.\np3 - upslope: ln(variance) on ascending side.\np4 - downslope: ln(variance) on descending side.\n\n\n\n8.9.3.10 Pattern 23 (size), 24 (size), and 20 (age) - Double Normal Selectivity\n\n\n\np1 - peak: beginning size (or age) for the plateau (in cm or year).\np2 - top: width of plateau, as logistic between peak and maximum length (or age).\np3 - ascending width: parameter value is ln(width).\np4 - descending width: parameter value is ln(width).\np5 - initial: selectivity at first bin, as logistic between 0 and 1.\np6 - final: selectivity at last bin, as logistic between 0 and 1 (for pattern 24). For pattern 23 age selectivity at last bin, as absolute value, so can be &gt; 1.0. Warning: do not allow this value to go above 1.0 if the F_method uses Pope’s approximation. It can above 1.0 when F is in exponential form. When this parameter is above 1.0, the overall selectivity pattern will have an intermediate plateau at 1.0 (according to peak and top), then will ascend further to the final value.\n\nNotes for Double Normal Selectivity:\n\nFor the initial selectivity parameter (parameter 5)\n\n-999 or -1000: ignore the initial selectivity algorithm and simply decay the small fish selectivity according to p3,\n&lt; -1000: ignore the initial selectivity algorithm as above and then set selectivity equal to 1.0e-06 for size bins 1 through bin = -1001 -value. So a value of -1003 would set selectivity to a nil level for bins 1 through 2 and begin using the modeled selectivity in bin 3.\n\nFor the final selectivity parameter (parameter 6)\n\n-999 or -1000: ignore the final selectivity algorithm and simply decay the large fish selectivity according to parameter 4,\n&lt;-1000: set selectivity constant for bins greater than bin number = -1000 - value.\n\n\n\n\n8.9.3.11 Pattern 15 (size or age) - Mirror\n\nNo parameters. Whole age range is mirrored from another fleet. The mirrored selectivity pattern must have be from a lower fleet number (e.g., already specified before the mirrored fleet).\n\n\n8.9.3.12 Pattern 16 (age) - Gaussian (similar to Coleraine)\n\n\n\np1 - age below which selectivity declines\np2 - scaling factor for decline\n\n\n\n8.9.3.13 Pattern 9 (size) and 19 (age) - Simple Double Logistic\n\nThis pattern has 4 parameters and 2 fixed input values.\nThe shape of the selectivity is provided by the function (here in terms of age \\(a\\) but similarly applicable to length bin \\(l\\))\n\\[S'_a = \\begin{cases} \\hfil 0 & \\text{if $a &lt; p_5$,} \\\\ \\left( \\frac{1}{\\exp\\left(-p_2 \\left( a - p_1 \\right) \\right) } \\right) \\left(1 - \\frac{1}{\\exp\\left(-p_4 \\left( a - [p_6 p_1 - p_3]\\right) \\right) } \\right) & \\text{if $a \\geq p_5$.} \\end{cases}\\]\nwhich is then rescaled by first adding a small constant to all values and then rescaling to have a maximum of 1.0:\n\\[S_a = (S'_a + 0.000001) / \\max_{a'}\\{S'_a + 0.000001\\}\\]\nwhere\n\np1 - ascending inflection age/size (in cm).\np2 - ascending slope.\np3 - descending inflection age/size (in cm).\np4 - descending slope.\np5 - age or size at first selection; this is a specification parameter, so must not be estimated. Enter integer that is age for pattern 19 and is bin number for pattern 9.\np6 - (0/1) where a value of 0 causes the descending inflection to be a standalone parameter, and a value of 1 causes the descending inflection to be interpreted as an offset from the ascending inflection. This is a specification parameter, so must not be estimated.\n\n\n\n8.9.3.14 Pattern 25 (size) and 26 (age) - Exponential logistic\n\nThe exponential logistic included is based on the exponential logistic selectivity detailed by Thompson (1994); however, the parameterization within SS3 differs. Explorations using this selectivity form has shown that the estimation of p1 can be highly unstable. Users are strongly encourage to use the double normal selectivity rather than the exponential logistic selectivity.\n\np1 - ascending rate, min: 0.02, max: 1.0, reasonable start value: 0.1.\np2 - peak, as fraction of way between min size and max size. Parameter min value: 0.01; max: 0.99; reasonable start value: 0.5.\np3 - descending rate, min: 0.001, max: 0.5, reasonable start value: 0.01. A value of 0.001 provides a nearly asymptotic curve. Values above 0.2 provide strongly dome-shaped function in which the p3 and p1 parameters interact strongly.\n\nThe exponential logistic selectivity is calculated as: \\[peak = \\text{min}(L_l) + p2(\\text{max}(L_l) - \\text{min}(L_l) )\\] where \\(L_l\\) is the length bins at bin \\(l\\) (if age-based substitute with age bins) and: \\[S_l = \\frac{e^{p3*p1(peak-L_l)}}{1-p3(1-e^{p1(peak- L_l)})}\\]\n\n\n\n\n\n8.9.3.15 Pattern 27 (size or age)- Cubic Spline\n\nThis selectivity pattern uses the ADMB implementation of the cubic spline function. This function requires input of the number of nodes, the positions of those nodes, the parameter values at those nodes, and the slope of the function at the first and last node.\nAn alternative to specifying or estimating the slope at the first and last nodes is to fix those values at 1e30 which will cause create a \"natural cubic spline\" which allows the slope (first derivative) to be flexible but sets the curvature (second derivative) to be 0 at the first and last nodes.\nThe number of nodes is specified in the \"special\" column of the selectivity set-up. The pattern number 27 is used to invoke cubic spline for size selectivity and for age selectivity; the input syntax is identical.\nFor a 3 node setup, the input parameters would be:\n\np1 - Code for initial set-up which controls whether or not auto-generation is applied (input options are 0, 1, 2, 10, 11, or 12) as explained below\np2 - Gradient at the first node (should be a small positive value, or fixed at 1e30 to implement a \"natural cubic spline\")\np3 - Gradient at the last node (should be zero, a small negative value, or fixed at 1e30 to implement a \"natural cubic spline\")\np4-p6 - The nodes in units of cm; must be in rank order and inside of the range of the population length bins. These must be held constant (not estimated, e.g., negative phase value) during a model run.\np7-p9 - The values at the nodes. Units are ln(selectivity) before rescaling.\n\nNotes:\n\nThere must be at least 3 nodes.\nOne of the node selectivity parameter values should be held constant so others are estimated relative to it.\nSelectivity is forced to be constant for sizes greater than the size at the last node.\nThe overall selectivity curve is scaled to have a peak equal to 1.0.\nLast node cannot be at the max population length bin.\n\nCode for implementing cubic spline selectivity can be found in SS_selex.tpl, search for \"SS_Label_Info_22.7.27\".\nOne potential problem that may occur with a cubic spline is a U-shaped pattern in the selectivity around the first node. If this occurs, the initial set-up code (auto-generation options described below) can be changed from 0, 1 or 2 to 10, 11, or 12 which will cause selectivity to be fixed at 0.0 for all bins below the first node. A natural cubic spline (noted above) may be an alternative solution to this problem.\nAuto-Generation of Cubic Spline Control File Set-Up: A new feature pioneered with the cubic spline function is a capability to produce more specific parameter labels and to auto-generate selectivity parameter setup. The auto-generation feature is controlled by the first selectivity parameter value for each fleet that is specified to use the cubic spline. There are 6 possible values for this setup parameter:\n\n0: no auto-generation, process parameter setup as read.\n1: auto-generate the node locations based on the specified number of nodes and on the cumulative size distribution of the data for this fleet/survey.\n2: auto-generate the nodes and also the min, max, prior, initial, and phase for each parameter.\n10: no auto-generation as in 0 above and fix selectivity = 0.0 below the first knot\n11: auto-generate the node locations as in 1 above and also fix selectivity = 0.0 below the first knot\n12: auto-generate the nodes and other inputs as in 2 above and also fix selectivity = 0.0 below the first knot\n\nWith either the auto-generate option 1, 2 11, or 12, it still is necessary to include in the parameter file placeholder rows of values so that the init_matrix command can input the current number of values because all selectivity parameter lines are read as a single matrix with the dimension of N parameters x 14 columns. The read values of min, max, initial, prior, prior type, prior standard deviation, and phase will be overwritten.\nCumulative size and age distribution is calculated for each fleet, summing across all samples and both sexes. These distributions are output in echoinput.sso and in a new OVERALL_COMPS section of report.sso.\nWhen the nodes are auto-generated, the first node is placed at the size corresponding to the 2.5% percentile of the cumulative size distribution, the last is placed at the 97.5% percentile of the size distribution, and the remainder are placed at equally spaced percentiles along the cumulative size distribution. These calculated node values are output into control.ss_new. So, the user could extract these nodes from control.ss_new, edit them to desired values, then, insert them into the input control file. Remember to turn off auto-generation in the revised control file.\nWhen the complete auto-generation is selected, the control.ss_new would look like the table below:\n\n\n\nLO\nHI\nINIT\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n\n\n0\n2\n2.0\n...\n0\n#SizeSpline Code\n\n\n-0.001\n1\n0.13\n...\n0\n#SizeSpline GradLo\n\n\n-1\n0.001\n-0.03\n...\n0\n#SizeSpline GradHi\n\n\n11\n95\n38\n...\n0\n#SizeSpline Knot1\n\n\n11\n95\n59\n...\n0\n#SizeSpline Knot2\n\n\n11\n95\n74\n...\n0\n#SizeSpline Knot3\n\n\n-9\n7\n-3\n...\n0\n#SizeSpline Value1\n\n\n-9\n7\n-1\n...\n0\n#SizeSpline Value2\n\n\n-9\n7\n-0.78\n...\n0\n#SizeSpline Value3\n\n\n\n\n\n8.9.3.16 Pattern 41 (age) - Random Walk with User-Defined Scaling\n\nSelectivity pattern 17 with two additional parameters. The two additional parameters are the bin numbers to define the range of bins for scaling. All of the selectivity values will be scaled (divided) by the mean value over this range. The low and high bin numbers are defined before the other selectivity parameters.\n\n\n\nLO\nHI\nINIT\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n\n\n0\n20\n10\n...\n0\n#AgeSel_ScaleAgeLo\n\n\n0\n20\n20\n...\n0\n#AgeSel_ScaleAgeHi\n\n\n\n\n\n8.9.3.17 Pattern 42 (size or age) - Cubic Spline with User-Defined Scaling\n\nSelectivity pattern 27 with two additional parameters. The two additional parameters are the bin numbers to define the range of bins for scaling. All of the selectivity values will be scaled (divided) by the mean value over this range. The low and high bin numbers are defined before the other selectivity parameters.\n\n\n\nLO\nHI\nINIT\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n\n\n0\n20\n10\n...\n0\n#AgeSpline_ScaleAgeLo\n\n\n0\n20\n20\n...\n0\n#AgeSpline_ScaleAgeHi\n\n\n\n\n\n8.9.3.18 Pattern 43 (size) - Non-parametric with User-Defined Scaling\n\nSelectivity pattern 6 with two additional parameters. The two additional parameters are the bin numbers to define the range of bins for scaling. All of the selectivity values will be scaled (divided) by the mean value over this range. The low and high bin numbers are defined before the other selectivity parameters.\n\n\n\nLO\nHI\nINIT\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n\n\n1\n80\n50\n...\n0\n#SizeSel_ScaleBinLo\n\n\n1\n80\n70\n...\n0\n#SizeSel_ScaleBinHi\n\n\n\n\n\n8.9.3.19 Pattern 44 (age)\n\nSimilar to pattern 17 but with separate parameters for males and females. This selectivity pattern provides for a random walk in ln(selectivity). In typical usage:\n\np1 - First parameter (for age 0) could have a value of -1000 so that the age 0 fish would get a selectivity of 0.0.\np2 - The first age for which mean selectivity = 1.\np3 - The last age for which mean selectivity = 1.\np4 - Male mean selectivity relative to the female selectivity mean entered as ln(ratio) for the male relative female selectivity.\np5-pn - Additional parameter lines for the natural log of the selectivity change between ages corresponding to the user specified number of changes in the \"special\" column for the selectivity specification for each sex with females entered first then males.\n-999 input indicates to the model to keep the change unchanged from the previous age (keeps same rate of change).\n-1000 input indicates used only for male selectivity indicates to the model to set the change in male selectivity equal to the female change in selectivity.\n\nAn example specification and setup for this selectivity option where selectivity is dome-shaped, peaking at age 2 with female and male selectivity are equal with 4 change points per sex:\n\n\n\n\n#Pattern\nDiscard\nMale\nSpecial\n\n\n\n\n44\n0\n0\n4\n\n\n\n\n\n\n\nLO\nHI\nINIT\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n\n\n0\n20\n0\n...\n0\n#first selex age\n\n\n0\n20\n2\n...\n0\n#first age peak selex (mean)\n\n\n0\n20\n2\n...\n0\n#last age peak selex (mean)\n\n\n-1\n2\n-0.001\n...\n0\n#male ln(ratio)\n\n\n-10\n10\n3.01\n...\n0\n#female ln(selex) change 1\n\n\n-10\n10\n1.56\n...\n0\n#female ln(selex) change 2\n\n\n-10\n10\n-0.15\n...\n0\n#female ln(selex) change 3\n\n\n-10\n10\n-0.15\n...\n0\n#female ln(selex) change 4\n\n\n-1000\n10\n-1000\n...\n0\n#male ln(selex) change 1\n\n\n-1000\n10\n-1000\n...\n0\n#male ln(selex) change 2\n\n\n-1000\n10\n-1000\n...\n0\n#male ln(selex) change 3\n\n\n-1000\n10\n-1000\n...\n0\n#male ln(selex) change 4\n\n\n\n\n\n8.9.3.20 Pattern 45 (age) - Revise Age\n\nSimilar to pattern 14 but with separate parameters for males and females. Age-selectivity pattern 45 to allow selectivity-at-age to be the same as selectivity at the next younger age.\n\np1 - is the first age with non-zero selectivity.\np2 - The first age in mean for peak selectivity\np3 - The last age in mean for peak selectivity\np4 - The male mean selectivity relative to the female mean, entered as ln(ratio) for the male relative female selectivity\n-999 input indicates to the model to keep the change unchanged from the previous age (keeps same rate of change).\n-1000 input indicates used only for male selectivity indicates to the model to set the change in male selectivity equal to the female change in selectivity.\n\nAn example specification and setup for this selectivity option where selectivity is asymptotic, with female and male selectivity are equal with 4 change points per sex:\n\n\n\n\n#Pattern\nDiscard\nMale\nSpecial\n\n\n\n\n45\n0\n0\n3\n\n\n\n\n\n\n\nLO\nHI\nINIT\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n\n\n0\n20\n2\n...\n0\n#first selex age\n\n\n0\n20\n5\n...\n0\n#first age peak selex (mean)\n\n\n0\n20\n5\n...\n0\n#last age peak selex (mean)\n\n\n-1\n2\n-0.001\n...\n0\n#male ln(ratio)\n\n\n-10\n10\n-8.1\n...\n0\n#female ln(selex) change 1\n\n\n-10\n10\n-4.1\n...\n0\n#female ln(selex) change 2\n\n\n-10\n10\n-1.8\n...\n0\n#female ln(selex) change 3\n\n\n-1000\n10\n-1000\n...\n0\n#male ln(selex) change 1\n\n\n-1000\n10\n-1000\n...\n0\n#male ln(selex) change 2\n\n\n-1000\n10\n-1000\n...\n0\n#male ln(selex) change 3\n\n\n\n\n\n\n8.9.4 Retention\nRetention is defined as a logistic function of size or age. It does not apply to surveys. Four parameters (for asymptotic retention) or seven parameters (for dome-shaped retention) are used:\n\nAsymptotic (4 parameters):\n\np1 - ascending inflection,\np2 - ascending slope,\np3 - maximum retention controlling the height of the asymptote (smaller values result in lower asymptotes), often a time-varying quantity to match the observed amount of discard. As of v. 3.30.01, this parameter is now input in logit space ranging between -10 and 10. A fixed value of -999 would assume no retention of fish and a value of 999 would set asymptotic retention equal to 1.0,\np4 - male offset to ascending inflection (arithmetic, not multiplicative),\n\nDome-shaped (add the following 3 parameters):\n\np5 - descending inflection,\np6 - descending slope, and\np7 - male offset to descending inflection (arithmetic, not multiplicative).\n\n\n\\[\\text{Retention}_l = \\left(\\frac{P3'}{1 + e^{\\frac{-(L_l-(P1+P4*male))}{P2}}}\\right)*\\left(1 - \\frac{1}{1 + e^{\\frac{-(L_l-(P5+P7*male))}{P6}}}\\right)\\] where \\(P3' = 1/(1+e^{-P3})\\) is the asymptotic retention calculated from the \\(P3\\) parameter which is in logit space.\n\n\n8.9.5 Discard Mortality\nDiscard mortality is defined as a logistic function of size such that mortality declines from 1.0 to an asymptotic level as fish get larger. It does not apply to surveys and it does not affect the calculation of expected values for discard data. It is applied so that the total mortality rate is:\n\ndead fish = selectivity * (retain + (1.0-retain)*discard mortality)\n\nIf discard mortality is 1.0, all selected fish are dead; if discard mortality is 0.0, only the retained fish are dead.\nFour parameters are used:\n\np1 - descending inflection\np2 - descending slope\np3 - maximum discard mortality\np4 - male offset to descending inflection (arithmetic, not multiplicative)\n\nDiscard mortality is calculated as: \\[\\text{Discard Mortality}_l = 1 - \\frac{1-P3}{1+e^{\\frac{-(L_l-(P1+P4*male))}{P2}}}\\]\n\n\n8.9.6 Sex-Specific Selectivity\nThere are two approaches to specifying sex specific selectivity. One approach allows male selectivity to be specified as a fraction of female selectivity (or vice versa). This first approach can be used for any selectivity pattern. The other option allows for separate selectivity parameters for each sex plus an additional parameter to define the scaling of one sex’s peak selectivity relative to the other sex’s peak. This second approach has only been implemented for a few selectivity patterns.\n\n8.9.6.1 Male Selectivity as a Fraction of Female Selectivity (or vice versa):\n\nIf the \"domale\" flag is set to 1, then the selectivity parameters define female selectivity and the offset defined below sets male selectivity relative to female selectivity. The two sexes switch roles if the \"domale\" flag is set to 2. Generally it is best to select the option so that the dependent sex has lower selectivity, thus obviating the need to rescale for selectivities that are greater than 1.0. Sex specific selectivity is done the same way for all size and age selectivity options.\n\np1 - size (age) at which a dogleg occurs (set to an integer at a bin boundary and do not estimate).\np2 - ln(relative selectivity) at minimum length or age = 0. Typically this will be set to a value of 0.0 (for no offset) and not estimated. It would be a rare circumstance in which the youngest/smallest fish had sex-specific selectivity.\np3 - ln(relative selectivity) at the dogleg.\np4 - ln(relative selectivity) at maximum length or max age.\n\nFor intermediate ages, the natural log values are linearly interpolated on size (age).\nIf selectivity for the dependent sex is greater than the selectivity for the first sex (which always peaks at 1.0), then the male-female selectivity matrix is rescaled to have a maximum of 1.0.\n\n\n\n\n\n8.9.6.2 Male Selectivity Estimated as Offsets from Female Selectivity (or vice versa):\n\nA new sex selectivity option (3 or 4) has been implemented for size selectivity patterns 1 (logistic) and 23 and 24 (double normal) or age selectivity pattern 20 (double normal age). Rather than calculate male selectivity as an offset from female selectivity, here the male selectivity is calculated by making the male parameters an offset from the female parameters (option 3), or females are offset from males with option 4. The description below applies to option 3. If the size selectivity pattern is 1 (logistic), then read 3 parameters:\n\nmale parameter 1 is added to the first selectivity parameter (inflection)\nmale parameter 2 is added to the second selectivity parameter (width of curve)\nmale parameter 3 is the asymptotic selectivity\n\nIf the size selectivity pattern is 2, 20, 23 or 24 (double normal), then:\n\nmale parameter 1 is added to the first selectivity parameter (peak)\nmale parameter 2 is added to the third selectivity parameter (width of ascending side); then exp(this sum) per previous transform\nmale parameter 3 is added to the fourth selectivity parameter (width of descending side); then exp(sum) per previous transform\nmale parameter 4 is added to the sixth selectivity parameter (selectivity at final size bin); then 1/(1+exp(-sum)) per previous transform\nmale parameter 5 in selectivity pattern 24 is the apical selectivity and the descending limb for males (see note below). For selectivity pattern 2, 20, and 23 it applies only to the apical selectivity.\n\nNotes:\n\nMale selectivity offsets currently cannot be time-varying because they are offsets from female selectivity, they inherit the time-varying characteristics of the female selectivity.\nPrior to version 3.30.19 male parameter 5 in pattern 24 scaled only the apical selectivity. This sometimes resulted in strange shapes when the final selectivity, which was shared between females and males in that parameterization, was higher than the estimated apical selectivity. For backwards compatibility to the pattern 24 parameterization prior to 3.30.19, use selectivity pattern 2.\n\n\n\n\n\n\n\n8.9.7 Dirichlet-Multinomial Error for Data Weighting\nIf the Dirichlet-multinomial error distribution was selected in the data file for length or age data weighting, add additional parameter line(s) immediately following the age selectivity parameter block. There should be 1 parameter line for each parameter in the data file.\nFor additional information about the Dirichlet-multinomial please see Thorson et al. (2017) and the detailed Data Weighting section.\n\n\n\nThe list of parameters would be something like:\n\n\n\n\nLO\nHI\nINIT\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n-5\n10\n0.5\n...\n0\n#ln(DM theta) Age or Length 1\n\n\n-5\n10\n0.5\n...\n0\n#ln(DM theta) Age or Length 2\n\n\n\n\n\n8.9.8 Selectivity Time-Varying Parameters\nTime-Varying selectivity can be used. Details on how to specify time-varying parameters can be found in the Time-Varying Parameter Specification and Setup section.\n\n\n8.9.9 Two-Dimensional Auto-Regressive Selectivity (Semi-parametric selectivity)\nAn experimental feature added version 3.30.03.02. Earlier versions do not have this feature and hence this input is not expected. This features allows for a matrix of auto-correlation selectivity deviations by age or length and time as described in Xu et al. (2019). Unlike other options for time-varying selectivity in SS3, these deviations are not in the selectivity parameters themselves, but exponential multipliers on whatever baseline selectivity has been chosen.\nWhen using this option for age-based selectivity, if there are not too many ages, a good choice for the baseline selectivity might be random walk selectivity (pattern 17) because it provides the most flexibility, allowing the deviations to be used only for the annual deviations around this baseline rather than the account for misspecification of the underlying functional form. Otherwise, a simple parametric selectivity form like logistic, exponential logistic, or double normal would be a better choices. This option has not yet been explored adequately to provide guidance on best practices.\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n1\nTwo-dimensional auto-regressive selectivity:\n\n\n\n0 = Not used,\n\n\n\n1 = Use 2D AR.\n\n\nCOND = 1 Read the following long parameter lines.\n\n\nIf multiple fleets are specified for 2D AR, the following lines are repeated for each fleet:\n\n\n\n\n\n\n\nSigma\nUse\nLen(1)/\n\nBefore\nAfter\n\n\nFleet\nYmin\nYmax\nAmin\nAmax\nAmax\nRho\nAge(2)\nPhase\nRange\nRange\n\n\n1\n1979\n2015\n2\n10\n1\n1\n2\n5\n0\n0\n\n\n\n\n\n\nPRIOR\nPRIOR\n\n\n\n\n\n\n\nLO\nHI\nINIT\nPRIOR\nSD\nTYPE\nPHASE\nLABEL\n\n\n0\n4\n1\n1\n0.1\n6\n-4\n#Sigma selex fleet 1, first age\n\n\n0\n4\n1\n1\n0.1\n6\n-4\n#Sigma selex fleet 1, second age\n\n\n0\n4\n1\n1\n0.1\n6\n-4\n#Sigma selex fleet 1, ... age\n\n\n-1\n1\n0\n0\n0.1\n6\n-4\n#Rho year fleet 1\n\n\n-1\n1\n0\n0\n0.1\n6\n-4\n#Rho age fleet 1\n\n\n# Additional fleets (e.g., fleet 2) with 2D AR selectivity\n\n\n\n\n\n\n\nSigma\nUse\nLen(1)/\n\nBefore\nAfter\n\n\nFleet\nYmin\nYmax\nAmin\nAmax\nAmax\nRho\nAge(2)\nPhase\nRange\nRange\n\n\n2\n1979\n2015\n2\n10\n1\n1\n2\n5\n0\n0\n\n\n\n\n\n\nPRIOR\nPRIOR\n\n\n\n\n\n\n\nLO\nHI\nINIT\nPRIOR\nSD\nTYPE\nPHASE\nLABEL\n\n\n0\n4\n1\n1\n0.1\n6\n-4\n#Sigma selex fleet 1, first age\n\n\n0\n4\n1\n1\n0.1\n6\n-4\n#Sigma selex fleet 1, second age\n\n\n0\n4\n1\n1\n0.1\n6\n-4\n#Sigma selex fleet 1, ... age\n\n\n-1\n1\n0\n0\n0.1\n6\n-4\n#Rho year fleet 1\n\n\n-1\n1\n0\n0\n0.1\n6\n-4\n#Rho age fleet 1\n\n\n# Terminator line of 11 in length indicates the end of parameter input lines\n\n\n-9999\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\nParameter Definitions:\n\nFleet: Fleet number to which semi-parametric deviations should be added.\nYmin: First year with deviations.\nYmax: Last year with deviations.\nAmin: First integer age (or population length bin index) with deviations.\nAmax: Last integer age (or population length bin index) with deviations.\nSigma Amax: the last age (or population length bin index) for which a separate sigma should be read. For simplicity, it is easiest to set Sigma Amax equal to the Amin value which requires only a single sigma line (otherwise N = Sigma Amax - Amin input lines are required for the sigma parameters). Also, a value &lt; 0 will allow for only one sigma parameter should be read and then used for all bins (this is option is available in versions 3.30.15 and higher).\nUse Rho: Use autocorrelation parameters.\nLen(1) / Age(2): 1 or 2 to specify whether the deviations should be applied to length- or age-based selectivity.\nPhase: Phase to begin estimation of the deviation parameters.\nBefore Range: How should selectivity be modeled in the years prior to Ymin? Available options are (0) apply no deviations, (1) use deviations from the first year with deviations (Ymin), and (3) use average across all years with deviations (Ymin to Ymax).\nAfter Range: Similar to Before Range but defines how selectivity should be modeled after Ymax."
  },
  {
    "objectID": "qmds/html.html#tag-recapture-parameters",
    "href": "qmds/html.html#tag-recapture-parameters",
    "title": "RAW HTML CONTENT",
    "section": "8.10 Tag Recapture Parameters",
    "text": "8.10 Tag Recapture Parameters\nSpecify if tagging data are being used:\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n1\nTagging Data Present:\n\n\n\n0 = No tagging data, or if tagging data is present in the data file, a value of 0\n\n\n\nhere will auto-generate the tag parameter section in the control.ss_new file.\n\n\n\n1 = Read following lines of tagging data.\n\n\nCOND = 1 Read the following long parameter lines:\n\n\nLO\nHI\nINIT\nPRIOR\n&lt;other entries&gt;\nBlock Fxn\nParameter Label\n\n\n-10\n10\n9\n9\n...\n0\n#TG loss init 1\n\n\n-10\n10\n9\n9\n...\n0\n#TG loss chronic 1\n\n\n1\n10\n2\n2\n...\n0\n#TG loss overdispersion 1\n\n\n-10\n10\n9\n9\n...\n0\n#TG report fleet 1\n\n\n-4\n0\n0\n0\n...\n0\n#TG report decay 1\n\n\n\nIf there are multiple tagging groups or multiple fleets reporting tagged fish the additional needed parameter lines should be entered in order for each parameter type (i.e, TG loss init 1, TG loss init 2, TG loss chronic 1, TG loss chronic 2,..., TG report decay 1, TG report decay 2).\nFive parameter types are required for tagging data. Both the tag loss parameters and the reporting rate parameters are input as any real number and a logistic transformation is used to keep the resulting rates between 0 and 1:\n\nInitial tag loss, representing fraction of tags that are shed or associated with tag-induced mortality immediately after tagging (1 parameter per tag group).\nChronic tag loss, representing annual rate of tag loss (1 parameter per tag group).\nOverdispersion parameter for the negative binomial likelihood associated with the total number of tag recaptures per tag group (1 parameter per tag group). This value represents the ratio of the variance to the mean of the distribution. As parameterized in ADMB, the likelihood is only defined for overdispersion parameters &gt; 1, but a value of 1.001 will produce results essentially equal to those associated with a Poisson likelihood in which the variance is equal to the mean. This parameter is not transformed like the other 4 types.\nInitial reporting rate (1 parameter per fleet), the reporting rate of tags for a specific fleet.\nReporting rate decay representing annual reduction in reporting rate (1 parameter per fleet).\n\nThe tagging reporting rate parameter is transformed during estimation to maintain a positive value and is reported according to the transformation: \\[\\text{Tagging Reporting Rate} = \\frac{e^{\\text{input parameter}}}{1+e^{\\text{input parameter}}}\\]\nCurrently, tag parameters cannot be time-varying.\nA shortcoming was identified in the recapture calculations when using Pope’s F Method and multiple seasons in SS3 prior to v.3.30.14. The internal calculations were corrected in version 3.30.14. Now the Z-at-age is applied internally for calculations of fishing pressure on the population when using the Pope calculations.\n\n8.10.0.1 Mirroring of Tagging Parameters\n\nIn version 3.30.14, the ability to mirror the tagging parameters from another tag group or fleet was added. With this approach, the user can have just one parameter value for each of the five tagging parameter types and mirror all other parameters. Note that parameter lines are still required for the mirrored parameters and only lower numbered parameters can be mirrored. Mirroring is evoked through the phase input in the tagging parameter section. The options are:\n\nNo mirroring among tag groups or fleets: phase &gt; -1000,\nMirror the next lower (i.e., already specified) tag group or fleet: phase = -1000 and set other parameter values the same as next lower Tag Group or fleet,\nMirror a lower (i.e., already specified) tag group of fleet x: phase = -100x and set other parameter values the same as the mirrored tag group or fleet( i.e., if you would like to mirror fleet 1 then the phase should -1001).\n\nTo avoid having to specify mirrored parameter lines, the tag parameters can be auto-generated. The control.ss_new file created after running this model will have a full set of tagging parameter lines to use in future model runs."
  },
  {
    "objectID": "qmds/html.html#variance-adjustment-factors",
    "href": "qmds/html.html#variance-adjustment-factors",
    "title": "RAW HTML CONTENT",
    "section": "8.11 Variance Adjustment Factors",
    "text": "8.11 Variance Adjustment Factors\nWhen doing iterative re-weighting of the input variance factors, it is convenient to do this in the control file, rather than the data file. This section creates that capability.\n\n\n\nRead variance adjustment factors to be applied:\n\n\n\n\nFactor\nFleet\nValue\nDescription\n\n\n1\n2\n0.5\n# Survey CV for survey/fleet 2\n\n\n4\n1\n0.25\n# Length data for fleet 1\n\n\n4\n2\n0.75\n# Length data for fleet 2\n\n\n-9999\n0\n0\n# End read\n\n\n\n\n8.11.0.1 Additive Survey CV - Factor 1\n\nWhile this functionality has been retained for backward compatibility with older model versions, this approach is no longer considered the best practice for tuning indices. Tuning indices should be conducted using the ability to estimate additional standard deviation which can be done in the Catchability setup.\nThe survey input variance (labeled survey CV) is actually the standard deviation of the ln(survey). The variance adjustment is added directly to this standard deviation. Set to 0.0 for no effect. Negative values are OK, but will crash if adjusted standard deviation value becomes negative.\n\n\n8.11.0.2 Additive Discard - Factor 2\n\nThe input variance is the CV of the observation. Because this will cause observations of near zero discard to appear overly precise, the variance adjustment is added to the discard standard deviation, not to the CV. Set to 0.0 for no effect.\n\n\n8.11.0.3 Additive Mean Body Weight - Factor 3\n\nThe input variance is in terms of the CV of the observation. Because such data are typically not very noisy, the variance adjustment is added to the CV and then multiplied by the observation to get the adjusted standard deviation of the observation.\n\n\n8.11.0.4 Multiplicative Length Composition - Factor 4\n\nThe input variance is in terms of an effective sample size. The variance adjustment is multiplied times this sample size. Set variance adjustment to 1.0 for no effect.\n\n\n8.11.0.5 Multiplicative Age Composition - Factor 5\n\nAge composition is treated the same way as length composition.\n\n\n8.11.0.6 Multiplicative Size-at-Age - Factor 6\n\nSize-at-age input variance is the sample size for the N observations at each age. The variance adjustment is multiplied by these N values. Set to 1.0 for no effect.\n\n\n8.11.0.7 Multiplicative Generalized Size Composition - Factor 7\n\nGeneralized size composition input variance is the sample size for each observation. The variance adjustment for each fleet is multiplied by these sample sizes. Set to 1.0 for no effect.\n\n\n8.11.0.8 Variance Adjustment Usage Notes\n\nThe report.sso output file contains information in the \"FIT_LEN_COMPS\" and \"FIT_AGE_COMPS\" useful for determining if an adjustment of these input values is warranted to better match the scale of the average residual to the input variance scale.\nBecause the actual input variance factors are modified, it is these modified variance factors that are used when creating parametric bootstrap data files. So, the control files used to analyze bootstrap generated data files should have the variance adjustment factors reset to null levels."
  },
  {
    "objectID": "qmds/html.html#lambdas-emphasis-factors",
    "href": "qmds/html.html#lambdas-emphasis-factors",
    "title": "RAW HTML CONTENT",
    "section": "8.12 Lambdas (Emphasis Factors)",
    "text": "8.12 Lambdas (Emphasis Factors)\nThese values are multiplied by the corresponding likelihood component to calculate the overall negative log likelihood to be minimized.\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n4\nMax lambda phase: read this number of lambda values for each element below. The last lambda value is used for all higher numbered phases.\n\n\n1\nSD offset:\n\n\n\n0 = The ln(like) to omit the + ln(s) term,\n\n\n\n1 = The ln(like) to include the ln(s) term for CPUE, discard, growth CV, mean body weight, recruitment deviations. If you are estimating any variance parameters, SD offset must be set to 1.\n\n\n\n\n8.12.0.1 Lambda Usage Notes\n\nIf the CV for size-at-age is being estimated and the model contains mean size-at-age data, then the flag for inclusion of the + ln(stddev) term in the likelihood must be included. Otherwise, the model will always get a better fit to the mean size-at-age data by increasing the parameter for CV of size-at-age.\nThe reading of the lambda values has been substantially altered with SS3 v.3.30. Instead of reading a matrix containing all the needed lambda values, the model now just reads those elements that will be given a value other than 1.0. After reading the datafile, the model sets lambda equal to 0.0 if there are no data for a particular fleet/data type, and a value of 1.0 if data exist. So beware if your data files had data but you had set the lambda to 0.0 in a previous version of SS3. First read an integer for the number of changes.\n\n\n\nRead the lambda adjustments by fleet and data type:\n\n\n\n\nLikelihood\n\n\nLambda\nSizeFreq\n\n\nComponent\nFleet\nPhase\nValue\nMethod\n\n\n1\n2\n2\n1.5\n1\n\n\n4\n2\n2\n10\n1\n\n\n4\n2\n3\n0.2\n1\n\n\n-9999\n1\n1\n1\n1\n\n\n\n\n\n\n\nThe codes for component are:\n\n\n\n\n1 = survey\n10 = recruitment deviations\n\n\n2 = discard\n11 = parameter priors\n\n\n3 = mean weight\n12 = parameter deviations\n\n\n4 = length\n13 = crash penalty\n\n\n5 = age\n14 = morph composition\n\n\n6 = size frequency\n15 = tag composition\n\n\n7 = size-at-age\n16 = tag negative binomial\n\n\n8 = catch\n17 = F ballpark\n\n\n9 = initial equilibrium catch (see note below)\n18 = regime shift\n\n\n\n\nStarting in SS3 v.3.30.16, the application of a lambda to initial equilibrium catch is now fleet specific. In previous versions, a single lambda was applied in the same manner across all fleets with an initial equilibrium catch specified."
  },
  {
    "objectID": "qmds/html.html#controls-for-variance-of-derived-quantities",
    "href": "qmds/html.html#controls-for-variance-of-derived-quantities",
    "title": "RAW HTML CONTENT",
    "section": "8.13 Controls for Variance of Derived Quantities",
    "text": "8.13 Controls for Variance of Derived Quantities\nAdditional standard deviation reported may be selected:\n\n\n\nTypical Value\nDescription and Options\n\n\n\n\n0\n0 = No additional std dev reporting;\n\n\n\n1 = read specification for reporting stdev for selectivity, size, numbers; and\n\n\n\n2 = read specification for reporting stdev for selectivity, size, numbers,\n\n\n\nnatural mortality, dynamic B0, and Summary Bio\n\n\n\nCOND = 1 or 2: Read the following lines (split into 3 rows for readability):\n\n4 values related to selectivity:\n\nfleet number (or 0 if not used),\ntype of selectivity to report: 1 = length, 2 = age, or 3 = combined (age with length),\nyear (enter -1 for end year), and\nnumber of bins to report, list entered below (note - combined will report in age bins).\n\n2 values related to growth:\n\ngrowth pattern (or 0 if not used), and\nnumber of ages for reporting: list entered below (note - each sex will be reported).\n\n3 values related to numbers-at-age:\n\narea (enter -1 for sum of all areas), or 0 for not used,\nyear (enter -1 for end year + 1), and\nnumber of ages to report, list entered below (note - each sex will be reported and summed across growth patterns).\n\n\nCOND = 2, enter the above quantities plus (available in versions 3.30.15 and higher):\n\n2 additional values related to natural mortality-at-age:\n\ngrowth pattern (or 0 for not used), and\nnumber of ages for reporting, list entered below (note each sex will be reported).\n\n1 additional value related to dynamic B0 (needed in versions 3.30.17 and higher):\n\n0 for not used, 1 to report SSB for dynamic B0, 2 to report SSB and recruitment\n\n1 additional value related to Summary Bio (needed in versions 3.30.17 and higher):\n\n0 for not used, 1 to report\n\n\nDepending upon the entered options above subsequent conditional inputs may be need.\n\nCOND if selectivity specs number of bins to report &gt; 0:\n\nVector of bin indexes or -1 in first bin will overwrite entered vector with auto-generated list to cover logical range\n\nCOND if growth specs number of bins to report &gt; 0 and empirical weight at age not used:\n\n… as for selectivity bins\n\nCOND if numbers-at-age specs number of ages &gt; 0:\n\n… as for selectivity bins\n\nCOND == 2 and mortality-at-age specs number of ages &gt; 0:\n\n… as for selectivity bins\n\n\n\n\n\nExample Input:\n\n\n\n\n2\n# 0 = No additional std dev reporting;\n\n\n\n# 1 = read values below; and\n\n\n\n# 2 = read specification for reporting stdev for selectivity, size,numbers, and\n\n\n\n# natural mortality.\n\n\n1 1 -1 5\n# Selectivity\n\n\n1 5\n# Growth\n\n\n1 -1 5\n# Numbers-at-age\n\n\n1 5\n# M-at-age\n\n\n1\n# Dynamic Bzero\n\n\n1\n# Summary Biomass\n\n\n5 15 25 35 38\n# Vector with selectivity std bins (-1 in first bin to self-generate)\n\n\n1 2 5 10 15\n# Vector with growth std ages picks (-1 in first bin to self-generate)\n\n\n1 2 5 10 15\n# Vector with numbers-at-age std ages (-1 in first bin to self-generate)\n\n\n1 2 5 10 15\n# Vector with M-at-age std ages (-1 in first bin to self-generate)\n\n\n999\n#End of the control file input"
  },
  {
    "objectID": "qmds/html.html#empirical-weight-at-age-wtatage.ss",
    "href": "qmds/html.html#empirical-weight-at-age-wtatage.ss",
    "title": "RAW HTML CONTENT",
    "section": "9.1 Empirical Weight-at-Age (wtatage.ss)",
    "text": "9.1 Empirical Weight-at-Age (wtatage.ss)\nThe model has the capability to read empirical body weight at age for the population and each fleet, in lieu of generating these weights internally from the growth parameters, weight-at-length, and size-selectivity. Selection of this option is done by setting an explicit switch near the top of the control file. The values are read from a separate file named, wtatage.ss. This file is only required to exist if this option is selected.\nThe first value read is a single integer for the maximum age used in reading this file. So if the maximum age is 40, there will be 41 columns of weight-at-age entries to read, with the first column being for age 0. If the number of ages specified in this table is greater than maximum age in the model, the extra weight-at-age values are ignored. If the number of ages in this table is less than maximum age in the model, the weight-at-age data for the number of ages in the file is filled in for all unread ages out to maximum age.\nThe format of this input file is:\n\n\n\n40\nMaximum Age\n\n\n\n\n\n\n\nGrowth\nBirth\n\n\n\n\n\n\nYear\nSeason\nSex\nPattern\nSeason\nFleet\nAge-0\nAge-1\n...\n\n\n\n1\n1\n1\n1\n-2\n0\n0\n0.1003\n\n\n\n1\n1\n1\n1\n-1\n0.0169\n0.0864\n0.2495\n\n\n\n1\n1\n1\n1\n0\n...\n...\n...\n\n\n\n1\n1\n1\n1\n1\n...\n...\n...\n\n\n\n1\n1\n1\n1\n2\n...\n...\n...\n\n\n-9999\n1\n1\n1\n1\n0\n...\n...\n...\n\n\n\nwhere:\n\nFleet = -2 is age-specific fecundity (e.g., fecundity-scalar*maturity*weight-at-age), so time-varying fecundity is possible to implement.\nFleet = -1 is population weight-at-age at middle of the season.\nFleet = 0 is population weight-at-age at the beginning of the season.\nThere must be an entry for fecundity, population weight-at-age at the middle of the season, population weight-at-age at the beginning of the season, and weight-at-age for each fleet (even if these vectors are identical in some cases).\nFleets that do not use biomass do not need to have weight-at-age assigned.\nGrowth pattern and birth season probably will never be used, but are included for completeness.\nA negative value for year will fill the table from that year through the ending year of the forecast, overwriting anything that has already been read for those years.\nJudicious use of negative years in the right order will allow user to enter blocks without having to enter a row of info for each year\nThere is no internal error checking to verify that weight-at-age has been read for every fleet and for every year.\nIn the future, there could be an option to use another value of the control file switch to turn off all aspects of growth parameters and size selectivity.\nThe values entered for endyr + 1 will be used for the benchmark calculations and for the forecast; this aspect needs a bit more checking.\n\n\n9.1.0.1 Caveats\n\n\n\nThe growth curves is calculated from the input parameters and can still calculate size-selectivity and can still examine size-composition data.\nHowever, there is no calculation of weight-at-age from the growth input, so there is no way to compare the input weight-at-age from the weight-at-age derived from the growth parameters.\nIf weight-at-age is read and size-selectivity is used, a warning is generated.\nIf weight-at-age is read and discard/retention is invoked, then a warning is generated because of untested consequences for the body weight of discarded fish.\nAge-0 fish seem to need to have weight=0 for spawning biomass calculation (code -2).\n\n\n\n9.1.0.2 User Testing\n\n\n\nSetup a model with age-maturity (option 2) and only age selectivity.\nTake the output calculation of weight-at-age and fecundity-at-age from report.sso and put into wtatage.ss (as shown above).\nRe-run the model with this input weight-at-age (Maturity_ Option 5) to see if identical results are produced relative to the run that had generated the weight-at-age from the growth parameters."
  },
  {
    "objectID": "qmds/html.html#runnumber.ss",
    "href": "qmds/html.html#runnumber.ss",
    "title": "RAW HTML CONTENT",
    "section": "9.2 runnumber.ss",
    "text": "9.2 runnumber.ss\nThis file contains a single integer value. It is read when the program starts, incremented by 1, used when processing the profile value inputs (see below), used as an identifier in the batch output, then saved with the incremented value. Note that this incrementation may not occur if a run crashes."
  },
  {
    "objectID": "qmds/html.html#profilevalues.ss",
    "href": "qmds/html.html#profilevalues.ss",
    "title": "RAW HTML CONTENT",
    "section": "9.3 profilevalues.ss",
    "text": "9.3 profilevalues.ss\nThis file contains information for changing the value of selected parameters for each run in a batch. In the control file, each parameter that will be subject to modification by profilevalues.ss is designated by setting its phase to -9999 .\nThe first value in profilevalues.ss is the number of parameters to be batched. This value MUST match the number of parameters with phase set equal to -9999 in the control file. The program performs no checks for this equality. If the value is zero in the first field, then nothing else will be read. Otherwise, the model will read runnumber * Nparameters values and use the last Nparameters of these to replace the initial values of parameters designated with phase = –9999 in the control file.\nUsage Note: If one of the batch runs crashes before saving the updated value of runnumber.ss, then the processing of the profilevalue.ss will not proceed as expected. Check the output carefully until a more robust procedure is developed. Also, this options was created before use of R became widespread. You probably can create a more flexible approach using R today."
  },
  {
    "objectID": "qmds/html.html#command-line-interface",
    "href": "qmds/html.html#command-line-interface",
    "title": "RAW HTML CONTENT",
    "section": "11.1 Command Line Interface",
    "text": "11.1 Command Line Interface\nThe name of the SS3 executable files often contains the phrase \"safe\" or \"opt\" (for optimized). The safe version includes checking for out of bounds values and should always be used whenever there is a change to the data file. The optimized version runs slightly faster but can result in data not being included in the model as intended if the safe version has not been run first. A file named \"ss.exe\" is typically the safe version unless the result of renaming by the user.\nOn Mac and Linux computers, the executable does not include an extension (like .exe on Windows). Running the executable on from the DOS command line in Windows simply require typing the executable name (without the .exe extension):\n\n &gt; ss\n    \n\nOn Mac and Linux computers, the executable name must be preceded by a period and slash (unless its location has been added to the user’s PATH). Note that the user may need to change permissions for Stock Synthesis to be executable before running SS3 for the first time:\n\n &gt; chmod a+x ss\n    &gt; ./ss\n    \n\nAdditional ADMB commands can follow the executable name, such as \"-nohess\" to avoid calculating the Hessian matrix. To see a full list of options, add \" -?\" after the executable name (with a space in between).\nOn all operating systems, a copy of the SS3 executable can either be located in the same directory as the model input files or in a central location and referenced either by adding it to the PATH or by a script files. Further discussion on script files for Windows is below.\nOften there is a need to run the model with no estimation. Alternative methods to run SS3 without estimating parameters are documented in the Running Without Estimation section.\nAs of ADMB 12.3, a new command called \"-hess_step\" is available to use and documented in the Using -hess_step to do additional Newton steps using the inverse Hessian\n\n11.1.1 Example of DOS batch input file\nOne file management approach is to put ss.exe in its own folder (example: C:\\SS_model) and to put your input files in separate folder (example: C:\\My Documents \\SS_runs). Then a DOS batch file in the SS_runs folder can be run at the command line to start ss.exe. All output will appear in SS_runs folder.\nA DOS batch file (e.g., SS.bat) might contain some explicit ADMB commands, some implicit commands, and some DOS commands:\n\n c:\\SS_model\\ss.exe -cbs 5000000000 -gbs 50000000000 \\%1 \\%2 \\%3 \\%4 \n    del ss.r0*\n    del ss.p0*\n    del ss.b0*\n    \n\nIn this batch file, the -cbs and -gbs arguments allocate a large amount of memory for SS3 to use (you may need to edit these for your computer and SS3 configuration), and the %1, %2 etc. allows passing of command line arguments such as -nox or -nohess. You add more items to the list of % arguments as needed.\nAn easy way to start a command line in your current directory (SS_runs) is to create a shortcut to the DOS command line prompt. The shortcut’s target would be:\n\n &gt; %SystemRoot%\\system32\\cmd.exe\n    \n\nAnd it would start in:\n\n &gt; %CURRDIR%\n    \n\nAn alternative shortcut is to have the executable within the model folder then use Ctrl+Shift+Right Click and then select either \"Open powershell window here\" or \"Open command window here\", depending upon your computer. From the command window the executable name can be typed along with additional inputs (e.g., -nohess) and the model run. If using the powershell type cmd and then hit enter prior to calling the model (ss).\n\n\n11.1.2 Simple Batch\nThis first example relies upon having a set of prototype SS3 input files, where a starter file named starter.r01 can be renamed to starter.ss and then used in the SS3 run. The example also copies one of the output files, ss.std, to a new name, ss-std01.txt, to save it from being overwritten in subsequent runs. The example code should be put in a batch file, which can have any name with the .bat extension. Note that brief output from each run will be appended to cumreport.sso (see below).\n\n del ss.cor\n    del ss.std\n    copy starter.r01 starter.ss\n    c:\\admodel\\ss\\ss.exe -sdonly\n    copy ss.std ss-std01.txt\n    \n\nThe commands could be repeated again, except the output should be copied to a different file, e.g., ss-std02.txt. This sequence can be repeated an unlimited number of times.\n\n\n11.1.3 Complicated Batch\nThis second example processes 25 dat files from a different directory, each time using the same ctl and nam file. The loop index is used in the file names, and the output is searched for particular keywords to accumulate a few key results into the file SUMMARY.TXT. Comparable batch processing can be accomplished by using R or other script processing programs.\n\n del summary.txt\n    del ss-report.txt\n    copy /Y runnumber.zero runnumber.ss\n    FOR /L \\%\\%i IN (1,1,25) DO (\n    copy /Y ..\\MakeData\\A1-D1-%%i.dat  Asel.dat\n    del ss.std\n    del ss.cor\n    del ss.par\n    c:\\admodel\\ss\\ss.exe\n    copy /Y ss.par A1-D1-A1-%%i.par\n    copy /Y ss.std A1-D1-A1-%%i.std\n    find \"Number\" A1-D1-A1-%%i.par &gt;&gt; Summary.txt\n    find \"hessian\" ss.cor &gt;&gt; Summary.txt)\n    \n\n\n\n\n\n\n11.1.4 Running Without Estimation\nThere may be time when users will want to run the model without parameter estimation. The ADMB command -noest will not work with Stock Synthesis, as it bypasses the procedure section. There are two suggested alternative approaches to do this with SS3 and ADMB.\nThe first approach requires the user to change the maximum phase value in the starter.ss file to 0 then running the model via the command widow as without calculating the hessian:\n\n ss -nohess\n    \n\nThe second approach is done all through the command window using the following commands:\n\n ss -maxfn 0 -phase 50 -nohess\n    \n\nwhere maxfun specifies the number of function calls and phase is the maximum phase for the model to start estimation where the number should be greater than the maximum phase for estimating parameters within the model.\nHowever, the approaches above differ in subtle ways. First, if the maximum phase is set to 0 in the starter file the total likelihood will differ by a small amount (0.25 likelihood units) compared to the second approach which sets the maxfun and phase in the command window. This small difference is due a dummy parameter which is evaluated by the objective function when maximum phase in the starter is set to 0, resulting in a small contribution to the total likelihood of 0.25. However, all other likelihood components should not change.\nThe second difference between the two no estimation approaches is the reported number of \"Active_count\" of parameters in the Report file. If the command line approach is used (ss -maxfn 0 -phase 50 -nohess) then the active number of parameters will equal the number of parameters with positive phases, but because the model is started in a phase greater than the maximum phase in the model, these parameters do not move from the initial values in the control file (or the par file). The first approach where the maximum phase is changed in the starter file will report the number of \"Active_count\" parameters as 0.\nThe final thing to consider when running a model without estimation is whether you are starting from the par file or the control file. If you start from the par file (specified in the starter file: 1=use ss.par) then all parameters, including parameter deviations, will be fixed at the estimated values. However, if the model is not run with the par file, any parameter deviations (e.g., recruitment deviations) will not be included in the model run (a user could paste in the estimated recruitment deviations into the control file).\n\n11.1.4.1 Generate .ss_new files\n\nThere may be times a user would like to generate the .ss_new files without running the model, with or without estimation. There are two approaches that a user can take. The first is to manually change the maxphase in the starter.ss file to -1 and running the model as normal will generate these files without running through the model dynamics (e.g., no Report file will be created). The maxphase in the starter.ss_new file will be set to -1 and will need to be manually changed back if the intent is the replace the original (i.e., starter.ss) file with the new files (i.e., starter.ss_new). The second approach is to modify the maxphase via the command line or power shell input. Calling the model using the commands:\n\n ss -stopph -1\n    \n\nwhere -1 is the maximum phase for the model to run through (e.g., can be other values if a user would like to only run through a specific parameter phase). This approach will create all the new files with the starter.ss_new reflecting the original maxphase value in the starter.ss file. This approach is available in v.3.30.16 and later.\n\n\n\n\n\n\n11.1.5 Using -hess_step to do additional Newton steps using the inverse Hessian\nThe optimizer in ADMB is designed to run until the maximum absolute gradient (mag) is small enough (e.g., 1e-05), after which it quits and does the uncertainty calculations. But if run for longer it cannot appreciably decrease this mag. In many cases it is interesting or advisable to get closer to the mode to confirm convergence of the model.\nA new feature as of ADMB 12.3 called \"-hess_step\" takes Newton steps to update the MLE using the information in the Hessian calculated as MLEnew=MLE-(inverse Hessian)*(gradient), where the Hessian and gradient are calculated from the original MLE. If the mag improves then this corroborates the optimizer has converged and that the negative log-likelihood surface is approximately quadratic at the mode as assumed in the asymptotic uncertainty calculations. The downside is the high computational cost due to the extra matrix calculations.\nThe feature is used by optimizing normally, and then from the command line running -hess_step for defaults (recommended), -hess_step N, or -hess_step_tol eps where N and eps are the maximum number of steps to take and the tolerance (i.e., a very small number like 1e-10) after which to stop. When running the Hessian first and then the -hess_step, ADMB will prompt you to run it with -binp ss.bar.\n\n\n11.1.6 Running Parameter Profiles\nUsers will often want to run profiles over specific parameter to evaluate the information in the model to estimate the parameter based on changes in the log likelihood. There are two ways this can be done.\nThe first option is the use functions within r4ss to run the profile, summarize quantities across runs, and plot the output. The SS_profile() function will run the profile based on function inputs, SSgetoutput() will read quantities from each run Report file, SSsummarize() will summarize key model quantities, and the SSplotProfile() and PinerPlot() functions can be used to visualize results. Additional information regarding r4ss can be found in the r4ss section.\nThe second way is to create and run a batch file to profile over parameters. This example will run a profile on natural mortality and spawner-recruitment steepness, of course. Edit the control file so that the natural mortality parameter and steepness parameter lines have the phase set to -9999. Edit starter.ss to refer to this control file and the appropriate data file.\n\n\n\n\nCreate a profilevalues.ss file\n\n\n\n2 # number of parameters using profile feature\n\n\n\n0.16 # value for first selected parameter when runnumber equals 1\n\n\n\n0.35 # value for second selected parameter when runnumber equals 1\n\n\n\n0.16 # value for first selected parameter when runnumber equals 2\n\n\n\n0.40 # value for second selected parameter when runnumber equals 2\n\n\n\n0.18 # value for first selected parameter when runnumber equals 3\n\n\n\n0.40 # value for second selected parameter when runnumber equals 3\n\n\n\netc.; make it as long as you like.\n\n\n\nCreate a batch file that looks something like this. Or make it more complicated as in the example above.\n\n del cumreport.sso\n    copy /Y runnumber.zero runnumber.ss  % so you will start with runnumber=0 \n    C:\\SS330\\ss.exe \n    C:\\SS330\\ss.exe \n    C:\\SS330\\ss.exe \n\nRepeat as many times as you have set up conditions in the profilevalues.ss file. The summary results will all be collected in the cumreport.sso file. Each step of the profile will have an unique run number and its output will include the values of the natural mortality and steepness parameters for that run.\n\n\n11.1.7 Re-Starting a Run\nModel runs can be restarted from a previously estimated set of parameter values. In the starter.ss file, enter a value of 1 on the first numeric input line. This will cause the model to read the file ss.par and use these parameter values in place of the initial values in the control file. This option only works if the number of parameters to be estimated in the new run is the same as the number of parameters in the previous run because only actively estimated parameters are saved to the file ss.par. The file ss.par can be edited with a text editor, so values can be changed and rows can be added or deleted. However, if the resulting number of elements does not match the setup in the control file, then unpredictable results will occur. Because ss.par is a text file, the values stored in it will not give exactly the same initial results as the run just completed. To achieve greater numerical accuracy, the model can also restart from ss.bar which is the binary version of ss.par. In order to do this, the user must make the change described above to the starter.ss file and must also enter -binp ss.bar as one of the command line options.\n\n\n11.1.8 Optional Output Subfolders\nAs of 3.30.19, users can optionally send .sso and .ss_new extension files to subfolders. To send files with a .sso extension to a subfolder within the model folder, create a subfolder called sso before running the model. To send files with a .ss_new extension to a separate subfolder, create a folder called ssnew before running the model."
  },
  {
    "objectID": "qmds/html.html#putting-stock-synthesis-in-your-path",
    "href": "qmds/html.html#putting-stock-synthesis-in-your-path",
    "title": "RAW HTML CONTENT",
    "section": "11.2 Putting Stock Synthesis in your PATH",
    "text": "11.2 Putting Stock Synthesis in your PATH\nInstead of copying the SS3 executable to each model folder, SS3 can be put in your system path, which is a list of folders that your operating system looks in whenever you type the name of a program on the command line. This approach saves on storage space since the SS3 binary (i.e., the SS3 executable or exe) is about 2.2 MB and having it located in each folder can be prohibitive in a large-scale simulation testing study. Even if you are not running a large simulation study, putting SS3 in your path may still be convenient, as you can use the same executable on many models, there is no need to specify a full file path to the executable each time you run a model, and no need to create a batch file that refers to the executable’s location.\n\n11.2.1 For Unix (OS X and Linux)\nTo check if SS3 is in your path, assuming the binary is named SS: open a Terminal window and type which SS and hit enter. If you get nothing returned, then SS3 (named SS or SS.exe) is not in your path. The easiest way to fix this is to move the SS3 binary to a folder that’s already in your path. To find existing path folders type echo $PATH in the terminal and hit enter. Now move the SS3 binary to one of these folders.\nFor example, in a Terminal window type:\n\n    sudo cp ~/Downloads/SS /usr/bin/\n\nto move an binary called SS from the Downloads folder to /usr/bin. You will need to use sudo and enter your password after to have permission to move a file to a folder like /usr/bin/, because doing so edits the system for other users also.\nAlso note that you may need to add executable permissions to the SS binary after downloading it. You can do that by switching to the folder where you placed the binary (cd /usr/bin/ if you followed the instructions above), and running the command:\n\n    sudo chmod +x SS\n\nCheck that SS3 is now executable and in your path:\n\n    which SS\n\nIf you followed the instructions above, you will see the following line returned:\n\n    /usr/bin/SS\n\nIf you have previously modified your path to add a non-standard location for the SS3 binary, you may need to also tell R about the new path. The path that R sees may not include additional paths that you have added through a configuration file like .bash_profile. If needed, you can add to the path that R sees by including a line like this in your .Rprofile file (.Rprofile is an invisible file in your home directory).\n\n    Sys.setenv(PATH=paste(Sys.getenv(\"PATH\"),\"/my/folder\",sep=\":\"))\n\n\n\n11.2.2 For Windows\nTo check if SS3 is in your path for Windows, open a DOS prompt (either Command Prompt or Powershell should work) and type SS -? and hit enter. If the prompt returns a message like SS is not recognized..., then SS3 is not in your path (assuming the SS3 executable is called SS.exe).\nTo add the SS3 binary file to your path, follow these steps:\n\nFind the correct version of the SS.exe binary on your computer (or download from the SS3 releases).\nMove to and note the folder location. E.g., C:/SS/\nClick on the start menu and type environment\nChoose Edit environment variables for your account under Control Panel\nClick on PATH if it exists, create it if does not exist\nChoose ‘PATH‘ and click edit\nIn the Edit User Variable window add to the end of the Variable value section a semicolon and the SS3 folder location you recorded earlier. E.g., ;C:/SS. Do not overwrite what was previously in the PATH variable.\nRestart your computer\nGo back to the DOS prompt and try typing SS -? and hitting return again."
  },
  {
    "objectID": "qmds/html.html#running-stock-synthesis-from-r",
    "href": "qmds/html.html#running-stock-synthesis-from-r",
    "title": "RAW HTML CONTENT",
    "section": "11.3 Running Stock Synthesis from R",
    "text": "11.3 Running Stock Synthesis from R\nUse system(“path/to/ss3”) to run Stock Synthesis from within the R console, where path/to/ss3 is the path to and name of the Stock Synthesis binary.\nAlternatively, use the function run_SS_models from the r4ss package within the R console:\n\n  # run 2 models, in directories folder_1 and folder_2, using the SS3 executable\n  # named ss3 that is in the path.\n  r4ss::run_SS_models(dirvec = c(\"folder_1\", \"folder_2\"),\n                      model = \"ss3\",\n                      exe_in_path = TRUE)\n\nRunning SS3 from within R may be desirable for setting up simulations where many runs of SS3 models are required (e.g., ss3sim) or if r4ss is already used to read model output."
  },
  {
    "objectID": "qmds/html.html#the-stock-synthesis-gui-ssi",
    "href": "qmds/html.html#the-stock-synthesis-gui-ssi",
    "title": "RAW HTML CONTENT",
    "section": "11.4 The Stock Synthesis GUI (SSI)",
    "text": "11.4 The Stock Synthesis GUI (SSI)\nStock Synthesis Interface (SSI or the SS3 GUI) provides an interface for loading, editing, and running model files, and also can link to r4ss to generate plots."
  },
  {
    "objectID": "qmds/html.html#debugging-tips",
    "href": "qmds/html.html#debugging-tips",
    "title": "RAW HTML CONTENT",
    "section": "11.5 Debugging Tips",
    "text": "11.5 Debugging Tips\nWhen input files are causing the program to crash or fail to produce sensible results, there are a few steps that can be taken to diagnose the problem. Before trying the steps below, examine the echoinput.sso file. It is highly annotated, so you should be able to see if the model is interpreting your input files as you intended. Additionally, users should check the warning.sso file when attempting to debug a non-running model.\n\nSet the turn_off_phase switch to 0 in the starter.ss file. This will cause the mode to not attempt to adjust any parameters and simply converges a dummy parameter. It will still produce a Report.sso file, which can be examined to see what has been calculated from the initial parameter values.\nTurn the verbosity level to 2 in the starter.ss file. This will cause the program to display the value of each likelihood component to the screen on each iteration. So it the program is creating an illegal computation (e.g., divide by zero), it may show you which likelihood component contains the problematic calculation. If the program is producing a Report.sso file, you may then see which observation is causing the illegal calculation.\nRun the program with the command ss &gt;&gt;SSpipe.txt. This will cause all screen display to go to the specified text file (note, delete this file before running because it will be appended to). Examination of this file will show detailed statements produced during the reading and preprocessing of input files.\nIf the model fails to achieve a proper Hessian it exits without writing the detailed outputs in the FINAL_SECTION. If this happens, you can do a run with the -nohess option so you can view the Report.sso to attempt to diagnose the problem.\nIf the problem is with reading one or more of the input files, please note that certain Mac line endings cannot be read by the model (although this is a rare occurance). Be sure to save the text files with Windows or Linux style line endings so that the executable can parse them."
  },
  {
    "objectID": "qmds/html.html#keyboard-tips",
    "href": "qmds/html.html#keyboard-tips",
    "title": "RAW HTML CONTENT",
    "section": "11.6 Keyboard Tips",
    "text": "11.6 Keyboard Tips\nTyping \"N\" during a run will cause ADMB to immediately advance to the next phase of estimation.\nTyping \"Q\" during a run will cause ADMB to immediately go to the final phase. This bypasses estimation of the Hessian and will produce all of the model outputs, which are coded in the FINAL_SECTION."
  },
  {
    "objectID": "qmds/html.html#running-mcmc",
    "href": "qmds/html.html#running-mcmc",
    "title": "RAW HTML CONTENT",
    "section": "11.7 Running MCMC",
    "text": "11.7 Running MCMC\nRunning SS3 with MCMC can be done through command line options using the default ADMB MCMC algorithm (described below). Another possibility is using the R package adnuts. See the adnuts vignette for more information. The MCMC guide for ADMB provides the most comprehensive guidance available for using MCMC with ADMB models (such as SS3). Additional guidance is available in (Monnahan et al. 2019).\nRunning SS3 with MCMC (instead of maximum likelihood estimation) provides maximum posterior density estimates, report file, Hessian matrix and the .cor file. Parameters stuck on bounds which will degrade efficiency of MCMC implementation. Two commands are needed to obtain the model results:\nRun SS3 with arguments -mcmc xxxx -mcsave yyyy\n\nWhere: xxxx is the number of iterations for the chain, and yyyy is the thinning interval (1000 is a good place to start).\nMCMC chain starts at the MPD values.\nRecommended: Remove existing .psv files in run directory to generate a new chain.\nRecommended: Before running, set the run detail switch in starter file to 0 to limit printing to the screen; reporting to screen will slow MCMC progress.\nOptional: Add -nohess to use the existing Hessian file without re-estimating.\nOptional: To start the MCMC chain from specific values change the par file: run the model with estimation, adjust the par file to the values that the chain should start from, change within the starter file for the model to begin from the par file, and call the MCMC function using ss -mcmc xxxx - mcsave yyyy -nohess -noest.\n\nRun SS3 with argument -mceval to get more summaries\n\nThis generates the posterior output files.\nOptional: Modify starter file entries to add a burn-in and thinning interval above and beyond the ADMB thinning interval applied at run time.\nRecommended: MCMC always begins with the maximum posterior density values and so a burn-in &gt;0 should always be used.\nThis step can be repeated for alternate forecast options (e.g., catch levels) without repeating step 2.\n\nNote that when the model is switched to MCMC or MCEVAL mode, all the bias adjustment factors become 1.0 for any years with recruitment deviations. A report file is not created after completing MCMC because it would show values based only on the last MCMC step."
  },
  {
    "objectID": "qmds/html.html#custom-reporting",
    "href": "qmds/html.html#custom-reporting",
    "title": "RAW HTML CONTENT",
    "section": "12.1 Custom Reporting",
    "text": "12.1 Custom Reporting\n\nAdditional\n\nuser control for what is included in the Report.sso file was added in v.3.30.16. This approach allows for full customizing of what is printed to the Report file by selecting custom reporting (option = 3) in the starter file where specific items now can be included or excluded depending upon a list passed to SS3 from the starter file. The numbering system for each item in the Report file is as follows:\n\n\n\n\nNum.\nReport Item\nNum.\nReport Item\n\n\n\n\n1\nDEFINITIONS\n31\nLEN SELEX\n\n\n2\nLIKELIHOOD\n32\nAGE SELEX\n\n\n3\nInput Variance Adjustment\n33\nENVIRONMENTAL DATA\n\n\n4\nParm devs detail\n34\nTAG Recapture\n\n\n5\nPARAMETERS\n35\nNUMBERS-AT-AGE\n\n\n6\nDERIVED QUANTITIES\n36\nBIOMASS-AT-AGE\n\n\n7\nMGparm By Year after adjustments\n37\nNUMBERS-AT-LENGTH\n\n\n8\nselparm(Size) By Year after adjustments\n38\nBIOMASS-AT-LENGTH\n\n\n9\nselparm(Age) By Year after adjustments\n39\nF-AT-AGE\n\n\n10\nRECRUITMENT DIST\n40\nCATCH-AT-AGE\n\n\n11\nMORPH INDEXING\n41\nDISCARD-AT-AGE\n\n\n12\nSIZEFREQ TRANSLATION\n42\nBIOLOGY\n\n\n13\nMOVEMENT\n43\nNatural Mortality\n\n\n14\nEXPLOITATION\n44\nAGE SPECIFIC K\n\n\n15\nCATCH\n45\nGrowth Parameters\n\n\n16\nTIME SERIES\n46\nSeas Effects\n\n\n17\nSPR SERIES\n47\nBiology at age in endyr\n\n\n18\nKobe Plot\n48\nMEAN BODY WT(Begin)\n\n\n19\nSPAWN RECRUIT\n49\nMEAN SIZE TIMESERIES\n\n\n20\nSPAWN RECR CURVE\n50\nAGE LENGTH KEY\n\n\n21\nINDEX 1\n51\nAGE AGE KEY\n\n\n22\nINDEX 2\n52\nCOMPOSITION DATABASE\n\n\n23\nINDEX 3\n53\nSELEX database\n\n\n24\nDISCARD SPECIFICATION\n54\nSPR/YPR Profile\n\n\n25\nDISCARD OUTPUT\n55\nGLOBAL MSY\n\n\n26\nMEAN BODY WT OUTPUT\n56\nSS_summary.sso\n\n\n27\nFIT LEN COMPS\n57\nrebuilder.sso\n\n\n28\nFIT AGE COMPS\n58\nSIStable.sso\n\n\n29\nFIT SIZE COMPS\n59\nDynamic Bzero\n\n\n30\nOVERALL COMPS\n60\nwtatage.ss_new"
  },
  {
    "objectID": "qmds/html.html#standard-admb-output-files",
    "href": "qmds/html.html#standard-admb-output-files",
    "title": "RAW HTML CONTENT",
    "section": "12.2 Standard ADMB output files",
    "text": "12.2 Standard ADMB output files\nStandard ADMB files are created by SS3. These are:\nss.par - This file has the final parameter values. They are listed in the order they are declared in SS3. This file can be read back into SS3 to restart a run with these values (see Running Stock Synthesis for more info).\nss.std - This file has the parameter values and their estimated standard deviation for those parameters that were active during the model run. It also contains the derived quantities declared as standard deviation report variables. All of this information is also report in the covar.sso. Also, the parameter section of Report.sso lists all the parameters with their SS3 generated names, denotes which were active in the reported run, displays the parameter standard deviations, then displays the derived quantities with their standard deviations.\nss.rep - This report file is created between phases so, unlike Report.sso, will be created even if the hessian fails. It does not contain as much output as shown in Report.sso.\nss.cor - This is the standard ADMB report for parameter and standard deviation report correlations. It is in matrix form and challenging to interpret. This same information is reported in covar.sso."
  },
  {
    "objectID": "qmds/html.html#stock-synthesis-summary",
    "href": "qmds/html.html#stock-synthesis-summary",
    "title": "RAW HTML CONTENT",
    "section": "12.3 Stock Synthesis Summary",
    "text": "12.3 Stock Synthesis Summary\nThe ss_summary.sso file (available for versions 3.30.08.03 and later) is designed to put key model outputs all in one concise place. It is organized as a list. At the top of the file are descriptors, followed by the 1) likelihoods for each component, 2) parameters and their standard errors, and 3) derived quantities and their standard errors. Total biomass, summary biomass, and catch were added to the quantities reported in this file in version 3.30.11 and later.\nBefore 3.30.17, TotBio and SmryBio did not always match values reported in columns of the TIME_SERIES table of Report.sso. The report file should be used instead of ss_summary.sso for correct calculation of these quantities before 3.30.17. Care should be taken when using the TotBio and SmryBio if the model configuration has recruitment after January 1 or in a later season, as TotBio and SmryBio quantities are always calculated on January 1. Consult the detailed age-, area-, and season-specific tables in report.sso for calculations done at times other than January 1."
  },
  {
    "objectID": "qmds/html.html#sis-table",
    "href": "qmds/html.html#sis-table",
    "title": "RAW HTML CONTENT",
    "section": "12.4 SIS table",
    "text": "12.4 SIS table\nThe SIS_table.sso is deprecated as of SS3 v.3.30.17. Please use the r4ss function get_SIS_info() instead.\nThe SIS_table.sso file contains model output formatted for reading into the NMFS Species Information System (SIS). This file includes an assessment summary for categories of information (abundance, recruitment, spawners, catch estimates) that are input into the SIS database. A time-series of estimated quantities which aggregates estimates across multiple areas and seasons are provided to summarize model results. Access to the SIS database is granted to all NOAA employees."
  },
  {
    "objectID": "qmds/html.html#derived-quantities",
    "href": "qmds/html.html#derived-quantities",
    "title": "RAW HTML CONTENT",
    "section": "12.5 Derived Quantities",
    "text": "12.5 Derived Quantities\nBefore listing the derived quantities reported to the standard deviation report, there are a couple of topics that deserve further explanation.\n\n\n\n\n12.5.1 Virgin Spawning Biomass vs Unfished Spawning Biomass\nUnfished is the condition for which reference points (benchmark) are calculated. Virgin Spawning Biomass (B0) is the initial condition on which the start of the time-series depends.If biology or spawner-recruitment parameters are time-varying, then the benchmark year input in the forecast file tells the model which years to average in order to calculate \"unfished\". In this case, virgin recruitment and/or the virgin spawning biomass will differ from their unfished counterparts. Virgin recruitment and spawning biomass are reported in the mgmt_quant portion of the sd_report and are now labeled as \"unfished\" for clarity. Note that if ln(R0) is time-varying, then this will cause unfished to differ from virgin. However, if regime shift parameter is time-varying, then unfished will remain the same as virgin because the regime shift is treated as a temporary offset from virgin. Virgin spawning biomass is denoted as SPB_virgin and spawning biomass unfished is denoted as SPB_unf in the report file.\nVirgin Spawning Biomass (B0) is used in four ways within SS3:\n\nAnchor for the spawner-recruitment relationship as virgin spawning biomass.\nBasis for the initial equilibrium abundance.\nBasis against which annual depletion is calculated.\nBenchmark calculations.\n\nHowever, if there is time-varying biology, then the 4th usage can have a different B0 calculation compared to the other usages.\n\n\n12.5.2 Metric for Fishing Mortality\nA generic single metric of annual fishing mortality is difficult to define in a generalized model that admits multiple areas, multiple biological cohorts, dome-shaped selectivity in size and age for each of many fleets. Several separate indices are provided and others could be calculated by a user from the detailed information in Report.sso.\n\n\n12.5.3 Equilibrium SPR\nThis index focuses on the effect of fishing on the spawning potential of the stock. It is calculated as the ratio of the equilibrium reproductive output per recruit that would occur with the current year’s F intensities and biology, to the equilibrium reproductive output per recruit that would occur with the current year’s biology and no fishing. Thus it internalizes all seasonality, movement, weird selectivity patterns, and other factors. Because this index moves in the opposite direction than F intensity itself, it is usually reported as 1-SPR. A benefit of this index is that it is a direct measure of common proxies used for FMSY, such as F40%. A shortcoming of this index is that it does not directly demonstrate the fraction of the stock that is caught each year. The SPR value is also calculated in the benchmarks (see below).\nThe derived quantities report shows an annual SPR statistic. The options, as specified in the starter.ss file, are:\n\n0 = skip\n1 = (1-SPR)/(1-SPRTGT)\n2 = (1-SPR)/(1-SPRMSY)\n3 = (1-SPR)/(1-SPRBtarget)\n4 = raw SPR\n\nThe SPR approach to measuring fishing intensity was implemented because the concept of a single annual F does not exist in SS3 because F varies by age, sex, and growth morph and season and area. There is no single F value that is applied to all ages unless you create a very simple model setup with knife-edge selectivity. So, what you see in the options are various ways to calculate annual fishing intensity. They can be broken down into three categories. One is exploitation rate calculated simply as total catch divided by biomass from a defined age range. Another is SPR, which is a single measure of the equilibrium effect of fishing according to the F. The third category are various ways to calculate an average F. Some measures of fishing intensity will be misleading if applied inappropriately. For example, the sum of the apical F’s will be misleading if different fleets have very different selectivities or, worse, if they occur in different areas. The F=Z-M approach to getting fishing intensity is a way to have a single F that represents a number’s weighted value across multiple areas, sexes, morphs, ages. An important distinction is that the exploitation rate and F-based approaches directly relate to the fraction of the population removed each year by fishing; whereas the SPR approach represents the cumulative effect of fishing so it’s equivalent in F-space depends on M.\n\n\n12.5.4 F std\nThis index provides a direct measure of fishing mortality. The options are:\n\n0 = skip\n1 = exploitation(Bio)\n2 = exploitation(Num)\n3 = sum(Frates)\n\nThe exploitation rates are calculated as the ratio of the total annual catch (in either biomass or numbers as specified) to the summary biomass or summary numbers on January 1. The sum of the F rates is simply the sum of all the apical Fs. This makes sense if the F method is in terms of instantaneous F (not Pope’s approximation) and if there are not fleets with widely different size/age at peak selectivity, and if there is no seasonality, and especially if there is only one area. In the derived quantities, there is an annual statistic that is the ratio of the can be annual F_std value to the corresponding benchmark statistic. The available options for the denominator are:\n\n0 = raw\n1 = F/FSPR\n2 = F/FMSY\n3 = F/FBtarget\n&gt;= 11 A new option to allow for the calculation of a multi-year trailing average in F was implemented in v. 3.30.16. This option is triggered by appending the number of years to calculate the average across where an input of 1 or 11 would result in the SPRtarget with no changes. Alternatively a value of 21 would calculate F as SPRtarget with a 2-year trailing average.\n\n\n\n12.5.5 F-at-Age\nBecause the annual F is so difficult to interpret as a sum of individual F components, an indirect calculation of F-at-age is reported at the end of the report.sso file. This section of the report calculates Z-at-age simply as \\(ln(N_{a+1,t+1}/N_{a,t})\\). This is done on an annual basis and summed over all areas. It is done once using the fishing intensities as estimated (to get Z), and once with the F intensities set to 0.0 to get M-at-age. This latter sequence also provides a measure of dynamic Bzero. The user can then subtract the table of M-at-age/year from the table of Z-at-age/year to get a table of F-at-age/year. From this apical F, average F over a range of ages, or other user-desired statistics could be calculated. Further work within SS3 with this table of values is anticipated.\n\n\n12.5.6 MSY and other Benchmark Items\nThe following quantities are included in the sdreport vector mgmt_quantities, so obtain estimates of variance. Some additional quantities can be found in the benchmarks section of the forecast_report.sso.\n\n\n\n\nBenchmark Item\nDescription\n\n\n\n\nBenchmark Item\nDescription\n\n\nSSB_Unfished\nUnfished reproductive potential (SSB is commonly female mature spawning biomass).\n\n\nTotBio_Unfished\nTotal age 0+ biomass on January 1.\n\n\nSmryBio_Unfished\nBiomass for ages at or above the summary age on January 1.\n\n\nRecr_Unfished\nUnfished recruitment.\n\n\nSSB_Btgt\nSSB at user specified SSB target.\n\n\nSPR_Btgt\nSpawner potential ratio (SPR) at F intensity that produces user specified SSB target.\n\n\nFstd_Btgt\nF statistic at F intensity that produces user specified SSB target.\n\n\nTotYield_Btgt\nTotal yield at F intensity that produces user specified SSB target.\n\n\nSSB_SPRtgt\nSSB at user specified SPR target (but taking into account the spawner-recruitment relationship).\n\n\nFstd_SPRtgt\nF intensity that produces user specified SPR target.\n\n\nTotYield_SPRtgt\nTotal yield at F intensity that produces user specified SPR target.\n\n\nSSB_MSY\nSSB at F intensity that is associated with MSY; this F intensity may be directly calculated to produce MSY, or can be mapped to F_SPR or F_Btgt.\n\n\nSPR_MSY\nSpawner potential ratio (SPR) at F intensity associated with MSY.\n\n\nFstd_MSY\nF statistic at F intensity associated with MSY.\n\n\nTotYield_MSY\nTotal yield (biomass) at MSY.\n\n\nRetYield_MSY\nRetained yield (biomass) at MSY."
  },
  {
    "objectID": "qmds/html.html#brief-cumulative-output",
    "href": "qmds/html.html#brief-cumulative-output",
    "title": "RAW HTML CONTENT",
    "section": "12.6 Brief cumulative output",
    "text": "12.6 Brief cumulative output\nCum_Report.sso: contains a brief version of the run output, which is appended to current content of file so results of several runs can be collected together. This is especially useful when a batch of runs is being processed. Unless this file is deleted, it will contain a cumulative record of all runs done in that subdirectory. The first column contains the run number."
  },
  {
    "objectID": "qmds/html.html#bootstrap-data-files",
    "href": "qmds/html.html#bootstrap-data-files",
    "title": "RAW HTML CONTENT",
    "section": "12.7 Bootstrap Data Files",
    "text": "12.7 Bootstrap Data Files\nIt is possible to create bootstrap data files for SS3 where an internal parametric bootstrap function generates a simulated data set by parametric bootstrap sampling the expected values given the input observation error. Starting in version 3.30.19, bootstrap data files are output separated in single numbered files (e.g., data_boot_001.ss). In version prior to version 3.30.19 a single file called data.ss_new was output that contained multiple sections: the original data echoed out, the expected data values based on the model fit, and then subsequent bootstrap data files.\nSpecifying the number of bootstrap data files has remained the same across model versions. Creating bootstrap data files is specified in the starter file via the \"Number of datafiles to produce\" line where a value of 3 or greater will create three files: the original data file, data_echo.ss_new, a data file with the model expected values, data_expval.ss, and single bootstrap data file, data_boot_001.ss. The first output provides the unaltered input data file (with annotations added). The second provides the expected values for only the data elements used in the model run. The third and subsequent outputs provide parametric bootstraps around the expected values.\nThe bootstrapping procedure within SS3 is done via the following steps:\n\nExpected values of all input data are calculated (these are also used in the likelihood which compares observed to expected values for all data). The calculation of these expected values is described in detail under the \"Observation Model\" section of the appendix to Methot and Wetzel (2013).  \nParametric bootstrap data are calculated for each observation by sampling from a probability distribution corresponding to the likelihood for that data type using the expected values noted above. Examples of how this happens include the following:\n\nIndices of abundance are sampled from the distribution used in the estimation model (as set in the “Errtype” column of the index configuration, most commonly lognormal but could be normal or T-distribution). The variability of the distribution from which the random sample is drawn is based on the combination of the input uncertainty and any estimated \"Extra SD\" parameter and any \"add_to_survey_CV\" value included under the input variance adjustments factors.\nLength and age compositions are sampled from multinomial distributions with expected proportions in each bin based on the expected values and sample size equal to the adjusted input sample size (input sample size multiplied by any inputs for \"mult_by_lencomp_N\" or \"mult_by_agecomp_N\" under the input variance adjustments factors).\nDiscard data (fractions or absolute amounts) are generated from the chosen distribution (T-distribution, normal, log-normal, truncated-normal).\nTagging data is generated using a negative binomial distribution to get the total number of recaptures for each tag group and a multinomial distribution to allocate those recaptures among fleets.\n\n\nGiven this, there are some assumptions implicit in the bootstrapping procedure (as implemented as of v.3.30.17) that users should be aware of:\n\nThis procedure is strictly an observation error approach (i.e., process error in recruitment or any other time-varying parameter is not added). For simulation analyses, a common approach has been to input a new time series of recruitment deviations for each bootstrap data set.\nThe sample size for conditional age-at-length data matches the inputs for each length bin. If stratified sampling is used, this may be appropriate, but if the ages represent a random subset of the selected population, this may result in less variability than if the associated length distribution were resampled.\nCurrently, the aging error matrix is multiplied by the expected distribution of proportions at age, while the more correct order of operations would be to sample true ages, and then sample the observed age including aging error (it is possible these are mathematically identical).\nOften there is need to explore the removal (not include in the model fitting) of specific years in a data set which can be done by specifying a negative fleet number. If bootstrapping a data file, note that specifying a negative fleet in the data inputs for indices, length composition, or age composition will include the \"observation\" in the model (hence generating predicted values and bootstrap data sets for the data), but not in the negative log likelihood. The \"observation values\" used with negative fleet do not influence the predicted values, except when using tail compression with length or age composition. Non-zero values greater than the minimum tail compression should be used for the observation values when tail compression is being used, as using zeros or values smaller than the minimum tail compression can cause the predicted values to be reported as zero and shift predictions to other bins.\nAs of SS3 v.3.30.15, age and length composition data that use the Dirichlet-Multinomial distribution in the model are generated using the Dirichlet-Multinomial in bootstrap data sets."
  },
  {
    "objectID": "qmds/html.html#forecast-and-reference-points-forecast-report.sso",
    "href": "qmds/html.html#forecast-and-reference-points-forecast-report.sso",
    "title": "RAW HTML CONTENT",
    "section": "12.8 Forecast and Reference Points (Forecast-report.sso)",
    "text": "12.8 Forecast and Reference Points (Forecast-report.sso)\nThe Forecast-report file contains output of fishery reference points and forecasts. It is designed to meet the needs of the Pacific Fishery Management Council’s Groundfish Fishery Management Plan, but it should be quite feasible to develop other regionally specific variants of this output.\nThe vector of forecast recruitment deviations is estimated during an additional model estimation phase. This vector includes any years after the end of the recruitment deviation time series and before or at the end year. When this vector starts before the ending year of the time series, then the estimates of these recruitments will be influenced by the data in these final years. This is problematic, because the original reason for not estimating these recruitments at the end of the time series was the poor signal/noise ratio in the available data. It is not that these data are worse than data from earlier in the time series, but the low amount of data accumulated for each cohort allows an individual datum to dominate the model’s fit. Thus, an additional control is provided so that forecast recruitment deviations during these years can receive an extra weighting in order to counter-balance the influence of noisy data at the end of the time series.\nAn additional control is provided for the fraction of the log-bias adjustment to apply to the forecast recruitments. Recall that R is the expected mean level of recruitment for a particular year as specified by the spawner-recruitment curve and R’ is the geometric mean recruitment level calculated by discounting R with the log-bias correction factor \\(e-0.5s^2\\). Thus a lognormal distribution of recruitment deviations centered on R’ will produce a mean level of recruitment equal to R. During the modeled time series, the virgin recruitment level and any recruitments prior to the first year of recruitment deviations are set at the level of R, and the lognormal recruitment deviations are centered on the R’ level. For the forecast recruitments, the fraction control can be set to 1.0 so that 100% of the log-bias correction is applied and the forecast recruitment deviations will be based on the R’ level. This is certainly the configuration to use when the model is in MCMC mode. Setting the fraction to 0.0 during maximum likelihood forecasts would center the recruitment deviations, which all have a value of 0.0 in maximum likelihood mode, on R. Thus would provide a mean forecast that would be more comparable to the mean of the ensemble of forecasts produced in MCMC mode. Further work on this topic is underway.\nNote:\n\nCohorts continue growing according to their specific growth parameters in the forecast period rather than staying static at the end year values.\nEnvironmental data entered for future years can be used to adjust expected recruitment levels. However, environmental data will not affect growth or selectivity parameters in the forecast.\n\nThe top of the Forecast-report file shows the search for FSPR and the search for FMSY, allowing the user to verify convergence. Note: if the STD file shows aberrant results, such as all the standard deviations being the same value for all recruitments, then check the FMSY search for convergence. The FMSY can be calculated, or set equal to one of the other F reference points per the selection made in starter.ss."
  },
  {
    "objectID": "qmds/html.html#main-output-file-report.sso",
    "href": "qmds/html.html#main-output-file-report.sso",
    "title": "RAW HTML CONTENT",
    "section": "12.9 Main Output File, Report.sso",
    "text": "12.9 Main Output File, Report.sso\nThis is the primary output file. Its major sections (as of SS3 v.3.30.16) are listed below.\nThe sections of the output file are:\n\nSS3 version number with date compiled. Time and date of model run. This info appears at the top of all output files.\nComments\n\nInput file lines starting with #C are echoed here.\n\nKeywords\n\nList of keywords used in searching for output sections.\n\nDefinitions\n\nList of definitions (e.g., fleet names, model start year) assigned in the data and control files.\n\nLikelihood\n\nFinal values of the negative log(likelihood) are presented.\n\nInput Variance Adjustments\n\nThe matrix of input variance adjustments is output here because these values affect the log likelihood calculations.\n\nParm deviations detail\n\nDetails about parameter deviations, if used in the model. Will be missing if no parameter devs were used.\n\nParameters\n\nThe parameters are listed here. For the estimated parameters, the display shows: Num (count of parameters), Label (as internally generated by SS3), Value, Active_Cnt, Phase, Min, Max, Init, Prior, Prior_type, Prior_SD, Prior_Like, Parm_StD (standard deviation of parameter as calculated from inverse Hessian), Status (e.g., near bound), and Pr_atMin (value of prior penalty if parameter was near bound). The Active_Cnt entry is a count of the parameters in the same order they appear in the ss.cor file.\n\nDerived Quantities\n\nThis section starts by showing the options selected from the starter.ss and forecast.ss input files:\n\nSPR ratio basis\nF report basis\nB ratio denominator\n\n\n\nThen the time series of output, with standard deviation of estimates, are produced with internally generated labels. Note that these time series extend through the forecast era. The order of the output is: spawning biomass, recruitment, SPRratio, Fratio, Bratio, management quantities, forecast catch (as a target level), forecast catch as a limit level (OFL), Selex_std, Grow_std, NatAge_std. For the three \"ratio\" quantities, there is an additional column of output showing a Z-score calculation of the probability that the ratio differs from 1.0. The \"management quantities\" section is designed to meet the terms of reference for west coast groundfish assessments; other formats could be made available upon request. The standard deviation quantities at the end are set up according to specifications at the end of the control input file. In some cases, a user may specify that no derived quantity output of a certain type be produced. In those cases, SS3 substitutes a repeat output of the virgin spawning biomass so that vectors of null length are not created.\n\nMortality and growth parameters by year after adjustments\n\nThis block shows the time series of mortality and growth parameters by year after adjustments by environmental links, blocks and deviations.\n\nSelectivity parameters (size) by year after adjustments\n\nThis block shows the size selectivity parameters, after adjustment, for each year in which a change occurs.\n\nSelectivity parameters (age) by year after adjustments\n\nThis block shows the age selectivity parameters, after adjustment, for each year in which a change occurs.\n\nRecruitment Distribution\n\nThis block shows the distribution of recruitment across growth patterns, sexes, birth seasons, and areas in the end year of the model.\n\nGrowth Morph Indexing\n\nThis block shows the internal index values for various quantities. It can be a useful reference for complex model setups. The vocabulary is: Bio_Pattern refers to a collection of cohorts with the same defined growth and natural mortality parameters; sex is the next main index. If recruitment occurs in multiple seasons, then birth season is the index for that factor. The index labeled \"Platoon\" is used as a continuous index across all the other factor-specific indices. If sub-platoons are used, they are nested within the Bio_Pattern x Sex x Birth Season platoon. However, some of the output tables use the column label \"platoon\" as a continuous index across platoons and sub-platoons. Note that there is no index here for area. Each of the cohorts is distributed across areas and they retain their biological characteristics as they move among areas.\n\nSize Frequency Translation\n\nIf the generalized size frequency approach is used, this block shows the translation probabilities between population length bins and the units of the defined size frequency method. If the method uses body weight as the accumulator, then output is in corresponding units.\n\nMovement\n\nThis block shows movement rate between areas in a multi-area model.\n\nExploitation\n\nThis block shows the time series of the selected F_std unit and the F multiplier for each fleet in terms of harvest rate (if Pope’s approximation is used) or fully selected F.\n\nCatch\nTime Series\nSPR Series\nKobe Plot\n\nReports output in a table needed to create a Kobe Plot.\n\nSpawn Recruit Parameters and Table\n\nExtensive information on Spawn-recruit parameter values and derived quantities.\n\nSpawn Recruit Curve\n\nA table containing information to recreate the spawn-recruit curve.\n\nIndex 1\nIndex 2\n\nThis section reports the observed and expected values for each index. All are reported in one list with index number included as a selection field. At the bottom of this section, the root mean squared error of the fit to each index is compared to the mean input error level to assist the user in gauging the goodness-of-fit and potentially adjusting the input level of imprecision.\n\nIndex 3\n\nThis section shows the parameter number assigned to each parameter used in this section.\n\nDiscard Specification\nDiscard Output\n\nThis is the list of observed and expected values for the amount (or fraction) discard.\n\nMean Body Wt\n\nThis is the list of observed and expected values for the mean body weight.\n\nFit Len Comps\n\nThis is the list of the goodness of fit to the length compositions. The input and output levels of effective sample size are shown as a guide to adjusting the input levels to better match the model’s ability to replicate these observations.\n\nFit Age Comps\n\nThis has the same format as the length composition section.\n\nFit Size Comps\n\nThis has the same format as the length composition section and is used for the generalized size composition summary.\n\nOverall Comps\nLen Selex\n\nHere is the length selectivity and other length specific quantities for each fishery and survey.\n\nAge Selex\n\nHere is reported the time series of age selectivity and other age-related quantities for each fishery and survey. Some are directly computed in terms of age, and others are derived from the combination of a length-based factor and the distribution of size-at-age.\n\nEnvironmental Data\n\nThe input values of environmental data are echoed here. In the future, the summary biomass in the previous year will be mirrored into environmental column -2 and that the age zero recruitment deviation into environmental column -1. Once so mirrored, they may enable density-dependent effects on model parameters.\n\nTag Recapture Information\nNumbers at Age\n\nThe output (in thousands of fish) is shown for each cohort tracked in the model.\n\nBiomass at Age\nNumbers at Length\n\nThe output is shown for each cohort tracked in the model.\n\nBiomass at Length\nF at Age\nCatch at Age\n\nThe output is shown for each fleet. It is not necessary to show by area because each fleet operates in only one area.\n\nDiscard at Age\nBiology\n\nThe first biology section shows the length-specific quantities in the ending year of the time series only. The derived quantity spawn is the product of female body weight, maturity and fecundity per weight. The second section shows natural mortality.\n\nNatural Mortality\nAge-specific K\nGrowth Parameters\n\nThis section shows the growth parameters, and associated derived quantities, for each year in which a change is estimated.\n\nSeasonal Effects\nBiology at Age\n\nThis section shows derived size-at-age and other quantities. As of v3.30.21 sex ratio is reported by area in this output table.\n\nMean Body Wt (begin)\n\nThis section reports the time series of mean body weight for each platoon. Values shown are for the beginning of each season of each year.\n\nMean Size Time series\n\nThis section shows the time series of mean length-at-age for each platoon. At the bottom is the average mean size as the weighted average across all platoons for each sex.\n\nAge Length Key\n\nThis is reported for the midpoint of each season in the ending year.\n\nAge Age Key\n\nThis is the calculated distribution of observed ages for each true age for each of the defined ageing keys.\n\nComposition Database\n\nContains the length composition, age composition, and mean size-at-age observed and expected values. It is arranged in a database format, rather than an array of vectors.\n\nSelectivity Database\n\nThis section contains the selectivities organized as a database, rather than as a set of vectors.\n\nSPR/YPR Profile\nGlobal MSY Report\nDynamic Bzero Report"
  },
  {
    "objectID": "qmds/html.html#using-time-varying-parameters",
    "href": "qmds/html.html#using-time-varying-parameters",
    "title": "RAW HTML CONTENT",
    "section": "14.1 Using Time-Varying Parameters",
    "text": "14.1 Using Time-Varying Parameters\n\n\n\n\n14.1.1 Time-Varying Parameters\nStarting in SS3.30, mortality-growth, some stock-recruitment, catchability, and selectivity base parameters can be time varying. Note that as of SS3.30.16, time-varying parameters cannot be used with tagging parameters. There are four ways a parameter can be time-varying in SS3:\n\nEnvironmental or Density dependent Linkages: Links the base parameter with environmental data or a model derived quantity.\nParameter deviations: Creates annual deviations from the base parameter during a user-specified range of years.\nTime blocks: The base parameter is changed during a \"block\" (or \"blocks\") of time (i.e., one or more consecutive years) as specified by the user.\nTrends: A trend (shape: cumulative normal distribution function) is applied to the parameter. Trends are specified using the same input column as time blocks, but with different codes. This means that trends and time blocks cannot be used simultaneously for the same base parameter.\n\nEnvironmental and density dependent linkages, parameter deviations, and either time blocks or trends can be applied to the same base parameter. The model processes each time-varying parameter specification (first time blocks and trends, then environmental linkages, then parameter deviations) and creates a time-series of intermediate values that are used as the model subsequently loops through years.\n\n\n\n\n\n\n14.1.2 Specification of Time-Varying Parameters: Long Parameter Lines\nTime-varying specifications for a parameter are invoked using elements 8 - 14 in the long parameter line setup. Each element and the options for selection related to time-varying parameters are as described below.\n\n\n\n\nEnvironmental or Density Dependent Link and variable (env_var&link; element 8)\n\nThe environmental or density dependent link and variable input is two inputs specified using a single three digit number. The hundreds place contains the option for the link function, while the tens and ones place is used to specify the environmental variable or derived quantity to which the parameter is linked. Note that environmental variables can only be included on an annual basis, so seasonal models would have the same effect applied to all seasons. If the environmental link and variable input is positive, then the parameter is linked to a variable specified in the data file environmental data; if it is negative, then the parameter is linked to a derived quantity. For example, env_var&link input 103 would use link type 1 and apply it to environmental data column 3, while the input -103 would use link type 1 and apply it to the \"-3\" column which is ln(relative summary biomass). The other options for both elements are enumerated below.\nThe link function options (hundreds place) for the env_var&link input are:\n\n1 = exponential scalar: \\(P_{y} = P_{base}e^{P_{t}E_{y}}\\)\n2 = linear offset: \\(P_{y} = P_{base} + P_{t}E_{y}\\)\n3 = Bounded replacement: \\(P_{y} = min(P_{base})+\\frac{max(P_{base})-min(P_{base})}{1+e^{P_tE_y+ln((P_{base}-min(P_{base})+0.0000001)/(max(P_{base})-P_{base}+0.0000001))}}\\)\n4 = Logistic: \\(P_{y} = P_{base}\\frac{2}{1+e^{-P_{t2}(E_{y}-P_{t1})}}\\)\n\nwhere:\n\n\\(P_{y}\\) = Parameter value in year \\(y\\)\n\\(P_{base}\\) = Base parameter value\n\\(P_{t}\\) = Link parameter value\n\\(P_{t1}\\) = First of 2 link parameters (offset)\n\\(P_{t2}\\) = Second of 2 link parameters (slope)\n\\(E_{y}\\) = Environmental index value or derived quantity value in year \\(y\\)\n\\(min(P_{base})\\) = the minimum parameter bound of base parameter\n\\(max(P_{base})\\) = the maximum parameter bound of base parameter\n\nThe variable options (tens and ones place, or \\(E_{y}\\)) for the env_var&link input are either 1) a positive integer from 1 to 99 referencing a time-series located in the environmental data section of the data file, or 2) a negative value of -1 to -4 where \\(E_y\\) is one of the following model-derived quantities:\n\n-1; for ln(relative spawning biomass)\n-2; for recruitment deviation\n-3; for ln(relative summary biomass) (e.g., current year summary biomass divided by the unfished summary biomass)\n-4; for ln(relative summary numbers)\n\nThe four derived quantities are all calculated at the beginning of each year within the model, so they are available to use as the basis for time-varying parameter links without violating any order of operations rules.\n\nDeviation Link (element 9). A positive integer invokes parameter deviations, but otherwise should be left as 0. SS3 expects the estimated deviations to be normal in distribution and the deviation values are multiplied by the standard error parameter as they are used. This differs from recruitment deviations and from the approach in SS3 v.3.24. Link options for parameter deviations are:\n\n1 = multiplicative: \\(P_y = P_{base,y}e^{\\text{dev}_y*\\text{dev}_{se}}\\),\n2 = additive: \\(P_y = P_{base,y} + \\text{dev}_y*\\text{dev}_{se}\\),\n3 = random walk. Random walk options are implemented by using \\(\\rho\\) in the objective function. \\(P_y = P_{base,y} + \\sum_{n=1}^{y} \\text{dev}_n*\\text{dev}_{se}\\)\n4 = mean reverting random walk with \\(\\rho\\).\n\n\\(X_1 = \\text{dev}_1*\\text{dev}_{se}\\)\n\\(P_1 = P_{base,y} + X_1\\)\n\\(X_y = \\rho*X_{y-1} + \\text{dev}_y*\\text{dev}_{se}\\)\n\\(P_y = P_{base,y} + X_y\\)\n\n5 = mean reverting random walk with \\(\\rho\\) and a logit transformation to stay within the minimum and maximum parameter bounds (approach added in SS3 v.3.30.16)\n\n\\(X_1 = \\text{dev}_1*\\text{dev}_{se}\\)\n\\(R = P_{max} - P_{min}\\)\n\\(Y_1 = ln(\\frac{P_{base,y} - P_{min} + \\text{nil}}{P_{max} - P_{base,y} + nil})\\)\n\\(P_1 = P_{min} + \\frac{R}{1 + e^{-Y_1 - X_1 }}\\). For the first year.\n\\(X_y = \\rho*X_{y-1} + \\text{dev}_y*\\text{dev}_{se}\\)\n\\(Y_y = ln(\\frac{P_{base,y} - P_{min} + nil}{P_{max} + P_{base,y} + nil})\\)\n\\(P_1 = P_{min} + \\frac{R}{1 + e^{-Y_y - X_y }}\\). For years after the first year.\n\n6 = mean reverting random walk with penalty to keep the root mean squared error (RMSE) near 1.0. Same as case 4, but with penalty applied.\nThe option of extending the final model year deviation value subsequent years (i.e., into the forecast period) was added in v. 3.30.13. This new option is specified by selecting the appropriate deviation link option and appending a 2 at the front (e.g, 25), which will use the final year deviation value for all forecast years.\n\nwhere:\n\n\\(P_{y}\\) = Parameter value in year \\(y\\)\n\\(P_{base,y}\\) = Base parameter value for year \\(y\\)\n\\(\\text{dev}_y\\) = deviation in year \\(y\\)\n\\(\\text{dev}_{se}\\) = standard error of the deviation\nnil is a small value (e.g., 0.0000001)\n\nDeviation Minimum Year (element 10). Year deviations start for the parameter. This must be specified if using parameter deviations, but otherwise should be left as 0.\nDeviation Maximum Year (element 11). Year deviations end for parameter. This must be specified if using parameter deviations, but otherwise should be left as 0.\nDeviation Phase (element 12). The phase in which the deviations for the parameter should be estimated. This must be specified if using parameter deviations, but otherwise should be left as 0.\nUse Time Blocks or Trends (element 13). Time blocks and trends are both specified using this input. If neither are used, this should be left as 0. For trend options, the cumulative normal distribution function is used as the shape of the trend in all cases, but the parameterization differs. In general, the trend used is: \\[P_y = P_{base} + P_{\\text{offset}}\\phi(\\frac{y - P_{\\text{infl}}}{P_{width}})\\] where\n\n\\(P_y\\) is the final parameter value in year \\(y\\)\n\\(P_{base}\\) is the base parameter value\n\\(P_{\\text{offset}}\\) is the parameter offset value\n\\(\\phi\\) is the standard cumulative normal distribution function\n\\(P_{\\text{infl}}\\) is the inflection year (i.e., the year in which half of the total change from the base parameter has occured)\n\\(P_{width}\\) is the standard deviation.\n\nIn all cases, 3 parameters are estimated and hence 3 short parameter lines are required. These parameter lines differ amongst the trend options.\nThe input value options for element 13 are:\n\n&gt;0: time block index for parameter. See the time blocks section of the control file for more information on specifying time blocks.\n-1: Trend Offset option. Three parameters are estimated: end trend value as a logistic offset (input as \\(ln(P_{\\text{offset}})\\)), inflection year logistic offset (input as \\(ln(P_{\\text{infl}})\\), and width (\\(P_{width}\\). Offset trend value is in natural log space. Inflection year is also in natural log space and offset from ln(0.5). Width is directly specified.\n-2: Trend Direct input option. In this case, \\(P_{\\text{offset}} = 1\\). Three parameters are input via short parameter lines: end trend parameter value (\\(P_y\\) where \\(y\\) is the final year), inflection year (\\(P_{\\text{infl}}\\), and width (\\(P_{width}\\)).\n-3: Trend Fractional option. In this case, \\(P_{\\text{offset}} = 1\\). Three parameters will be estimated: end trend parameter value as a fraction of base parameter maximum - minimum, inflection year as a fraction of end year - start year, and width (\\(P_{width}\\)). Width is directly input.\n\nTime Block Functional Form (element 14). Leave as 0, unless time blocks are used.\n\n0: multiplicative parameter (\\(P_{block} = P_{base}*e^{P_t}\\))\n1: additive parameter (\\(P_{block} = P_{base} + P_t\\))\n2: replace parameter (\\(P_{block} = P_t\\))\n3: random walk across blocks (\\(P_{block} = P_{block,-1} + P_t\\))\n\nwhere:\n\n\\(P_{block}\\) = Final parameter value in time block \\(block\\)\n\\(P_{base}\\) = Base parameter value\n\\(P_{t}\\) = Time-varying parameter value for a time block\n\\(P_{block,-1}\\) = Final parameter value in the previous time block\n\n\nCode for the deviation link can be found in SS_timevaryparm.tpl, search for \"SS_Label_Info_14.3\".\n\n\n14.1.3 Specification of Time-Varying Parameters: Short Parameter Lines\nIf a time-varying specification set up in the long parameter lines for a particular section requires additional parameters, short parameter lines need to be created following the long parameter lines for the section (unless autogeneration is used, which creates short parameter lines in control.ss_new upon running the model). The number of parameter lines required depends on the time-varying parameter specification.\nFor example, if two parameters were specified to have environmental linkages in the MG parameter section, below the MG parameters would be two parameter lines (when not auto-generating these lines), which is an environmental linkage parameter for each time-varying base parameter:\n\n\n\n\n\n\nPrior\nPrior\nPrior\n\n\n\n\nLO\nHI\nINIT\nValue\nSD\nType\nPhase\nParameter Label\n\n\n\n\n\nPrior\nPrior\nPrior\n\n\n\n\nLO\nHI\nINIT\nValue\nSD\nType\nPhase\nParameter Label\n\n\n\n\n\n\n\n\n\n\n\n\n-99\n99\n1\n0\n0.01\n0\n-1\n#Wtlen_1_Fem_ENV_add\n\n\n-99\n99\n1\n0\n0.01\n0\n-1\n#Wtlen_2_Fem_ENV_add\n\n\n\nIn SS3 v.3.30, the time-varying input short parameter lines are organized such that all parameters that affect a base parameter are clustered together with time blocks (or trend) first, then environmental linkages, then parameter deviations. For example, if the mortality-growth (MG) base parameters 3 and 7 had time varying changes, the order would look like:\n\n\n\n\nMG base parameter 3\nBlock parameter 3-1\n\n\n\nBlock parameter 3-2\n\n\n\nEnvironmental link parameter 3-1\n\n\n\nDeviation se parameter 3\n\n\n\nDeviation \\(\\rho\\) parameter 3\n\n\nMG base parameter 7\nBlock parameter 7-1\n\n\n\nDeviation se parameter 7\n\n\n\nDeviation \\(\\rho\\) parameter 7\n\n\n\n\nThe number of short parameter lines for each time-varying setup selected depends on the selection options. The autogeneration feature can be used to figure out which parameter lines are needed. The short parameter lines needed for different time-varying options are:\n\nEnvironmental Linkages: Requires 1 short parameter line (\\(P_{t}\\)), except for link option 4, which requires 2 short parameter lines (\\(P_{t1}\\) and \\(P_{t2}\\)).\nParameter deviations: Requires 2 short parameter lines, one for the standard error (\\(\\text{dev}_{se}\\)), followed by one for \\(\\rho\\). Note that an input for \\(\\rho\\) is required but only used with random walk options. For the random walk options, \\(\\rho\\) can be set at 1 for a random walk with no drift or &gt;1 for a random walk with drift.\nTime Blocks: One parameter for each time block (\\(P_{t}\\)) set up in the pattern.\nTrends: Requires 3 parameter lines. The interpretation of the parameters differs by the trend option selected, but in general they are a parameter for the final parameter value, a parameter for the inflection point year, and a parameter for the width (i.e., the standard deviation).\n\n\n\n14.1.4 Example Time-varying Parameter Setups\nThe time-varying parameter options in Stock Synthesis are flexible. Below are some example setups that illustrate how the time-varying options could be used in a model, although there are many more possible setups.\n\n14.1.4.1 Environmental and density dependent linkages\n\n\n\nSuppose growth rate is found to be linked with an index of water temperature. The water temperature proxy could be input into the data file as environmental data. If it is input as index number 1, the growth parameter \\(K\\) (if using a von Bertalanffy growth equation) could be linked to the water temperature proxy data by specifying the code \"201\" in the env_var&link function input. This would establish an offset link between the parameter and the temperature proxy. One additional parameter line is required after the \"MG parameter\" long parameter lines section.\nSuppose for a fishery, selectivity is thought to shift depending on population size. Smaller fish are selected when there are lower population numbers, while larger fish are selected when there are higher population numbers. The selectivity parameter could be made time-varying using the code \"-104\" in the env_var&link option, which assumes a exponential scalar link between the base selectivity parameter and the time varying parameter value. One additional parameter line is required at the end of the selectivity long parameter lines section.\n\n\n\n14.1.4.2 Parameter Deviations\n\n\n\nSuppose a selectivity parameter is thought to drift every year during 2000-2010. This could be represented using a random walk link option available within the parameter deviations options. To implement this, the user could input 3 into the \"dev link\" input on the long parameter line for the selectivity parameter, and then input values 2000 and 2010 for \"dev min yr\" and \"dev max yr\", respectively. The dev phase could be set to 3. With this setup, 2 additional short parameter lines would be expected, one for the standard error and one for \\(\\rho\\). Both of these will be used since a random walk option is selected. To use a random walk without drift, \\(\\rho\\) is set at 1 with a negative phase.\n\n\n\n14.1.4.3 Time Blocks\n\n\n\nOffset approach: One or more time blocks are created and cover all or a subset of the years. Each block gets a parameter that is used as an offset from the base parameter (time block functional form 1). In this situation, typically the base parameter and each of the offset parameters are estimated. In years not covered by blocks, the base parameter alone is used. However, if blocks cover all the years, then the value of the block parameter is completely correlated with the mean of the block offsets, so model convergence and variance estimation could be affected. The recommended approach when using offsets is to not have all years covered by blocks or to fix the base parameter value at a reasonable level when doing offsets for all years.\nReplacement approach, Option A: Time blocks are created which cover a subset of the years. The base parameter is used in the non-block years and the value of the base parameter is replaced by the block parameter in each respective block (time block functional form 2). In this situation, typically the base parameter and each of the block parameters are estimated.\nReplacement approach, Option B: Replacement time blocks are created for all the years, so the base parameter is simply a placeholder that is always replaced by a block parameter (time block functional form 2). In this situation, do not allow the model to estimate the base parameter and only estimate the corresponding block replacement parameters, otherwise, the search algorithm will be attempting to estimate parameters that do not contribute to the log likelihood, so model convergence and variance estimation could be affected.\n\n\n\n14.1.4.4 Trends\n\n\n\nSuppose natural mortality was thought to increase from 0.1 to 0.2 during 2000 to 2010. This could be input as a trend. First, the natural mortality parameter would be fixed at an initial value of 0.1. Then, a value of -2 could be input into the \"use block\" column of the natural mortality long parameter line to indicate that the direct input option for trends should be used. The long parameter line for M could look like:\n\n\n\n\nLO\nHI\nINIT\n&lt;other entries&gt;\nPHASE\n&lt;other entries&gt;\nUse_Block\nBlock Fxn\nParameter Label\n\n\n\n\n0\n4\n0.1\n...\n-1\n...\n-2\n0\n#M\n\n\n\n\nThree short parameter lines are then expected after the mortality-growth long parameter lines, one for the final value, one for the inflection year and one for the width. The final value could be fixed by using 0.2 as the final value on the short parameter line and a negative phase value. The inflection year could be fixed at 2005 by inputting 2005 for the inflection year in the short parameter line with a negative phase. Finally, the width value (i.e., standard deviation of the cumulative normal distribution) could be set at 3 years. The short parameter lines could look like:\n\n\n\n\n\n\nPrior\nPrior\nPrior\n\n\n\n\nLO\nHI\nINIT\nValue\nSD\nType\nPhase\nParameter Label\n\n\n\n\n\nPrior\nPrior\nPrior\n\n\n\n\nLO\nHI\nINIT\nValue\nSD\nType\nPhase\nParameter Label\n\n\n0.001\n4\n0.2\n0\n0.01\n0\n-1\n#M_TrendFinal\n\n\n1999\n2011\n2005\n0\n0.01\n0\n-1\n#M_TrendInfl\n\n\n-99\n99\n3\n0\n0.01\n0\n-1\n#M_TrendWidth_yrs\n\n\n\n\n\n\n\n\n\n\n14.1.5 Time-Varying Growth Considerations\nWhen time-varying growth is used, there are some additional considerations to be aware of:\n\nGrowth in the forecast with time blocks: Growth deviations propagate into the forecast because growth is by cohort according to the current year’s growth parameters. The user can select which growth parameters get used during the forecast by setting the end year of the last block, if using time blocks. If the last block ends in the model’s end year, then the growth parameters in effect during the forecast will be the base parameters. By setting the end year of the last block to one year past the model end year (endyr), the model will continue the last block’s growth parameter levels throughout the forecast.\nThe equilibrium benchmark quantities (MSY, F40%, etc.) previously used the model end year’s (endyr) body size-at-age, which is not in equilibrium. Through the forecast file, it is possible to specify a range of years over which to average the size-at-age used in the benchmark calculations. An option to create equilibrium growth from averaged growth parameters would be a more realistic option and is under consideration, but is not yet available.\n\n\n\n\n\n\n14.1.6 Time-Varying Stock-Recruitment Considerations\n\nThe \\(\\sigma_R\\) and autocorrelation parameters cannot be time-varying.\nThe autocorrelation parameter cannot be estimated accurately within SS3 (Johnson et al. 2016), so external (i.e., external to SS3) estimation for selecting an autocorrelation value is currently recommended. The autocorrelation of the recruitment deviations appears in the report file, which can aid in selecting the autocorrelation value.\nThe value of R0 and steepness in the initial year are used within virgin calculations and within the benchmarks for calculation of the denominator in depletion estimates. The average value of R0 and steepness in the range of years specified as the benchmark years inputs 9 and 10 (see the forecast file specifications) is used for MSY-type calculations.\nThe spawner-recruit regime parameter is a modifier on R0. The regime shift parameter line allows for multi-year or environmentally driven deviations from R0 without changing R0 itself. The regime shift base parameter should have a base value of 0.0 and not be estimated (i.e., have a negative phase). Similar to the cohort-growth deviation, it serves simply as a base for adding time-varying adjustments.\nThe same algebraic effect on the calculated recruitment can be achieved by different combinations of spawner-recruit parameter options (e.g., changing R0 directly instead of the regime shift parameter). It is recommended to use block, trend or environmental effects on R0 only for long-term effects, and use time-vary effects on the regime shift parameter for transitory but multi-year deviations from R0.\nIf the R0 or steepness parameters are time-varying, then the model will use the current year’s parameters to calculate the expected value of recruits as a function of the spawning biomass, then applies the recruitment deviations. If the regime shift parameter is time-varying, then the model applies the change in the regime shift parameter after calculating the expected value of recruits as a function of spawning biomass.\n\n\n\n14.1.7 Forecast Considerations with Time-Varying Parameters\nUsers should judiciously consider which parameter values are applied during forecast years. SS3 will default to use all base parameter values during the forecast period, but alternatively, which years of selectivity, relative F, and recruitment should be used during the forecast period by specifying in the forecast file.\nTime-varying parameters can extend into the forecast period. For example, a parameter with a time block that stops at the model end year will revert to the base parameter value for the forecast, but when the block definition extends to include some or all forecast years, the last block will apply to the forecast. A good practice is to use 9999 as the terminal year for the last block to ensure including all forecast years. If a parameter has deviations and the deviations’ year range includes the forecast years, then the parameter will have process uncertainty in the forecast years and MCMC draws(if using) will include the variability."
  },
  {
    "objectID": "qmds/html.html#parameterizing-the-two-dimensional-autoregressive-selectivity",
    "href": "qmds/html.html#parameterizing-the-two-dimensional-autoregressive-selectivity",
    "title": "RAW HTML CONTENT",
    "section": "14.2 Parameterizing the Two-Dimensional Autoregressive Selectivity",
    "text": "14.2 Parameterizing the Two-Dimensional Autoregressive Selectivity\nWhen the two-dimensional autoregressive selectivity feature is turned on for a fleet, the selectivity is calculated as a product of the assumed selectivity pattern and a non-parametric deviation term deviating from this assumed pattern:\n\\[\\hat{S}_{a,t} = S_aexp^{\\epsilon_{a,t}}\\]\nwhere \\(S_a\\) is specified in the corresponding age/length selectivity types section and it can be either parametric (recommended) or non-parametric (including any of the existing selectivity options in SS3); \\(\\epsilon_{a,t}\\) is simulated as a two-dimensional first-order autoregressive (2D AR1) process:\n\\[vec(\\epsilon) \\sim MVN(\\mathbf{0},\\sigma_s^2\\mathbf{R_{total}})\\]\nwhere \\(\\epsilon\\) is the two-dimensional deviation matrix and \\(\\sigma_s^2\\mathbf{R_{total}}\\) is the covariance matrix for the 2D AR1 process. More specifically, \\(\\sigma_s^2\\) quantifies the variance in selectivity deviations and \\(\\mathbf{R_{total}}\\) is equal to the kronecker product (\\(\\otimes\\)) of the two correlation matrices for the among-age and among-year AR1 processes:\n\\[\\mathbf{R_{total}}=\\mathbf{R}\\otimes\\mathbf{\\tilde{R}}\\]\n\\[\\mathbf{R}_{a,\\tilde{a}}=\\rho_a^{|a-\\tilde{a}|}\\]\n\\[\\mathbf{\\tilde{R}}_{t,\\tilde{t}}=\\rho_t^{|t-\\tilde{t}|}\\]\nwhere \\(\\rho_a\\) and \\(\\rho_t\\) are the among age and among year AR1 coefficients, respectively. When both of them are zero, \\(\\mathbf{R}\\) and \\(\\mathbf{\\tilde{R}}\\) are two identity matrices and their Kronecker product, \\(\\mathbf{R_{total}}\\), is also an identity matrix. In this case selectivity deviations are essentially identical and mutually independent:\n\\[\\epsilon_{a,t}\\sim N(0,\\sigma_s^2)\\]\n\n14.2.0.1 Using the Two-Dimensional Autoregressive Selectivity\n\nNote, Xu et al. (2019) has additional information on tuning the 2D AR selectivity parameters. First, fix the two AR1 coefficients (\\(\\rho_a\\) and \\(\\rho_t\\)) at 0 and tune \\(\\sigma_s\\) iteratively to match the relationship:\n\\[\\sigma_s^2=SD(\\epsilon)^2+\\frac{1}{(a_{max}-a_{min}+1)(t_{max}-t_{min}+1)}\\sum_{a=a_{min}}^{a_{max}}\\sum_{t=t_{min}}^{t_{max}}SE(\\epsilon_{a,t})^2\\]\nThe minimal and maximal ages/lengths and years for the 2D AR1 process can be freely specified by users in the control file. However, we recommend specifying the minimal and maximal ages and years to cover the relatively \"data-rich\" age/length and year ranges only. Particularly we introduce:\n\\[b=1-\\frac{\\frac{1}{(a_{max}-a_{min}+1)(t_{max}-t{min}+1)}\\sum_{a=a_{min}}^{a_{max}}\\sum_{t=t_{min}}^{t_{max}}SE(\\epsilon_{a,t})^2}{\\sigma_s^2}\\]\nas a measure of how rich the composition data is regarding estimating selectivity deviations. We also recommend using the Dirichlet-Multinomial method to \"weight\" the corresponding composition data while \\(\\sigma_s\\) is interactively tuned in this step.\nSecond, fix \\(\\sigma_s\\) at the value iteratively tuned in the previous step and estimate \\(\\epsilon_{a,t}\\). Plot both Pearson residuals and \\(\\epsilon_{a,t}\\) out on the age-year surface to check their 2D dimensions. If their distributions seems to be not random but rather be autocorrelated (deviation estimates have the same sign several ages and/or years in a row), users should consider estimating and then including the autocorrelations in \\(\\epsilon_{a,t}\\).\nThird, extract the estimated selectivity deviation samples from the previous step for estimating \\(\\rho_a\\) and \\(\\rho_t\\) externally by fitting the samples to a stand-alone model written in Template-Model Builder (TMB). In this model, both \\(\\rho_a\\) and \\(\\rho_t\\) are bounded between 0 and 1 via applying a logic transformation. If at least one of the two AR1 coefficients are notably different from 0, the model should be run one more time by fixing the two AR1 coefficients at their values externally estimated from deviation samples. The Pearson residuals and \\(\\epsilon_{a,t}\\) from this run are expected to distribute more randomly as the autocorrelations in selectivity deviations can be at least partially included in the 2D AR1 process."
  },
  {
    "objectID": "qmds/html.html#continuous-seasonal-recruitment",
    "href": "qmds/html.html#continuous-seasonal-recruitment",
    "title": "RAW HTML CONTENT",
    "section": "14.3 Continuous seasonal recruitment",
    "text": "14.3 Continuous seasonal recruitment\nSetting up a seasonal model such that recruitment can occur with similar and independent probability in any season of any year is awkward in SS3. Instead, SS3 can be set up so that each quarter appears as a year (i.e., a seasons as years model). All the data and parameters are set up to treat quarters as if they were years. Note that setting up a seasons as years model also requires that all rate parameters be re-scaled to correctly account for the quarters being treated as years.\nOther adjustments to make when using seasons as years include:\n\nRe-index all \"year seas\" inputs to be in terms of quarter-year because all are now season 1; increase end year (endyr) value in sync with this.\nIncrease max age because age is now in quarters.\nIn the age error definitions, increase the number of entries to reflect that age is now in quarters.\nIn the age error definitions, recode so that each quarter-age gets assigned to the correct age bin. This is because the age data are still in terms of age bins; i.e., the first 4 entries for quarter-ages 1 through 4 will all be assigned to age bin 1.5; the next four to age bin 2.5; you cannot accomplish the same result by editing the age bin values because the standard deviation of ageing error is in terms of age bin.\nIn the control file, multiply the natural mortality age breakpoints and growth Amin and Amax values by 1/season duration.\nDecrease the R0 parameter starting value because it is now the average number of recruitments per quarter year.\nEdit the recruitment deviation (rec_dev) start and end years to be in terms of quarter year.\nEdit any age selectivity parameters that refer to age, because they are now in terms of quarter age.\nIf there needs to be some degree of seasonality to a parameter, then you could create a cyclic pattern in the environmental input and make the parameter a function of this cyclic pattern."
  },
  {
    "objectID": "qmds/html.html#jitter",
    "href": "qmds/html.html#jitter",
    "title": "RAW HTML CONTENT",
    "section": "15.1 Jitter",
    "text": "15.1 Jitter\n\n\n\nThe jitter function has been updated with SS3.30. The following steps are now performed to determine the jittered starting parameter values (illustrated in Figure 2):\n\nA normal distribution is calculated such that the pr(PMIN) = 0.1% and the pr(PMAX) = 99.9%.\nA jitter shift value, termed \"K\", is calculated from the distribution equal to pr(PCURRENT).\nA random value is drawn, \"J\", from the range of K-jitter to K+jitter.\nAny value which falls outside the 0-1 range (in the cumulative normal space) is mapped back from the bound to a point one-tenth of the way from the bound to the initial value.\nJ is a new cumulative normal probability value.\nCalculate a new parameter value, PJITTERED, such that pr(PJITTERED) = J.\n\n\n\n\n\nIn SS3, the jitter fraction defines a uniform distribution in cumulative normal space +/- the jitter fraction from the initial value (in cumulative normal space). The normal distribution for each parameter, for this purpose, is defined such that the minimum bound is at 0.001, and the maximum at 0.999 of the cumulative distribution. If the jitter faction and original initial value are such that a portion of the uniform distribution goes beyond 0.001 or 0.999 of the cumulative normal, the new value is set to one-tenth of the way from the bound to the original initial value.\nTherefore sigma = (max-min) / 6.18. For parameters that are on the log-scale, sigma may be the correct measure of variation for jitters, for real-space parameters, CV (= sigma/original initial value) may be a better measure.\nIf the original initial value is at or near the middle of the min-max range, then for each 0.1 of jitter, the range of jitters extends about 0.25 sigmas to either side of the original value (though as the total jitter increases the relationship varies more than this), and the average absolute jitter is about half of that. For values far from the middle of the min-max range, the resulting jitter is skewed in parameter space, and may hit the bound, invoking the resetting mentioned above.\nTo evaluate the jittering, the bounds, and the original initial values, a jitter_info table is available from r4ss, including sigma, CV and InitLocation columns (the latter referring to location within the cumulative normal - too close to 0 or 1 indicates a potential issue).\nNote: parameters with min \\(\\leq\\) -99 or max \\(\\geq\\) 999 are not jittered to avoid unreasonable values (a warning is produced to indicate this)."
  },
  {
    "objectID": "qmds/html.html#parameter-priors",
    "href": "qmds/html.html#parameter-priors",
    "title": "RAW HTML CONTENT",
    "section": "15.2 Parameter Priors",
    "text": "15.2 Parameter Priors\nPriors on parameters fulfill two roles in SS3. First, for parameters provided with an informative prior, SS3 is receiving additional information about the true value of the parameter. This information works with the information in the data through the overall log likelihood function to arrive at the final parameter estimate. Second, diffuse priors provide only weak information about the value of a prior and serve to manage model performance during execution. For example, some selectivity parameters may become unimportant depending upon the values of other parameters of that selectivity function. In the double normal selectivity function, the parameters controlling the width of the peak and the slope of the descending side become redundant if the parameter controlling the final selectivity moves to a value indicating asymptotic selectivity. The width and slope parameters would no longer have any effect on the log likelihood, so they would have no gradient in the log likelihood and would drift aimlessly. A diffuse prior would then steer them towards a central value and avoid them crashing into the bounds. Another benefit of diffuse priors is the control of parameters that are given unnaturally wide bounds. When a parameter is given too broad of a bound, then early in a model run it could drift into this tail and potentially get into a situation where the gradient with respect that parameter approaches zero even though it is not at its global best value. Here the diffuse prior helps move the parameter back towards the middle of its range where it presumably will be more influential and estimable.\nThe options for parameter priors are described as a function of \\(Pval\\), the value of the parameter for which a prior is being calculated, as well as the parameter bounds in the case of the beta distribution (\\(Pmax\\) and \\(Pmin\\)), and the input values for \\(Prior\\) and \\(Pr\\_SD\\), which in some cases are the mean and standard deviation, but interpretation depends on the prior type. The Prior Likelihoods below represent the negative log likelihood in all cases.\n\n15.2.0.1 Prior Types\n\nNote that the numbering in SS3 v.3.30 is different from that used in SS3 v.3.24 (where confusingly -1 indicated no prior and 0 indicated a normal prior). The calculation of the negative log likelihood is provided below for each prior types, as a function of the following inputs:\n\n\n\n\\(P_\\text{init}\\)\nThe value of the parameter for which a prior is being calculated where init can either be\n\n\n\nthe initial un-estimated value or the estimated value (3rd column in control or\n\n\n\ncontrol.ss_new file)\n\n\n\\(P_\\text{LB}\\)\nThe lower bound of the parameter (1st column in control file)\n\n\n\\(P_\\text{UB}\\)\nThe upper bound of the parameter (2nd column in control file)\n\n\n\\(P_\\text{PR}\\)\nThe input value for the prior input (4th column in control file)\n\n\n\\(P_\\text{PRSD}\\)\nThe standard deviation input value for the prior (5th column in control file)\n\n\n\n\nPrior Type = 0 = No prior applied\nIn a Bayesian context this is equivalent to a uniform prior between the parameter bounds.\nPrior Type = 1 = Symmetric beta prior\nThe symmetric beta is scaled between parameter bounds, imposing a larger penalty near the bounds. Prior standard deviation of 0.05 is very diffuse and a value of 5.0 provides a smooth U-shaped prior. The prior input is ignored for this prior type. \\[\\mu = -P_\\text{PRSD} \\cdot ln\\left(\\frac{P_\\text{UB}+P_\\text{LB}}{2} - P_\\text{LB} \\right) - P_\\text{PRSD} \\cdot ln(0.5)\\]\n\\[\\begin{split} \\text{Prior Likelihood} = &-\\mu -P_\\text{PRSD} \\cdot ln\\left(P_\\text{init}-P_\\text{LB}+0.0001\\right) - \\\\ & P_\\text{PRSD} \\cdot ln\\left(1-\\frac{P_\\text{init}-P_\\text{LB}-0.0001}{P_\\text{UB}-P_\\text{LB}}\\right) \\end{split}\\]\n\n\n\n\nPrior Type = 2 = Beta prior\nThe definition of \\(\\mu\\) is consistent with CASAL’s formulation with the \\(\\beta_\\text{PR}\\) and \\(\\alpha_\\text{PR}\\) corresponding to the \\(m\\) and \\(n\\) parameters. \\[\\mu = \\frac{P_\\text{PR}-P_\\text{LB}}{P_\\text{UB}-P_\\text{LB}}\\] \\[\\tau = \\frac{(P_\\text{PR}-P_\\text{LB})(P_\\text{UB}-P_\\text{PR})}{P_\\text{PRSD}^2}-1\\] \\[\\beta_\\text{PR} = \\tau \\cdot \\mu\\] \\[\\alpha_\\text{PR} = \\tau (1-\\mu)\\]\n\\[\\begin{split} \\text{Prior Likelihood} = &(1 - \\beta_\\text{PR}) \\cdot ln(0.0001 + P_\\text{init} - P_\\text{LB}) + \\\\ &(1 - \\alpha_\\text{PR}) \\cdot ln(0.0001 + P_\\text{UB} - P_\\text{init}) - \\\\ &(1 - \\beta_\\text{PR}) \\cdot ln(0.0001 + P_\\text{PR} - P_\\text{LB}) - \\\\ &(1 - \\alpha_\\text{PR}) \\cdot ln(0.0001 + P_\\text{UB} - P_\\text{PR}) \\end{split}\\]\nPrior Type 3 = Lognormal prior\nNote that this is undefined for \\(p &lt;= 0\\) so the lower bound on the parameter must be &gt; 0. The prior value is input into the parameter line in natural log space while the initial parameter value is defined in normal space (e.g., init = 0.20, prior = -1.609438). \\[\\text{Prior Likelihood} = \\frac{1}{2} \\left(\\frac{ln(P_\\text{init})-P_\\text{PR}}{P_\\text{PRSD}}\\right)^2\\]\nPrior Type 4 = Lognormal prior with bias correction\nThis option allows the prior mean value to be entered as the ln(mean). Note that this is undefined for \\(p &lt;= 0\\) so the lower bound on the parameter must be &gt; 0. \\[\\text{Prior Likelihood} = \\frac{1}{2} \\left(\\frac{ln(P_\\text{init})-P_\\text{PR} + \\frac{1}{2}{P_\\text{PRSD}}^2}{P_\\text{PRSD}}\\right)^2\\]\nPrior Type 5 = Gamma prior\nThe lower bound should be 0 or greater. \\[\\text{scale} = \\frac{{P_\\text{PRSD}}^2}{P_\\text{PR}}\\] \\[\\text{shape} = \\frac{P_\\text{PR}}{\\text{scale}}\\] \\[\\text{Prior Likelihood} = -\\text{shape} \\cdot ln(\\text{scale}) - ln\\big(\\Gamma(\\text{shape})\\big) + (\\text{shape} - 1) \\cdot ln(P_\\text{init}) - \\frac{P_\\text{init}}{\\text{scale}}\\]\nPrior Type 6 = Normal prior\nNote that this function is independent of the parameter bounds. \\[\\text{Prior Likelihood} = \\frac{1}{2} \\left(\\frac{P_\\text{init} - P_\\text{PR}}{P_\\text{PRSD}}\\right)^2\\]"
  },
  {
    "objectID": "qmds/html.html#sec:forecast",
    "href": "qmds/html.html#sec:forecast",
    "title": "RAW HTML CONTENT",
    "section": "15.3 Forecast Module: Benchmark and Forecasting Calculations",
    "text": "15.3 Forecast Module: Benchmark and Forecasting Calculations\nSS3 v.3.20 introduced substantial upgrades to the benchmark and forecast module. The general intent was to make the forecast outputs more consistent with the requirement to set catch limits that have a known probability of exceeding the overfishing limit. In addition, this upgrade addressed several inadequacies with the previous module, including:\n\nThe average selectivity and relative F was the same for the benchmark and the forecast calculations;\nThe biology-at-age in endyr+1 was used as the biology for the benchmark, but biology-at-age propagated forward in the forecast if there was time-varying growth;\nThe forecast module had a inefficient approach to calculation of overfishing limit (OFL) conditioned on previously catching ABC;\nThe forecast module implementation of catch caps was incomplete and applied some caps on a seasonally, rather than the more logical annual basis;\nThe Fmult scalar for fishing intensity presented a confusing concept for many users;\nNo provision for specification of catch allocation among fleets;\nThe forecast allowed for a blend of fixed input catches and catches calculated from target F; this is not optimal for calculation of the variance of F conditioned on a catch policy that sets annual catch limits (ACLs).\n\nThe V3.20 module addressed these issues by:\n\nProviding for unique specification of a range of years from which to calculate average selectivity for benchmark, average selectivity for forecast, relative F for benchmark, and relative F for forecast;\nCreate a new specification for the range of years over which to average size-at-age and fecundity-at-age for the benchmark calculation. In a setup with time-varying growth, it may make sense to do this over the entire range of years in the time series. Note that some additional quantities still use their endyr values, notably the migration rates and the allocation of recruitments among areas. This will be addressed shortly;\nCreate a multiple pass approach that rectifies the OFL calculation;\nImprove the specification of catch caps and implement specification of catch allocations so that there can be an annual cap for each fleet, an annual cap for each area, and an annual allocation among groups of fleets (e.g., all recreational fleets vs. all commercial fleets);\nIntroduce capability to have implementation error in the forecast catch (single value applied to all fleets in all seasons of the year).\n\n\n15.3.0.1 Multiple Pass Forecast\n\nThe most complicated aspect of the changes is with regard to the multiple pass aspect of the forecast. This multiple pass approach is needed to calculate both OFL and ABC in a single model run. More importantly, the multiple passes are needed in order to mimic the actual sequence of assessment-management action - catch over a multi-year period. The first pass calculates OFL based on catching OFL each year, so presents the absolute maximum upper limit to catches. The second pass forecasts a catch based on a harvest policy, then applies catch caps and allocations, then updates the F’s to match these catches. In the third pass, stochastic recruitment and catch implementation error are implemented and SS3 calculates the F that would be needed in order to catch the adjusted catch amount previously calculated in the second pass. With this approach, SS3 is able to produce improved estimates of the probability that F would exceed the overfishing F. In effect it is the complement of the P* approach. Rather than the P* approach that calculates the stream of annual catches that would have an annual probability of F&gt;Flimit, SS3 calculates the expected time series of P* that would result from a specified harvest policy implemented as a buffer between Ftarget and Flimit.\nThe sequence of multiple forecast passes is as follows:\n\nPass 1 (a.k.a. Fcast_Loop1)\n\nLoop Years\n\nSubLoop (a.k.a. ABC_Loop) = 1\n\nR=f(SSB) with no deviations\nF=Flimit\nFixed input catch amounts ignored\nNo catch adjustments (caps and allocations)\nNo implementation error\nResult: OFL conditioned on catching OFL each year\n\n\n\nPass 2\n\nLoop Years\n\nSubLoop = 1\n\nR=f(SSB) with no deviations\nF=Flimit\nFixed input catch amounts ignored\nNo catch adjustments (caps and allocations)\nNo implementation error\nResult: OFL conditioned on catching ABC previous year. Stored in std_vector.\n\nSubLoop = 2\n\nR=f(SSB) with no deviations\nF=Ftarget to get catch for each fleet in each season\nFixed input catch amounts replace catch from step 2\nCatch adjustments (caps and allocations) applied on annual basis (after looping through seasons and and areas within this year). These adjustments utilize the logistic joiner approach so the overall results remain completely differentiable.\nNo implementation error\nResult: ABC as adjusted for caps and allocations\n\nSubLoop = 3\n\nR=f(SSB) with no deviations\nCatches from Pass 2 multiplied by the random term for implementation error\nF=adjusted to match the catch*error while taking into account the random recruitments. This is most easily visualized in a MCMC context where the recruitment deviation and the implementation error deviations take on non-zero values in each instance. In MLE, because the forecast recruitments and implementation error are estimated parameters with variance, their variance still propagates to the derived quantities in the forecast.\nResult: Values for F, SSB, Recruitment, Catch are stored in std-vectors\n\nIn addition, the ratios F/Flimit and SSB/SSBlimit or SSB/SSBtarget are also stored in std_vectors.\nEstimated variance in these ratios allows calculation of annual probability that F&gt;Flimit or B&lt;Blimit. This is essentially the realized P* conditioned on the specified harvest policy.\n\n\n\n\n\n\n\n15.3.0.2 Example Effects on Correlations\n\nAn example that illustrates the above process was conducted. The situation was a low M, late-maturing species, so changes are not dramatic. The example conducted a 10 year forecast and examined correlations with derived quantities in the last year of the forecast. This was done once with the full set of 3 passes as described above, and again with only 2 passes and stochastic recruitment occurring in pass 2, rather than 3. This alternative setup is more similar to forecasts done using previous model versions.\n\n\n\n\n\n2 Forecast Passes with F from\n\n2 Forecast Passes with catch from\n\n\n\nABC and random recruitment\n\ntarget F and equilibrium recruitment\n\n\n\nFactor X\nFactor Y\nCorr\n\nFactor X\nFactor Y\nCorr\n\n\nA1\nF 2011\nRecrDev 2002\n-0.126\nA2\nF 2011\nRecrDev 2002\n0.090\n\n\nB1\nF 2011\nRecr 2002\n0.312\nB2\nF 2011\nRecr 2002\n0.518\n\n\nC1\nForeCatch 2011\nRecrDev 2002\n0.000\nC2\nForeCatch 2011\nRecrDev 2002\n0.129\n\n\nD1\nForeCatch 2011\nRecr 2002\n0.455\nD2\nForecatch 2011\nRecr 2002\n0.555\n\n\n\n\nCorrelation A2 shows a small positive correlation between the recruitment deviation in 2002 and the F in 2011. This is probably due to the fact that a positive deviation in recruitment in 2002 will reduce the chances that the biomass in 2011 will be below the inflection point in the control rule. This occurs because in calculating catch from F, the model effectively \"knows\" the future recruitments. I predict that this B1 correlation would be near zero if there was no inflection in the control rule.\nCorrelation A1 shows this turning into a negative correlation. This is because the future catches are first calculated from equilibrium recruitment, then when random recruitments are implemented, a positive recruitment deviation will cause a negative deviation in the F needed to catch that now \"fixed\" amount of future catch.\nCorrelations B1 and B2 are in terms of absolute recruitment, not recruitment deviation. Now overall model conditions that cause a higher absolute recruitment level will also result in a higher forecast level. No surprise there, and the correlation is stronger when variance is based on catch is calculated from F (B2).\nCorrelation C2 shows a positive correlation between recruitment deviation in 2002 and forecast catch in 2011. However, correlation C1 is 0.0 because the forecast catch in 2011 is set based on equilibrium recruitment and is not influenced by the recruitment deviations.\n\n\n15.3.0.3 Future Work\n\n\n\nMore testing with high M, rapid turnover conditions\nTesting without inflection in control rule\nConsider separating implementation error into a pass #4 so results will more clearly show effect of assessment uncertainty separate from implementation uncertainty\nConsider adding a random \"assessment\" error which essentially is a random variable that scales population abundance before passing into the forecast stage. Complication is figuring out how to link it to the correlated error in the benchmark quantities\nBecause all of these calculations occur only in the standard deviation phase (sdphase) or the MCMC evaluation (mceval) phase, it would be feasible for mceval calls to add an additional pass that is implemented many times and in which random forecast recruitment draws are made.\nFactors like selectivity and fleet relative F levels are calculated as an average of these values during the time series. This is internally consistent if these factors do not vary during the time series (although clearly this is a stiff model that will underestimate process variance). However, if these factors do vary over time, then the average used for the forecast will under-represent the variance. A better approach would be to set up the parameters of selectivity as a random process that extends throughout the forecast period, and to update estimated selectivity in each year of the forecast based upon the random realization of these parameters."
  },
  {
    "objectID": "qmds/html.html#fishing-mortality-in-stock-synthesis",
    "href": "qmds/html.html#fishing-mortality-in-stock-synthesis",
    "title": "RAW HTML CONTENT",
    "section": "15.4 Fishing Mortality in Stock Synthesis",
    "text": "15.4 Fishing Mortality in Stock Synthesis\nThe implementation and reporting of fishing mortality rate, \\(F\\), in SS3 has some aspects that can be confusing. This description provides an overview of the ways in which \\(F\\) is calculated, used, and reported.\n\n15.4.0.1 Rationale\n\nFishery management systems expect to have a measure of annual fishing mortality that describes the intensity of the fishery such that an optimal level of \\(F\\) can be articulated and accountability measures can be invoked if \\(F\\) is too high, e.g., overfishing. This concept is simple and straightforward if the model is a simple biomass dynamics such that a single annual \\(F\\) value operates on the entirety of a non-age structured population. It also is simple for age-structured models that have a single fishing fleet and knife-edge selectivity beginning at some specified age.\nThe simplicity of \\(F\\) disappears quickly as models invoke a variety of realistic complexities such as: allowing the \\(F\\) to differ among ages or to be based on size; using a collection of fleets with different \\(F\\) levels and different age patterns for \\(F\\); spreading the population across areas and allowing different fleets with different \\(F\\) among the areas. An unambiguous measure of annual fishing intensity that represents the cumulative effect of all that complexity has not been defined. This problem has not been solved with SS3, but some logical alternatives have been made available.\n\n\n15.4.0.2 Nomenclature\n\nThe nomenclature below ignores sex, morphs and areas for simplicity. The quantities associated with \\(F\\) calculations are defined as:\n\\(f\\) is fleet.\n\\(t\\) is a time step; continuous across years \\(y\\) and seasons \\(s\\); equivalent to year if only 1 season.\n\\(a\\) is age.\n\\(C_{t,f}\\) is fleet-specific catch in a time step.\n\\(B_{t,f}\\) is fleet specific available biomass, e.g., total biomass filtered by fleet-specific age selectivity, \\(s_{t,f,a}\\).\n\\(s_{t,f,a}\\) is age-specific selectivity for a fleet. If selectivity is length-specific, then age-specific selectivity is calculated as the dot product across length bins of length selectivity and the normal (or lognormal) distribution of length-at-age. If selectivity is both length- and age-based, which is an entirely normal concept in SS3, then age selectivity due to length selectivity is calculated first, then multiplied by the direct age selectivity. This compound age selectivity is used in the mortality calculations and is reported as asel2 in report file. See appendix to Methot and Wetzel (2013) for more detail on this.\n\\(F_{t,f}'\\) is the apical fishing mortality for a fleet. This means that it is the rate for the age that has selectivity equal to 1.0. If your model is using \\(F'\\)s as parameters, then the parameter values are for \\(F'\\).\n\\(F_{t,f,a}\\) is age and fleet-specific fishing mortality rate equal to \\(F_{t,f}' * s_{t,f,a}\\). Note that it is possible for no age to have a selectivity equal to 1.0. In this case, \\(F'\\) is still the rate for the hypothetical age that has selectivity equal to 1.0. The reported \\(F'\\) values are not rescaled to be an \\(F\\) for the age with peak selectivity. Users need to take this into account if they are comparing reported \\(F'\\) values to reported vector of \\(F_{t,f,a}\\) values.\n\\(\\text{ann}F_y\\) is a measure of the total fishing intensity for a year, based on one of several user-specified options (see below).\n\\(F\\text{std}_y\\) is a standardized measure of the total fishing intensity for a year and is reported in the derived quantities, so variance is calculated for this quantity. See below for how it relates to \\(annF\\).\nTerminology and reporting of \\(\\text{ann}F\\) and \\(F\\text{std}\\) has been slightly revised for clarity in 3.30.15.00 and the description here follows the new conventions.\n\n\n15.4.0.3 \\(F\\) Calculation\n\nSS3 allows for three approaches to estimate the \\(F'\\) that will match the input values for retained catch. Note that SS3 is calculating the \\(F'\\) to match the retained catch conditional on the fraction of total catch that is retained, e.g., the total catch is partitioned into retained and discarded portions.\n\nPope’s method decays the numbers-at-age to the middle of the season, calculates a harvest rate for each fleet, \\(H_{t,f}\\), that is the ratio of \\(C_{t,f}\\) to \\(B_{t,f}\\), then decays the survivors to the end of the season. the total mortality, \\(Z_{t,a}\\), from the ratio of survivors to initial numbers, is then calculated. The \\(Z\\) is subsequently used for in-season interpolation to get expected values for observations.\n\\(F\\) as parameters uses the standard Baranov catch equation and lets ADMB find the \\(F'\\) parameter values that produce the lowest negative log-likelihood, which includes fit to the input catch data. \\(F\\) as parameters method tends to work better than Pope’s or hybrid in high \\(F\\) situations because it allows for some lack of fit to catch levels in early iterations and can later improve this fit as it closes in on the best solution.\nHybrid \\(F\\) starts by calculating a harvest rate, \\(H\\), using Pope’s, then converts these \\(H\\) values, which have units of fractional harvest rate, into an approximate of \\(F'\\) in exponential units, tuning these \\(F'\\) values over a few iterations to get a better match to each fleet’s catch.\n\nItems to note:\n\nSS3 includes a permutation on the \\(F\\) as parameters method. In the first few phases, SS3 uses hybrid, then between phases it converts these directly calculated \\(F'\\) values into parameters and proceeds in subsequent phases and MCMC to use the parameter approach. This variation on the parameter method is the recommend approach in high \\(F\\) situations.\nWith Pope’s method, the \\(H\\) values are fraction caught, so duration of the season does not matter. Parameter and hybrid treat \\(F'\\) identically and multiply the \\(F'\\) values by season duration (which has units of fraction of a year) as it is used. Each of the \\(F\\) methods ends up with a \\(Z_{t,f}\\) that is used for in-season interpolation.\n\n\n\n15.4.0.4 Relative \\(F\\) and \\(F\\)mult\n\nThe \\(F'\\) is fleet-specific, so it is useful to have a concept of relative \\(F\\), \\(\\text{rel}F_f\\), among fleets. In SS3, \\(\\text{rel}F_f= F_{t,f}'/\\sum_{f}^{}F_{t,f}'\\) for a single time period \\(t\\). In the benchmark and forecast routines, SS3 can calculate \\(\\text{rel}F_f\\) using \\(F_{t,f}'\\) over a range of years, or the user can input custom \\(\\text{rel}F\\) values for benchmark and forecast in the forecast.ss file. Note that in a multi-season model setup, \\(\\text{rel}F_f\\) is implemented as \\(\\text{rel}F_{s,f}\\) where \\(s\\) is the season. These get multiplied by season duration as they are used.\nIn the benchmark section of the code, SS3 searches for an \\(F\\)mult to achieve various management reference points (often referred to as benchmarks). In this search, SS3 calculates a benchmark \\(F\\) as \\(F_{ben,f}' = F\\text{mult} * \\text{rel}F_f\\), then calculates equilibrium yield and spawning biomass per recruit (SPR). SS3 searches for the \\(F\\)mult that satisfies the search conditions, first for user-specified SPR, then for user-specified spawning biomass at a management target (BTGT or \\(F_{0.1}\\)), then for MSY. The resultant benchmark quantities are reported in the derived quantities, but \\(F\\)mult and \\(F_{ben,f}'\\) are only reported in the Forecast_report.sso file. SS3 stores the benchmark \\(F\\)mult values so that user can invoke them for the forecast.\n\n\n15.4.0.5 Annual \\(F\\)\n\nThe \\(\\text{ann}F\\) is a single annual value across all fleets and areas according to F_report_units, which is specified by users in the starter file. If there are many fleets, across several areas and with very different selectivity patterns, \\(\\text{ann}F\\) can have a complicated relationship to apical \\(F\\). The F_report_units specification in the starter.ss file, see example line below, allows user to calculate it using \\(F'\\) directly, use exploitation rate, or be derived from \\(Z\\)-at-age.\nExample \\(F\\) reporting unit specification in the starter.ss file:\n\n\n\n\n5\n# F_report_units:\n\n\n\n0 = skip;\n\n\n\n1 = exploitation(Bio);\n\n\n\n2 = exploitation(Num);\n\n\n\n3 = sum(Frates);\n\n\n\n4 = true F for range of ages;\n\n\n\n5 = unweighted avg. F for range of ages.\n\n\n3 7\n# min and max age over which average F will be calculated\n\n\n\n\nFor options 4 and 5 of F_report_units, the \\(F\\) is calculated as \\(Z-M\\) where \\(Z\\) is calculated as \\(ln(N_{t+1,a+1}/N_{t,a})\\), thus \\(Z\\) subsumes the effect of \\(F\\).\nThe ann\\(F\\) is calculated for each year of the estimated time series and of the forecast. Additionally, an ann\\(F\\) is calculated in the benchmark calculations to provide equilibrium values that have the same units as ann\\(F\\) from the time series. In versions previous to 3.30.15, it was labeled inaccurately as \\(F\\)std in the output, not ann\\(F\\). For example, in the Management Quantities section of derived quantities prior to 3.30.15, there is a quantity labeled Fstd_Btgt. This is more accurately labeled as the annual \\(F\\) associated with the biomass target, ann_F_Btgt, in 3.30.15.\n\n\n15.4.0.6 \\(F\\)std\n\n\\(F\\)std is a single annual value based on ann\\(F\\) and the relationship to ann\\(F\\) is specified by F_report_basis in the starter.ss file. The benchmark ann\\(F\\) may be used to rescale the time series of ann\\(F\\)s to become a time series of standardized values representing the intensity of fishing, \\(F\\)std. The report basis is selected in the starter file as:\n\n\n\n\n0\n# F_report_basis:\n\n\n\n0 = raw F report;\n\n\n\n1 = F / FSPR;\n\n\n\n2 = F / FMSY;\n\n\n\n3 = F / FBTGT."
  },
  {
    "objectID": "qmds/index.html",
    "href": "qmds/index.html",
    "title": "quarto_doc_website",
    "section": "",
    "text": "This is a Quarto website.\nTest change.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  }
]